# üîß Informe de Gaps y Roadmap: Sistema Actual ‚Üí Apollo Original

**Objetivo**: Documentar todas las diferencias entre el sistema actual y Apollo, con plan de implementaci√≥n detallado para cerrar los gaps.

**Audiencia**: Personal (documentaci√≥n + futuro trabajo de implementaci√≥n)

---

## üìã √çndice

1. **Resumen Ejecutivo de Gaps**
2. **Gap #1: Assignment Algorithm (Hungarian ‚Üí Selection)**
3. **Gap #2: M√∫ltiples Detecciones por ROI**
4. **Gap #3: Projection Boxes Din√°micas**
5. **Gap #4: ID Management (Row Index ‚Üí Semantic ID)**
6. **Gap #5: Multi-Camera Fusion**
7. **Gap #6: Dependencia Espacial del Recognizer**
8. **Roadmap de Implementaci√≥n Priorizado**
9. **Plan de Testing y Validaci√≥n**

---

## 1. üìä Resumen Ejecutivo de Gaps

### Tabla de Gaps Identificados

| # | Gap | Impacto | Complejidad | Prioridad |
| --- | --- | --- | --- | --- |
| **1** | Hungarian ‚Üí Selection Algorithm | üî¥ Alto | üü° Media | **P0** |
| **2** | M√∫ltiples Detections/ROI no manejadas | üî¥ Alto | üü° Media | **P0** |
| **3** | Projection Boxes Est√°ticas ‚Üí Din√°micas | üî¥ Alto | üî¥ Alta | **P1** |
| **4** | Row Index ‚Üí Semantic ID | üü† Medio | üü¢ Baja | **P2** |
| **5** | Single Camera ‚Üí Multi-Camera | üü° Bajo | üî¥ Alta | **P3** |
| **6** | Dependencia Espacial Recognizer | üî¥ Alto | üî¥ Alta | **P1** |

### Impacto por Categor√≠a

```
Funcionalidad Core (Detecci√≥n/Assignment):
‚îú‚îÄ‚îÄ Hungarian vs Selection .............. üî¥ CR√çTICO
‚îú‚îÄ‚îÄ M√∫ltiples detections ................ üî¥ CR√çTICO
‚îî‚îÄ‚îÄ Projection boxes din√°micas .......... üî¥ CR√çTICO

Tracking/IDs:
‚îú‚îÄ‚îÄ Row index vs Semantic ID ............ üü† IMPORTANTE
‚îî‚îÄ‚îÄ Cross-history transfer .............. üî¥ CR√çTICO (causado por gaps anteriores)

Performance/Robustez:
‚îú‚îÄ‚îÄ Dependencia espacial ................ ÔøΩÔøΩ CR√çTICO
‚îî‚îÄ‚îÄ Multi-camera ....................... üü° NICE-TO-HAVE

```

---

## 2. üéØ Gap #1: Assignment Algorithm (Hungarian ‚Üí Selection)

### 2.1 Estado Actual vs Apollo

### **Sistema Actual: Hungarian Algorithm**

```python
# src/tlr/selector.py
def select_tls(ho, detections, projections, item_shape):
    costs = torch.zeros([len(projections), len(detections)])

    # Matriz de costos M√óN
    for row, projection in enumerate(projections):
        for col, detection in enumerate(detections):
            distance_score = calc_2d_gaussian_score(...)
            detection_score = torch.max(detection[5:])
            costs[row, col] = 0.3 * detection_score + 0.7 * distance_score

    # Assignment √≥ptimo 1:1
    assignments = ho.maximize(costs)  # [[proj_idx, det_idx], ...]

    return assignments

```

**Caracter√≠sticas**:

- ‚úÖ Assignment √≥ptimo global (maximiza suma de scores)
- ‚úÖ Garantiza no-conflictos (1 detection ‚Üí max 1 projection)
- ‚ùå Solo 2 m√©tricas (distance + confidence)
- ‚ùå¬†**Constraint 1:1 estricto**¬†(si 2 detections para 1 projection, solo 1 se asigna)
- ‚ùå No fusiona m√∫ltiples detections del mismo sem√°foro
- ‚ùå Complejidad O(N¬≥) (costoso para muchas detections)

---

### **Apollo Original: Score-based Selection**

```cpp
// Apollo's selection algorithm (pseudo-c√≥digo basado en documentaci√≥n)
struct SelectionCriteria {
    float detection_score;        // 0.4 weight
    float spatial_proximity;      // 0.3 weight
    float shape_consistency;      // 0.2 weight
    float temporal_consistency;   // 0.1 weight
};

for (auto &hd_light : hd_map_lights) {  // Para cada sem√°foro HD-Map
    vector<Detection> candidates;

    // Encontrar todas las detecciones cercanas
    for (auto &detection : all_detections) {
        if (distance(hd_light.projection, detection.bbox) < threshold) {
            candidates.push_back(detection);
        }
    }

    // Calcular score para cada candidato
    float best_score = -1;
    Detection* best_detection = nullptr;

    for (auto &candidate : candidates) {
        float score = 0.4 * candidate.confidence +
                     0.3 * spatial_score(hd_light, candidate) +
                     0.2 * shape_score(hd_light, candidate) +
                     0.1 * temporal_score(hd_light, candidate);

        if (score > best_score) {
            best_score = score;
            best_detection = &candidate;
        }
    }

    // Asignar mejor detection a este HD-Map light
    if (best_detection != nullptr) {
        assignments[hd_light.id] = best_detection;
    }
}

```

**Caracter√≠sticas**:

- ‚úÖ¬†**N detections ‚Üí 1 selecci√≥n**¬†por sem√°foro HD-Map
- ‚úÖ¬†**4 m√©tricas**¬†de evaluaci√≥n (m√°s robusto)
- ‚úÖ Fusiona m√∫ltiples detections del mismo objeto (elige la mejor)
- ‚úÖ¬†**Temporal consistency**¬†incluida en selection
- ‚úÖ Complejidad O(N) por sem√°foro (m√°s eficiente)
- ‚úÖ¬†**Permite detections sin asignar**¬†(no fuerza 1:1)

---

### 2.2 Diferencias Cr√≠ticas

| Aspecto | Hungarian (Actual) | Selection (Apollo) |
| --- | --- | --- |
| **Objetivo** | Maximizar suma global de scores | Encontrar mejor detection por sem√°foro |
| **Constraint** | 1:1 estricto | N:1 permitido (m√∫ltiples det ‚Üí 1 sem√°foro) |
| **Criterios** | 2 (distance + confidence) | 4 (+ shape + temporal) |
| **M√∫ltiples det mismo objeto** | Solo asigna 1, resto ‚Üí ID -1 | Fusiona (selecciona mejor) |
| **Temporal info** | ‚ùå No considerada | ‚úÖ Incluida (0.1 weight) |
| **Complejidad** | O(N¬≥) | O(N√óM) ‚âà O(N) |
| **Robustez** | Baja (solo spatial) | Alta (multi-criterio) |

---

### 2.3 ¬øPor Qu√© Apollo Usa Selection en vez de Hungarian?

### **Raz√≥n 1: M√∫ltiples Detections del Mismo Sem√°foro**

```
Escenario real:
- 1 sem√°foro f√≠sico genera 2-3 bboxes levemente diferentes
- Detector SSD puede producir m√∫ltiples proposals para mismo objeto

Hungarian: Solo asigna 1, resto quedan ID -1 (p√©rdida de informaci√≥n)
Selection: Selecciona la mejor, ignora duplicados (fusi√≥n impl√≠cita)

```

### **Raz√≥n 2: No Requiere Assignment Perfecto**

```
Hungarian: Necesita asignar TODAS las detections (o dejarlas ID -1)
           - Problema si hay N detections pero M<N projections

Selection: Cada HD-Map light busca su mejor detection independientemente
          - No importa si sobran detections (pueden ser false positives)

```

### **Raz√≥n 3: Temporal Consistency en Assignment**

```
Apollo: Usa historial para validar si una detection es consistente temporalmente
        - Si sem√°foro era verde y detection dice rojo ‚Üí score bajo
        - Si sem√°foro era verde y detection dice verde ‚Üí score alto

Sistema actual: Temporal consistency solo DESPU√âS de assignment (en tracker)

```

### **Raz√≥n 4: Shape Validation**

```
Apollo: Valida geometr√≠a (aspect ratio, tama√±o esperado vs detectado)
        - Detections con forma incorrecta ‚Üí score bajo

Sistema actual: No valida geometr√≠a en assignment

```

---

### 2.4 Impacto del Gap

### **Problemas Causados por Hungarian**

**Problema 1: ID -1 Excesivo**

```python
# Escenario: 1 sem√°foro genera 2 detections
projections = [proj_0]  # 1 projection
detections = [det_A, det_B]  # 2 detections del mismo sem√°foro

# Hungarian: Solo puede asignar 1:1
assignments = [[0, 0]]  # Asigna det_A a proj_0
# det_B queda ID -1 (perdido)

# Selection Apollo: Evaluar√≠a ambas, seleccionar√≠a la mejor
best = max(det_A, det_B, key=lambda d: score(d))
assignments = [best]  # Solo 1 resultado, pero eligi√≥ el mejor

```

**Frecuencia observada**: 5-10% de detections v√°lidas ‚Üí ID -1

---

**Problema 2: No Considera Temporal Consistency**

```python
# Frame 100: Sem√°foro proj_0 est√° en GREEN (history[0].color = 'green')
# Frame 101: 2 detections cerca de proj_0
det_A: clasificado como GREEN (consistente con history)
det_B: clasificado como RED (inconsistente - posible error)

# Hungarian: Solo usa distance + confidence
# Si det_B est√° m√°s cerca o tiene mayor confidence ‚Üí se asigna (INCORRECTO)

# Selection Apollo: Usar√≠a temporal_score
temporal_score(det_A) = high (GREEN ‚Üí GREEN transition OK)
temporal_score(det_B) = low (GREEN ‚Üí RED sin YELLOW = invalid)
# Resultado: det_A seleccionado (CORRECTO)

```

---

**Problema 3: Performance O(N¬≥)**

```python
# Con muchas detections:
10 projections √ó 20 detections = Hungarian O(30¬≥) ‚âà 27,000 ops
                                 Selection O(10√ó20) = 200 ops

# Impacto real:
Hungarian: 5-20ms para assignment
Selection: <1ms para assignment

```

---

### 2.5 Implementaci√≥n Propuesta: Apollo Selection Algorithm

### **Paso 1: Definir Criterios de Scoring**

```python
# src/tlr/apollo_selector.py (NUEVO ARCHIVO)

import torch
import numpy as np
from typing import List, Dict, Tuple, Optional

class SelectionCriteria:
    """Apollo-style selection criteria"""

    WEIGHT_DETECTION = 0.4
    WEIGHT_SPATIAL = 0.3
    WEIGHT_SHAPE = 0.2
    WEIGHT_TEMPORAL = 0.1

    @staticmethod
    def detection_score(detection: torch.Tensor) -> float:
        """
        Score basado en confidence del detector
        detection[0] = confidence score
        """
        return float(detection[0])

    @staticmethod
    def spatial_score(projection, detection: torch.Tensor,
                     sigma_x: float = 100, sigma_y: float = 100) -> float:
        """
        Score basado en distancia 2D gaussiana
        Mismo c√°lculo que Hungarian pero como score independiente
        """
        proj_center_x = (projection.xl + projection.xr) / 2
        proj_center_y = (projection.yt + projection.yb) / 2

        det_center_x = (detection[1] + detection[3]) / 2
        det_center_y = (detection[2] + detection[4]) / 2

        dx = proj_center_x - det_center_x
        dy = proj_center_y - det_center_y

        score = np.exp(-0.5 * ((dx/sigma_x)**2 + (dy/sigma_y)**2))
        return score

    @staticmethod
    def shape_score(projection, detection: torch.Tensor) -> float:
        """
        Score basado en consistencia geom√©trica
        Valida aspect ratio y tama√±o esperado
        """
        # Tama√±o esperado de projection
        proj_w = projection.xr - projection.xl
        proj_h = projection.yb - projection.yt
        proj_aspect = proj_w / proj_h if proj_h > 0 else 1.0

        # Tama√±o de detection
        det_w = detection[3] - detection[1]
        det_h = detection[4] - detection[2]
        det_aspect = det_w / det_h if det_h > 0 else 1.0

        # Score de aspect ratio (penaliza diferencias grandes)
        aspect_diff = abs(proj_aspect - det_aspect) / max(proj_aspect, det_aspect)
        aspect_score = 1.0 - min(aspect_diff, 1.0)

        # Score de tama√±o (penaliza detections muy grandes/peque√±as)
        size_ratio = (det_w * det_h) / (proj_w * proj_h)
        size_score = 1.0 if 0.5 <= size_ratio <= 2.0 else 0.5

        return (aspect_score + size_score) / 2.0

    @staticmethod
    def temporal_score(projection_id: int, detection: torch.Tensor,
                      history: Dict, recognitions: List[List[float]],
                      det_idx: int) -> float:
        """
        Score basado en consistencia temporal
        Valida si el cambio de estado es v√°lido
        """
        if projection_id not in history:
            return 0.5  # Neutral si no hay historial

        prev_color = history[projection_id].color

        # Obtener color actual de esta detection
        if det_idx < len(recognitions):
            curr_cls = int(max(range(len(recognitions[det_idx])),
                             key=lambda i: recognitions[det_idx][i]))
            curr_color = ["black", "red", "yellow", "green"][curr_cls]
        else:
            return 0.0

        # Validar transiciones
        valid_transitions = {
            "black": ["red", "yellow", "green", "black"],  # Desde unknown, todo OK
            "red": ["red", "yellow", "green"],             # Red puede ir a yellow/green
            "yellow": ["yellow", "red"],                   # Yellow solo a red (safety)
            "green": ["green", "yellow"]                   # Green a yellow (normal)
        }

        if curr_color in valid_transitions.get(prev_color, []):
            return 1.0  # Transici√≥n v√°lida
        else:
            return 0.2  # Transici√≥n inv√°lida (pero no imposible)

```

---

### **Paso 2: Implementar Selection Algorithm**

```python
# src/tlr/apollo_selector.py (continuaci√≥n)

class ApolloSelector:
    """
    Apollo-style selection algorithm
    Para cada projection, selecciona la mejor detection bas√°ndose en m√∫ltiples criterios
    """

    def __init__(self):
        self.criteria = SelectionCriteria()

    def select(self,
               detections: torch.Tensor,  # (N, 9) tensor
               projections: List,          # List of ProjectionROI
               history: Dict,              # Tracking history
               recognitions: List[List[float]],  # Recognition results
               distance_threshold: float = 200.0  # Max distance to consider
               ) -> List[Tuple[int, int]]:  # [(proj_id, det_idx), ...]
        """
        Selecciona la mejor detection para cada projection

        Args:
            detections: Todas las detecciones (N√ó9)
            projections: Lista de projection boxes
            history: Historial de tracking
            recognitions: Resultados de reconocimiento
            distance_threshold: Distancia m√°xima para considerar candidatos

        Returns:
            Lista de assignments [(proj_id, det_idx), ...]
        """
        assignments = []

        for proj_id, projection in enumerate(projections):
            # Paso 1: Filtrar candidates por distancia
            candidates = []

            for det_idx, detection in enumerate(detections):
                spatial = self.criteria.spatial_score(projection, detection)

                # Convertir score a distancia aproximada para threshold
                # (spatial_score alto = distancia baja)
                if spatial > 0.1:  # Score m√≠nimo (equivale a ~200px)
                    candidates.append((det_idx, detection))

            if len(candidates) == 0:
                continue  # No hay candidates para esta projection

            # Paso 2: Calcular score total para cada candidate
            best_score = -1
            best_det_idx = None

            for det_idx, detection in candidates:
                # 4 componentes del score
                det_score = self.criteria.detection_score(detection)
                spatial_score = self.criteria.spatial_score(projection, detection)
                shape_score = self.criteria.shape_score(projection, detection)
                temporal_score = self.criteria.temporal_score(
                    proj_id, detection, history, recognitions, det_idx
                )

                # Score total ponderado
                total_score = (
                    SelectionCriteria.WEIGHT_DETECTION * det_score +
                    SelectionCriteria.WEIGHT_SPATIAL * spatial_score +
                    SelectionCriteria.WEIGHT_SHAPE * shape_score +
                    SelectionCriteria.WEIGHT_TEMPORAL * temporal_score
                )

                if total_score > best_score:
                    best_score = total_score
                    best_det_idx = det_idx

            # Paso 3: Asignar mejor detection
            if best_det_idx is not None:
                assignments.append((proj_id, best_det_idx))

        return assignments

```

---

### **Paso 3: Integrar en Pipeline**

```python
# src/tlr/pipeline.py (MODIFICAR)

from tlr.apollo_selector import ApolloSelector  # NUEVO

class Pipeline(nn.Module):
    def __init__(self, detector, classifiers, ho, means_det, means_rec,
                 device=None, tracker=None, use_apollo_selector=True):  # NUEVO FLAG
        super().__init__()
        self.detector = detector
        self.classifiers = classifiers
        self.means_det = means_det
        self.means_rec = means_rec
        self.ho = ho  # Mantener para compatibilidad
        self.device = device
        self.tracker = tracker

        # NUEVO: Apollo selector
        self.use_apollo_selector = use_apollo_selector
        if use_apollo_selector:
            self.apollo_selector = ApolloSelector()

    def forward(self, img, boxes, frame_ts=None):
        # ... (detecci√≥n igual) ...

        detections = self.detect(img, boxes)
        tl_types = torch.argmax(detections[:, 5:], dim=1)
        valid_mask = tl_types != 0
        valid_detections = detections[valid_mask]
        invalid_detections = detections[~valid_mask]

        # MODIFICAR: Selecci√≥n de assignments
        if self.use_apollo_selector:
            # NUEVO: Apollo-style selection
            # Primero necesitamos recognitions para temporal scoring
            temp_recognitions = []
            if len(valid_detections) > 0:
                temp_recognitions = self.recognize(img, valid_detections,
                                                   tl_types[valid_mask]).cpu().tolist()

            # Obtener historial del tracker
            history = self.tracker.semantic.history if self.tracker else {}

            # Selection algorithm
            assignments = self.apollo_selector.select(
                valid_detections,
                boxes2projections(boxes),
                history,
                temp_recognitions
            )
            assignments = torch.tensor(assignments, device=self.device)
        else:
            # ORIGINAL: Hungarian algorithm
            assignments = select_tls(self.ho, valid_detections,
                                    boxes2projections(boxes), img.shape)

        # ... (resto del pipeline igual) ...

```

---

### 2.6 Plan de Testing

### **Test 1: M√∫ltiples Detections Mismo Sem√°foro**

```python
# test_apollo_selector.py

def test_multiple_detections_same_light():
    """
    Test: 1 sem√°foro genera 2 detections
    Esperado: Selection elige la mejor (mayor confidence)
    """
    projections = [
        ProjectionROI(400, 150, 60, 70)  # 1 projection
    ]

    detections = torch.tensor([
        [0.85, 410, 160, 450, 210, 0.01, 0.95, 0.03, 0.01],  # Det A: conf=0.85
        [0.92, 408, 162, 448, 212, 0.01, 0.96, 0.02, 0.01],  # Det B: conf=0.92 (mejor)
    ])

    selector = ApolloSelector()
    assignments = selector.select(detections, projections, {}, [])

    # Verificar que selecciona det_idx=1 (mayor confidence)
    assert len(assignments) == 1
    assert assignments[0] == (0, 1)  # proj_0 ‚Üí det_1

```

---

### **Test 2: Temporal Consistency**

```python
def test_temporal_consistency():
    """
    Test: Detection inconsistente temporalmente recibe score bajo
    """
    projections = [ProjectionROI(400, 150, 60, 70)]

    # History: Sem√°foro estaba en GREEN
    history = {
        0: SemanticTable(0, 1.0, 'green')
    }

    detections = torch.tensor([
        [0.90, 410, 160, 450, 210, 0.01, 0.95, 0.03, 0.01],  # Det A
        [0.95, 412, 162, 452, 212, 0.01, 0.94, 0.04, 0.01],  # Det B (mayor conf)
    ])

    recognitions = [
        [0, 0, 1, 0],  # Det A: YELLOW (transici√≥n v√°lida GREEN‚ÜíYELLOW)
        [0, 1, 0, 0],  # Det B: RED (transici√≥n INV√ÅLIDA GREEN‚ÜíRED)
    ]

    selector = ApolloSelector()
    assignments = selector.select(detections, projections, history, recognitions)

    # Verificar que selecciona Det A (consistente) a pesar de menor confidence
    assert assignments[0] == (0, 0)  # Temporal score compensa

```

---

### **Test 3: Shape Validation**

```python
def test_shape_validation():
    """
    Test: Detection con geometr√≠a incorrecta recibe score bajo
    """
    # Projection: aspect ratio vertical (60√ó70 ‚âà 0.86)
    projections = [ProjectionROI(400, 150, 60, 70)]

    detections = torch.tensor([
        [0.90, 410, 160, 450, 210, 0.01, 0.95, 0.03, 0.01],  # Det A: ~1:1.25 (OK)
        [0.92, 410, 160, 480, 180, 0.01, 0.96, 0.02, 0.01],  # Det B: ~3.5:1 (horizontal - MAL)
    ])

    selector = ApolloSelector()
    assignments = selector.select(detections, projections, {}, [])

    # Verificar que selecciona Det A (aspect ratio correcto)
    assert assignments[0] == (0, 0)

```

---

### 2.7 Migraci√≥n Gradual

### **Fase 1: Implementaci√≥n Paralela (Semana 1)**

```python
# Correr ambos algoritmos, comparar resultados
assignments_hungarian = select_tls(ho, detections, projections, shape)
assignments_apollo = apollo_selector.select(detections, projections, history, recognitions)

# Log diferencias
if not torch.equal(assignments_hungarian, torch.tensor(assignments_apollo)):
    log_difference(assignments_hungarian, assignments_apollo)

```

### **Fase 2: A/B Testing (Semana 2-3)**

```python
# Alternar entre algoritmos en diferentes frames
if frame_num % 2 == 0:
    assignments = apollo_selector.select(...)
else:
    assignments = select_tls(ho, ...)

# Comparar m√©tricas de performance

```

### **Fase 3: Deployment Completo (Semana 4)**

```python
# Usar Apollo selector por defecto
pipeline = load_pipeline(device, use_apollo_selector=True)

```

---

## 3. üîÑ Gap #2: M√∫ltiples Detecciones por ROI

### 3.1 Problema Actual

### **Comportamiento Observado**

```python
# Frame con 1 sem√°foro f√≠sico
projections = [
    [400, 150, 460, 220, 0]  # 1 projection para sem√°foro
]

# Detector genera m√∫ltiples bboxes
detections_from_detector = [
    [0.95, 410, 160, 450, 210, 0.01, 0.95, 0.03, 0.01],  # Det A: ligeramente arriba
    [0.92, 408, 162, 448, 212, 0.01, 0.96, 0.02, 0.01],  # Det B: ligeramente abajo
    [0.88, 412, 159, 452, 211, 0.01, 0.94, 0.04, 0.01],  # Det C: ligeramente a la derecha
]

# PASO 1: NMS global (threshold=0.7)
# IoU entre Det A y Det B = 0.85 (alta superposici√≥n)
# IoU entre Det A y Det C = 0.82
# ‚Üí NMS elimina Det B y Det C, mantiene Det A

detections_after_nms = [
    [0.95, 410, 160, 450, 210, 0.01, 0.95, 0.03, 0.01]  # Solo Det A
]

# PASO 2: Hungarian assignment
assignments = [[0, 0]]  # proj_0 ‚Üí det_0

# RESULTADO: ‚úÖ Funciona en este caso (solo 1 detection sobrevive)

```

**Pero...**

```python
# Frame con 2 sem√°foros f√≠sicos DISTINTOS en 1 ROI grande
projections = [
    [300, 100, 600, 400, 0]  # 1 ROI grande (crop_scale=2.5)
]

# Detector encuentra ambos sem√°foros
detections_from_detector = [
    [0.95, 320, 150, 360, 200, 0.01, 0.95, 0.03, 0.01],  # Sem√°foro izq
    [0.92, 520, 150, 560, 200, 0.01, 0.96, 0.02, 0.01],  # Sem√°foro der
]

# PASO 1: NMS global (threshold=0.7)
# IoU entre ambos = 0.0 (no se superponen)
# ‚Üí NMS mantiene ambos

detections_after_nms = [
    [0.95, 320, 150, 360, 200, ...],  # Det 0
    [0.92, 520, 150, 560, 200, ...]   # Det 1
]

# PASO 2: Hungarian assignment
# ‚ö†Ô∏è PROBLEMA: Solo 1 projection pero 2 detections
# Hungarian fuerza 1:1 ‚Üí solo asigna 1

assignments = [[0, 0]]  # proj_0 ‚Üí det_0
# Det 1 queda ID -1 ‚ùå (sem√°foro real perdido)

```

---

### 3.2 Dise√±o de Apollo: Multi-Detection Handling

### **Filosof√≠a de Apollo**

```cpp
// Apollo's multi-detection design principle:
// "Better to detect too many than to miss real traffic lights"

// 1. ROI expansion creates large search areas
float crop_scale = 2.5;  // ROI puede ser 2.5√ó m√°s grande que projection

// 2. Detector puede encontrar m√∫ltiples lights en 1 ROI
vector<Detection> detections_in_roi = detector.Infer(roi);
// detections_in_roi.size() puede ser 0, 1, 2, 3+...

// 3. Selection NO impone l√≠mite 1:1
for (auto &hd_light : hd_map_lights) {
    // Cada HD-Map light busca su mejor detection independientemente
    // M√∫ltiples HD-Map lights pueden estar en el mismo ROI
    best_detection = SelectBestDetection(hd_light, detections_in_roi);
}

```

---

### 3.3 ¬øCu√°ndo Ocurre Multi-Detection en 1 ROI?

### **Caso 1: M√∫ltiples Proposals del Mismo Sem√°foro**

```
Detector SSD genera m√∫ltiples bounding boxes para mismo objeto:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ  ‚Üê Det A (confidence 0.95)
    ‚îÇ ‚îÇ    ‚îÇ ‚îÇ
    ‚îî‚îÄ‚î§    ‚îú‚îÄ‚îò  ‚Üê Det B (confidence 0.92)
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò

NMS elimina duplicados (IoU > 0.7)
Resultado: 1 detection final ‚úÖ

```

**Manejo actual**: ‚úÖ Funciona correctamente (NMS hace su trabajo)

---

### **Caso 2: Sem√°foros Cercanos (Mismo Poste)**

```
ROI grande contiene 2 sem√°foros f√≠sicos distintos:

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îê          ‚îå‚îÄ‚îÄ‚îê ‚îÇ
    ‚îÇ  ‚îÇüî¥‚îÇ          ‚îÇüü¢‚îÇ ‚îÇ  ‚Üê 2 sem√°foros reales
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îò          ‚îî‚îÄ‚îÄ‚îò ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üë                    ‚Üë
   Det A              Det B

NMS NO elimina (IoU ‚âà 0)
Resultado: 2 detections v√°lidas

Apollo: Asigna ambas a diferentes HD-Map lights ‚úÖ
Sistema actual: Solo asigna 1, otra queda ID -1 ‚ùå

```

**Manejo actual**: ‚ùå Problema (Hungarian 1:1)

---

### **Caso 3: Intersecci√≥n Compleja**

```
ROI muy grande en intersecci√≥n:

    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  ‚îå‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îê       ‚îå‚îÄ‚îÄ‚îê   ‚îÇ
    ‚îÇ  ‚îÇüî¥‚îÇ  ‚îÇüü°‚îÇ       ‚îÇüü¢‚îÇ   ‚îÇ  ‚Üê 3+ sem√°foros
    ‚îÇ  ‚îî‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îò       ‚îî‚îÄ‚îÄ‚îò   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚Üë      ‚Üë           ‚Üë
   TL1    TL2         TL3

Apollo: 3 HD-Map lights, cada uno busca su mejor detection
Sistema actual: 1 projection, solo 1 assignment

```

**Manejo actual**: ‚ùå Problema cr√≠tico

---

### 3.4 An√°lisis del Gap

| Escenario | Detections | Projections | Apollo | Sistema Actual |
| --- | --- | --- | --- | --- |
| **Duplicados mismo objeto** | 2-3 | 1 | NMS fusiona ‚Üí 1 | NMS fusiona ‚Üí 1 ‚úÖ |
| **2 sem√°foros cercanos** | 2 | 1 | Requiere 2 HD-Map lights | Solo asigna 1 ‚ùå |
| **Intersecci√≥n compleja** | 5+ | 1 | Requiere 5+ HD-Map lights | Solo asigna 1 ‚ùå |

**Root cause**: Sistema actual asume¬†**1 Projection = 1 Sem√°foro esperado**

---

### 3.5 Soluci√≥n Propuesta

### **Opci√≥n A: Projection Boxes Espec√≠ficas (Quick Fix)**

```python
# En vez de 1 ROI grande, definir m√∫ltiples projections espec√≠ficas

# ‚ùå Actual (1 ROI grande):
projections = [
    [300, 100, 600, 400, 0]  # Cubre ambos sem√°foros
]

# ‚úÖ Propuesto (2 ROIs espec√≠ficas):
projections = [
    [320, 150, 360, 200, 0],  # Sem√°foro izquierdo
    [520, 150, 560, 200, 1]   # Sem√°foro derecho
]

```

**Ventajas**:

- ‚úÖ No requiere cambios en c√≥digo
- ‚úÖ Funciona con Hungarian 1:1
- ‚úÖ F√°cil de implementar

**Desventajas**:

- ‚ùå Requiere annotaci√≥n manual muy precisa
- ‚ùå No escala para intersecciones complejas
- ‚ùå No maneja sem√°foros inesperados (fuera de projections)

---

### **Opci√≥n B: Detecci√≥n Iterativa dentro de ROI (Apollo-like)**

```python
# src/tlr/pipeline.py (NUEVA IMPLEMENTACI√ìN)

def detect_multi(self, image, boxes):
    """
    Detecci√≥n multi-sem√°foro dentro de cada ROI
    Permite m√∫ltiples detections por projection
    """
    all_detections = []
    detection_to_projection = []  # Mapeo det_idx ‚Üí proj_id

    for proj_id, box in enumerate(boxes):
        projection = box2projection(box)

        # Crop y resize ROI
        input = preprocess4det(image, projection, self.means_det)

        # Detector encuentra N sem√°foros en esta ROI
        bboxes = self.detector(input.unsqueeze(0).permute(0, 3, 1, 2))

        # Restaurar coordenadas
        restored = restore_boxes_to_full_image(image, [bboxes], [projection])[0]

        # Agregar todas las detections de esta ROI
        for det in restored:
            all_detections.append(det)
            detection_to_projection.append(proj_id)  # Recordar de qu√© ROI vino

    all_detections = torch.vstack(all_detections) if all_detections else torch.empty((0,9))

    # NMS global (elimina duplicados entre ROIs)
    idxs = nms(all_detections[:, 1:5], 0.7)
    final_detections = all_detections[idxs]

    # Mantener mapeo despu√©s de NMS
    final_projection_map = [detection_to_projection[i] for i in idxs]

    return final_detections, final_projection_map

```

**Uso con Selection Algorithm**:

```python
def forward(self, img, boxes, frame_ts=None):
    # Detecci√≥n multi
    detections, det_to_proj_map = self.detect_multi(img, boxes)

    # Filtrado
    tl_types = torch.argmax(detections[:, 5:], dim=1)
    valid_mask = tl_types != 0
    valid_detections = detections[valid_mask]

    # Reconocimiento
    recognitions = self.recognize(img, valid_detections, tl_types[valid_mask])

    # Selection (ya implementado en Gap #1)
    assignments = self.apollo_selector.select(
        valid_detections,
        boxes2projections(boxes),
        history,
        recognitions.cpu().tolist()
    )

    # ‚ö†Ô∏è IMPORTANTE: Verificar que detection pertenece a ROI correcto
    validated_assignments = []
    for proj_id, det_idx in assignments:
        # Solo asignar si detection vino de esta ROI (o ROI cercana)
        original_proj = det_to_proj_map[det_idx]
        if original_proj == proj_id or spatial_distance_ok(original_proj, proj_id):
            validated_assignments.append((proj_id, det_idx))

    return validated_assignments

```

**Ventajas**:

- ‚úÖ Maneja m√∫ltiples sem√°foros por ROI
- ‚úÖ Compatible con Selection Algorithm
- ‚úÖ M√°s cercano a dise√±o Apollo

**Desventajas**:

- ‚ö†Ô∏è Requiere l√≥gica de validaci√≥n adicional
- ‚ö†Ô∏è M√°s complejo de debuggear

---

### **Opci√≥n C: HD-Map Integration (Soluci√≥n Completa Apollo)**

```python
# src/tlr/hdmap_projections.py (NUEVO M√ìDULO)

class HDMapProjector:
    """
    Proyecta sem√°foros del HD-Map a coordenadas 2D de imagen
    Replica funcionalidad Apollo de projection din√°mica
    """

    def __init__(self, hdmap_file: str, camera_calib: dict):
        self.hdmap = self.load_hdmap(hdmap_file)
        self.camera_calib = camera_calib

    def load_hdmap(self, hdmap_file):
        """
        Carga HD-Map con coordenadas 3D de sem√°foros

        Formato esperado (JSON):
        {
            "traffic_lights": [
                {
                    "id": "TL001",
                    "position_3d": [x, y, z],  # Coordenadas mundo
                    "orientation": "vertical",
                    "expected_states": ["red", "yellow", "green"]
                },
                ...
            ]
        }
        """
        import json
        with open(hdmap_file) as f:
            return json.load(f)

    def project_lights(self, vehicle_pose: dict) -> List[dict]:
        """
        Proyecta sem√°foros 3D a 2D seg√∫n pose del veh√≠culo

        Args:
            vehicle_pose: {
                'position': [x, y, z],
                'orientation': [roll, pitch, yaw],
                'timestamp': float
            }

        Returns:
            Lista de projections 2D:
            [
                {
                    'semantic_id': 'TL001',
                    'bbox_2d': [x1, y1, x2, y2],
                    'distance': float,
                    'orientation': str
                },
                ...
            ]
        """
        projections = []

        for tl in self.hdmap['traffic_lights']:
            # Calcular transformaci√≥n 3D‚Üí2D
            pos_3d = np.array(tl['position_3d'])

            # Transform: World ‚Üí Vehicle ‚Üí Camera
            pos_vehicle = self.world_to_vehicle(pos_3d, vehicle_pose)
            pos_camera = self.vehicle_to_camera(pos_vehicle)

            # Proyecci√≥n perspectiva
            u, v = self.camera_to_pixel(pos_camera, self.camera_calib)

            # Estimar tama√±o en imagen (basado en distancia)
            distance = np.linalg.norm(pos_camera)
            estimated_size = self.estimate_size(distance, tl['orientation'])

            # Crear bbox 2D
            x1 = int(u - estimated_size[0] / 2)
            y1 = int(v - estimated_size[1] / 2)
            x2 = int(u + estimated_size[0] / 2)
            y2 = int(v + estimated_size[1] / 2)

            projections.append({
                'semantic_id': tl['id'],
                'bbox_2d': [x1, y1, x2, y2],
                'distance': distance,
                'orientation': tl['orientation']
            })

        return projections

```

**Integraci√≥n en Pipeline**:

```python
# src/tlr/pipeline.py (con HD-Map)

class Pipeline(nn.Module):
    def __init__(self, ..., hdmap_projector=None):
        # ...
        self.hdmap_projector = hdmap_projector

    def forward(self, img, vehicle_pose, frame_ts=None):
        # PASO 1: Proyectar sem√°foros del HD-Map
        if self.hdmap_projector:
            hdmap_projections = self.hdmap_projector.project_lights(vehicle_pose)
            boxes = [[p['bbox_2d'][0], p['bbox_2d'][1],
                     p['bbox_2d'][2], p['bbox_2d'][3],
                     p['semantic_id']] for p in hdmap_projections]
        else:
            # Fallback a boxes est√°ticas
            boxes = load_static_boxes()

        # PASO 2-N: Pipeline normal
        detections = self.detect(img, boxes)
        # ...

```

**Ventajas**:

- ‚úÖ Soluci√≥n completa Apollo-style
- ‚úÖ Projection boxes din√°micas (siguen sem√°foros f√≠sicos)
- ‚úÖ IDs sem√°nticos persistentes
- ‚úÖ Elimina cross-history transfer
- ‚úÖ Escalable a cualquier escenario

**Desventajas**:

- ‚ùå Requiere HD-Map con coordenadas 3D
- ‚ùå Requiere pose del veh√≠culo (GPS + IMU)
- ‚ùå Requiere calibraci√≥n de c√°mara precisa
- ‚ùå Complejidad alta de implementaci√≥n

---

### 3.6 Recomendaci√≥n de Implementaci√≥n

### **Roadmap Sugerido**

**Fase 1 (Corto Plazo - 1 semana)**: Opci√≥n A - Projection Boxes Espec√≠ficas

- Revisar annotations actuales
- Split ROIs grandes en m√∫ltiples projections espec√≠ficas
- Re-generar¬†`projection_bboxes_master.txt`
- **Resultado**: Funciona con c√≥digo actual, 0 cambios necesarios

**Fase 2 (Mediano Plazo - 2-3 semanas)**: Opci√≥n B - Detecci√≥n Iterativa

- Implementar¬†`detect_multi()`¬†con mapeo det‚Üíproj
- Integrar con Selection Algorithm (Gap #1)
- Testing exhaustivo con m√∫ltiples sem√°foros
- **Resultado**: M√°s robusto, maneja casos inesperados

**Fase 3 (Largo Plazo - 1-2 meses)**: Opci√≥n C - HD-Map Integration

- Crear m√≥dulo¬†`HDMapProjector`
- Obtener/crear HD-Map del escenario de prueba
- Implementar transformaciones 3D‚Üí2D
- Integrar con vehicle pose tracking
- **Resultado**: Sistema completo Apollo-equivalent

---

### 3.7 Testing Multi-Detection

```python
# test_multi_detection.py

def test_two_lights_same_roi():
    """
    Test: 2 sem√°foros distintos en 1 ROI grande
    Esperado: Ambos detectados y asignados
    """
    # 1 ROI grande que contiene 2 sem√°foros
    boxes = [[300, 100, 600, 400, 0]]

    # Imagen sint√©tica con 2 sem√°foros
    img = create_synthetic_image_with_two_lights(
        light1_pos=(350, 200),  # Izquierda
        light2_pos=(550, 200)   # Derecha
    )

    pipeline = load_pipeline(device, use_apollo_selector=True)
    valid_dets, recs, assigns, _, _ = pipeline(img, boxes, frame_ts=0.0)

    # Verificar: Deber√≠an haber 2 detections v√°lidas
    assert len(valid_dets) >= 2, f"Solo detect√≥ {len(valid_dets)}, esperaba 2"

    # Con Selection Algorithm, ambas deber√≠an asignarse
    # (requiere 2 projections espec√≠ficas o HD-Map con 2 semantic IDs)
    assert len(assigns) == 2, f"Solo {len(assigns)} assignments, esperaba 2"

def test_detection_mapping():
    """
    Test: Verificar que detection‚Üíprojection mapping es correcto
    """
    boxes = [
        [300, 100, 400, 200, 0],  # ROI izquierda
        [500, 100, 600, 200, 1]   # ROI derecha
    ]

    img = create_synthetic_image_with_two_lights(
        light1_pos=(350, 150),
        light2_pos=(550, 150)
    )

    pipeline = load_pipeline(device)
    dets, det_to_proj = pipeline.detect_multi(img, boxes)

    # Verificar mapeo
    for det_idx, proj_id in enumerate(det_to_proj):
        det = dets[det_idx]
        det_center_x = (det[1] + det[3]) / 2

        if det_center_x < 450:  # Izquierda
            assert proj_id == 0
        else:  # Derecha
            assert proj_id == 1
```

---

## 4. üó∫Ô∏è Gap #3: Projection Boxes Din√°micas

### 4.1 Estado Actual vs Apollo

### **Sistema Actual: Projection Boxes Est√°ticas**

```python
# projection_bboxes_master.txt (archivo manual)
frame_000001.jpg 421,165,460,223,0 466,165,511,256,1
frame_000002.jpg 421,165,460,223,0 466,165,511,256,1  # ‚Üê Mismas coordenadas
frame_000003.jpg 421,165,460,223,0 466,165,511,256,1  # ‚Üê Mismas coordenadas
# ...

# Carga en pipeline
def load_boxes_from_file(frame_name):
    with open('projection_bboxes_master.txt') as f:
        for line in f:
            if frame_name in line:
                boxes = parse_boxes(line)
                return boxes
    return []

# Resultado: Boxes NO se actualizan, son est√°ticas por video completo

```

**Caracter√≠sticas**:

- ‚ùå Coordenadas fijas (no siguen movimiento de c√°mara)
- ‚ùå IDs son √≠ndices de array (row index), no sem√°nticos
- ‚ùå Requiere annotaci√≥n manual para cada video
- ‚ùå No escala a nuevos escenarios
- ‚úÖ Simple de implementar y debuggear

---

### **Apollo Original: Projection Boxes Din√°micas**

```cpp
// Apollo's dynamic projection flow (cada frame)

// 1. Obtener pose del veh√≠culo
CarPose current_pose = GetVehiclePose();  // GPS + IMU + Odometry
// current_pose = {position: [x, y, z], orientation: [roll, pitch, yaw]}

// 2. Query HD-Map por sem√°foros cercanos
vector<TrafficLight> nearby_lights = hdmap_->GetTrafficLightsInRange(
    current_pose.position,
    search_radius = 200.0  // metros
);

// 3. Proyectar cada sem√°foro 3D ‚Üí 2D
for (auto &light : nearby_lights) {
    // Transformar coordenadas: World ‚Üí Vehicle ‚Üí Camera ‚Üí Image
    Eigen::Vector3d pos_world = light.position_3d;
    Eigen::Vector3d pos_camera = WorldToCamera(pos_world, current_pose);

    // Proyecci√≥n perspectiva
    Eigen::Vector2d pixel_coords = CameraToPixel(pos_camera, camera_calib_);

    // Estimar tama√±o en imagen (funci√≥n de distancia)
    float distance = pos_camera.norm();
    Eigen::Vector2i size = EstimateSizeFromDistance(distance, light.type);

    // Crear projection box 2D
    base::RectI projection_box;
    projection_box.x = pixel_coords.x() - size.x() / 2;
    projection_box.y = pixel_coords.y() - size.y() / 2;
    projection_box.width = size.x();
    projection_box.height = size.y();

    // Asignar semantic ID (del HD-Map, persistente)
    light.id = light.semantic_id;  // e.g., "TL_001"
    light.projection = projection_box;
}

// Resultado: Projection boxes actualizadas cada frame, siguen sem√°foros f√≠sicos

```

**Caracter√≠sticas**:

- ‚úÖ Coordenadas din√°micas (siguen movimiento de c√°mara/veh√≠culo)
- ‚úÖ IDs sem√°nticos persistentes (del HD-Map)
- ‚úÖ Autom√°tico (no requiere annotaci√≥n manual)
- ‚úÖ Escala a cualquier escenario con HD-Map
- ‚ùå Requiere infraestructura compleja (HD-Map + localization)

---

### 4.2 Por Qu√© Apollo Usa Projection Boxes Din√°micas

### **Raz√≥n 1: Compensar Movimiento del Veh√≠culo**

```
Frame N (veh√≠culo en posici√≥n A):
    Sem√°foro f√≠sico en (X=100, Y=50, Z=5) coords mundo
    ‚îî‚Üí Proyecci√≥n 2D: bbox (432, 176, 452, 212) en imagen

Frame N+1 (veh√≠culo avanz√≥ 5 metros):
    Mismo sem√°foro en (X=100, Y=50, Z=5) coords mundo
    ‚îî‚Üí Proyecci√≥n 2D: bbox (440, 180, 460, 216) en imagen ‚Üê CAMBI√ì

Sin actualizaci√≥n din√°mica:
    - Projection box en frame N+1 sigue en (432, 176, 452, 212)
    - Sem√°foro f√≠sico ahora est√° en (440, 180, 460, 216)
    - ‚ùå Projection box ya NO cubre el sem√°foro ‚Üí detecci√≥n falla

```

---

### **Raz√≥n 2: Mantener IDs Sem√°nticos Persistentes**

```cpp
// Con projection boxes din√°micas:

// Frame N:
TL_001 (sem√°foro izquierdo) ‚Üí bbox (432, 176, 452, 212)
TL_002 (sem√°foro derecho)   ‚Üí bbox (476, 175, 501, 247)

// Frame N+100 (veh√≠culo gir√≥, sem√°foros cambiaron posiciones en imagen):
TL_001 (ahora en derecha)   ‚Üí bbox (520, 190, 540, 230)  // ‚Üê Actualizado
TL_002 (ahora en izquierda) ‚Üí bbox (380, 185, 400, 225)  // ‚Üê Actualizado

// Historial:
history["TL_001"] = estado_semaforo_izquierdo  // ‚Üê ID sem√°ntico persistente
history["TL_002"] = estado_semaforo_derecho    // ‚Üê ID sem√°ntico persistente

// ‚úÖ Historial sigue al sem√°foro f√≠sico, NO a la posici√≥n espacial

```

**Contraste con sistema actual**:

```python
# Frame N:
history[0] = estado_semaforo_en_posicion_izquierda  # row_index=0
history[1] = estado_semaforo_en_posicion_derecha    # row_index=1

# Frame N+100 (sem√°foros intercambiaron posiciones):
history[0] = ??? # Ahora tiene historial del sem√°foro que EST√â en posici√≥n row=0
history[1] = ??? # (puede ser diferente sem√°foro f√≠sico)

# ‚ùå Cross-history transfer ocurre

```

---

### **Raz√≥n 3: Adaptaci√≥n Autom√°tica a Nuevos Escenarios**

```
Apollo con HD-Map:
    - Nuevo escenario ‚Üí Cargar HD-Map del √°rea
    - Projection boxes generadas autom√°ticamente
    - No requiere annotaci√≥n manual

Sistema actual:
    - Nuevo escenario ‚Üí Annotaci√≥n manual de projection_bboxes_master.txt
    - Frame por frame (o propagaci√≥n manual)
    - Propenso a errores humanos

```

---

### 4.3 Componentes Necesarios para Projection Boxes Din√°micas

### **Componente 1: HD-Map (High Definition Map)**

```json
// Ejemplo: hdmap_intersection_001.json
{
    "map_version": "1.0",
    "coordinate_system": "WGS84",  // GPS coords
    "traffic_lights": [
        {
            "semantic_id": "TL_INT001_001",
            "position_3d": {
                "latitude": -34.603722,
                "longitude": -58.381592,
                "altitude": 25.5
            },
            "orientation": "vertical",
            "lanes_controlled": ["lane_001", "lane_002"],
            "expected_states": ["red", "yellow", "green"],
            "metadata": {
                "installation_date": "2023-01-15",
                "type": "vehicle_signal"
            }
        },
        {
            "semantic_id": "TL_INT001_002",
            "position_3d": {
                "latitude": -34.603735,
                "longitude": -58.381605,
                "altitude": 25.5
            },
            "orientation": "horizontal",
            "lanes_controlled": ["lane_003"],
            "expected_states": ["red", "yellow_arrow", "green_arrow"]
        }
    ],
    "lanes": [
        {
            "id": "lane_001",
            "type": "driving",
            "direction": "north",
            "waypoints": [...]
        }
    ]
}

```

**Herramientas para crear HD-Map**:

- **Manual**: Google Earth + mediciones GPS
- **Semi-autom√°tico**: LiDAR scan + annotation tool (e.g., Apollo Studio)
- **Autom√°tico**: SLAM + semantic segmentation

---

### **Componente 2: Vehicle Localization**

```python
# src/tlr/localization.py (NUEVO M√ìDULO)

class VehicleLocalizer:
    """
    Provee pose del veh√≠culo en tiempo real
    Fusiona m√∫ltiples sensores para localization robusta
    """

    def __init__(self):
        self.gps = GPSSensor()
        self.imu = IMUSensor()
        self.odometry = WheelOdometry()

        # Kalman filter para fusi√≥n de sensores
        self.ekf = ExtendedKalmanFilter(
            state_dim=6,  # [x, y, z, roll, pitch, yaw]
            measurement_dim=9  # GPS(3) + IMU(3) + Odom(3)
        )

    def get_current_pose(self) -> dict:
        """
        Retorna pose actual del veh√≠culo

        Returns:
            {
                'position': [x, y, z],      # Coordenadas mundo (metros)
                'orientation': [r, p, y],   # Roll, pitch, yaw (radianes)
                'velocity': [vx, vy, vz],   # Velocidad (m/s)
                'timestamp': float,         # Unix timestamp
                'confidence': float         # 0-1
            }
        """
        # Leer sensores
        gps_data = self.gps.read()      # [lat, lon, alt]
        imu_data = self.imu.read()      # [ax, ay, az, gx, gy, gz]
        odom_data = self.odometry.read()  # [dx, dy, dtheta]

        # Fusi√≥n con Kalman filter
        measurement = np.concatenate([gps_data, imu_data[:3], odom_data])
        self.ekf.update(measurement)

        state = self.ekf.get_state()

        return {
            'position': state[:3].tolist(),
            'orientation': state[3:6].tolist(),
            'velocity': self.compute_velocity(state),
            'timestamp': time.time(),
            'confidence': self.ekf.get_confidence()
        }

```

**Alternativa simplificada (para testing sin hardware)**:

```python
class SimulatedLocalizer:
    """
    Localizer simulado para testing sin sensores reales
    Usa odometr√≠a visual o asume veh√≠culo est√°tico
    """

    def __init__(self, initial_pose=None):
        self.pose = initial_pose or {
            'position': [0, 0, 0],
            'orientation': [0, 0, 0],
            'velocity': [0, 0, 0],
            'timestamp': time.time(),
            'confidence': 1.0
        }

    def get_current_pose(self):
        return self.pose

    def update_from_visual_odometry(self, prev_frame, curr_frame):
        """
        Estima movimiento usando feature matching entre frames
        """
        # Detectar features (ORB, SIFT, etc.)
        kp1, desc1 = self.feature_detector.detect(prev_frame)
        kp2, desc2 = self.feature_detector.detect(curr_frame)

        # Match features
        matches = self.matcher.match(desc1, desc2)

        # Estimar transformaci√≥n (Essential matrix ‚Üí R, t)
        E, mask = cv2.findEssentialMat(
            pts1, pts2, self.camera_matrix
        )
        _, R, t, _ = cv2.recoverPose(E, pts1, pts2, self.camera_matrix)

        # Actualizar pose
        self.pose['position'] += t.flatten().tolist()
        # ... (actualizar orientation con R)

```

---

### **Componente 3: Calibraci√≥n de C√°mara**

```python
# camera_calibration.json
{
    "camera_name": "front_6mm",
    "image_width": 1920,
    "image_height": 1080,
    "intrinsics": {
        "fx": 1000.0,      # Focal length X (pixels)
        "fy": 1000.0,      # Focal length Y (pixels)
        "cx": 960.0,       # Principal point X
        "cy": 540.0,       # Principal point Y
        "skew": 0.0        # Axis skew (usualmente 0)
    },
    "distortion": {
        "model": "radial-tangential",
        "k1": -0.15,       # Radial distortion coef 1
        "k2": 0.08,        # Radial distortion coef 2
        "p1": 0.001,       # Tangential distortion 1
        "p2": -0.002,      # Tangential distortion 2
        "k3": -0.01        # Radial distortion coef 3
    },
    "extrinsics": {
        "position": [1.5, 0.0, 1.2],    # C√°mara relativa a veh√≠culo (m)
        "rotation": [0, 0.1, 0]         # Roll, pitch, yaw (rad)
    }
}

```

**Herramientas para calibraci√≥n**:

- OpenCV calibration tool (checkerboard pattern)
- Kalibr (multi-camera calibration)
- MATLAB Camera Calibrator app

---

### **Componente 4: Proyecci√≥n 3D ‚Üí 2D**

```python
# src/tlr/projection_3d_to_2d.py (NUEVO M√ìDULO)

import numpy as np

class Projector3Dto2D:
    """
    Proyecta coordenadas 3D del mundo a p√≠xeles 2D de imagen
    """

    def __init__(self, camera_calib: dict):
        self.calib = camera_calib

        # Matriz intr√≠nseca de c√°mara (K)
        fx = camera_calib['intrinsics']['fx']
        fy = camera_calib['intrinsics']['fy']
        cx = camera_calib['intrinsics']['cx']
        cy = camera_calib['intrinsics']['cy']

        self.K = np.array([
            [fx,  0, cx],
            [ 0, fy, cy],
            [ 0,  0,  1]
        ])

        # Distorsi√≥n
        self.dist_coeffs = np.array([
            camera_calib['distortion']['k1'],
            camera_calib['distortion']['k2'],
            camera_calib['distortion']['p1'],
            camera_calib['distortion']['p2'],
            camera_calib['distortion']['k3']
        ])

        # Extr√≠nsecos (c√°mara relativa a veh√≠culo)
        self.T_cam_vehicle = self.build_transform_matrix(
            camera_calib['extrinsics']['position'],
            camera_calib['extrinsics']['rotation']
        )

    def world_to_camera(self, point_world: np.ndarray, vehicle_pose: dict) -> np.ndarray:
        """
        Transforma punto del mundo a coordenadas de c√°mara

        point_world: [x, y, z] en coordenadas mundo
        vehicle_pose: pose del veh√≠culo

        Returns: [x, y, z] en coordenadas de c√°mara
        """
        # Transformaci√≥n: World ‚Üí Vehicle
        T_vehicle_world = self.build_transform_matrix(
            vehicle_pose['position'],
            vehicle_pose['orientation']
        )

        # Transformaci√≥n completa: World ‚Üí Vehicle ‚Üí Camera
        T_cam_world = self.T_cam_vehicle @ np.linalg.inv(T_vehicle_world)

        # Aplicar transformaci√≥n
        point_world_h = np.append(point_world, 1)  # Homogeneous coords
        point_camera_h = T_cam_world @ point_world_h

        return point_camera_h[:3]

    def camera_to_pixel(self, point_camera: np.ndarray) -> tuple:
        """
        Proyecta punto 3D de c√°mara a p√≠xel 2D

        point_camera: [x, y, z] en coordenadas de c√°mara

        Returns: (u, v) coordenadas de p√≠xel
        """
        # Proyecci√≥n perspectiva
        x, y, z = point_camera

        if z <= 0:
            return None  # Punto detr√°s de la c√°mara

        # Proyecci√≥n (sin distorsi√≥n)
        u_norm = x / z
        v_norm = y / z

        # Aplicar distorsi√≥n radial-tangencial
        r2 = u_norm**2 + v_norm**2
        k1, k2, p1, p2, k3 = self.dist_coeffs

        radial = 1 + k1*r2 + k2*r2**2 + k3*r2**3
        u_dist = u_norm * radial + 2*p1*u_norm*v_norm + p2*(r2 + 2*u_norm**2)
        v_dist = v_norm * radial + p1*(r2 + 2*v_norm**2) + 2*p2*u_norm*v_norm

        # Aplicar intr√≠nsecos
        u = self.K[0,0] * u_dist + self.K[0,2]
        v = self.K[1,1] * v_dist + self.K[1,2]

        return (int(u), int(v))

    def project_traffic_light(self, tl_world_pos: np.ndarray,
                             vehicle_pose: dict,
                             tl_type: str = "vertical") -> dict:
        """
        Proyecta sem√°foro 3D a bbox 2D

        Args:
            tl_world_pos: Posici√≥n 3D del sem√°foro en mundo
            vehicle_pose: Pose actual del veh√≠culo
            tl_type: "vertical", "horizontal", "quad"

        Returns:
            {
                'bbox': [x1, y1, x2, y2],
                'center': (u, v),
                'distance': float,
                'visible': bool
            }
        """
        # Transformar a coordenadas de c√°mara
        point_cam = self.world_to_camera(tl_world_pos, vehicle_pose)

        # Proyectar a p√≠xel
        pixel_coords = self.camera_to_pixel(point_cam)

        if pixel_coords is None:
            return {'visible': False}

        u, v = pixel_coords

        # Calcular distancia
        distance = np.linalg.norm(point_cam)

        # Estimar tama√±o en imagen (funci√≥n de distancia)
        # Tama√±o t√≠pico: 30cm ancho √ó 90cm alto para sem√°foro vertical
        size_world = {'vertical': (0.3, 0.9),
                     'horizontal': (0.9, 0.3),
                     'quad': (0.6, 0.6)}[tl_type]

        # Proyecci√≥n de tama√±o (aproximaci√≥n simple)
        focal_length = self.K[0,0]
        width_pixels = int((size_world[0] * focal_length) / distance)
        height_pixels = int((size_world[1] * focal_length) / distance)

        # Crear bbox
        x1 = u - width_pixels // 2
        y1 = v - height_pixels // 2
        x2 = u + width_pixels // 2
        y2 = v + height_pixels // 2

        return {
            'bbox': [x1, y1, x2, y2],
            'center': (u, v),
            'distance': distance,
            'visible': True
        }

    @staticmethod
    def build_transform_matrix(position, rotation):
        """
        Construye matriz de transformaci√≥n 4√ó4

        position: [x, y, z]
        rotation: [roll, pitch, yaw]
        """
        from scipy.spatial.transform import Rotation

        R = Rotation.from_euler('xyz', rotation).as_matrix()
        T = np.eye(4)
        T[:3, :3] = R
        T[:3, 3] = position

        return T

```

---

### 4.4 Implementaci√≥n Completa: Dynamic Projector

```python
# src/tlr/dynamic_projector.py (NUEVO M√ìDULO)

class DynamicProjector:
    """
    Sistema completo de projection boxes din√°micas
    Integra HD-Map + Localization + Projection 3D‚Üí2D
    """

    def __init__(self, hdmap_file: str, camera_calib_file: str):
        # Cargar HD-Map
        with open(hdmap_file) as f:
            self.hdmap = json.load(f)

        # Cargar calibraci√≥n
        with open(camera_calib_file) as f:
            camera_calib = json.load(f)

        # Inicializar projector
        self.projector = Projector3Dto2D(camera_calib)

        # Cache de sem√°foros cercanos
        self.nearby_lights_cache = []
        self.cache_position = None
        self.cache_radius = 50.0  # metros

    def get_projection_boxes(self, vehicle_pose: dict) -> List[dict]:
        """
        Genera projection boxes din√°micas para frame actual

        Args:
            vehicle_pose: Pose del veh√≠culo (de Localizer)

        Returns:
            Lista de projections:
            [
                {
                    'semantic_id': 'TL_001',
                    'bbox': [x1, y1, x2, y2],
                    'distance': float,
                    'orientation': str,
                    'visible': bool
                },
                ...
            ]
        """
        # Actualizar cache si veh√≠culo se movi√≥ significativamente
        if self._should_update_cache(vehicle_pose):
            self._update_nearby_lights(vehicle_pose)

        projections = []

        for tl in self.nearby_lights_cache:
            # Proyectar sem√°foro 3D ‚Üí 2D
            proj_result = self.projector.project_traffic_light(
                np.array(tl['position_3d']),
                vehicle_pose,
                tl['orientation']
            )

            if not proj_result['visible']:
                continue

            # Verificar que bbox est√° dentro de imagen
            bbox = proj_result['bbox']
            if not self._is_bbox_valid(bbox):
                continue

            projections.append({
                'semantic_id': tl['semantic_id'],
                'bbox': bbox,
                'distance': proj_result['distance'],
                'orientation': tl['orientation'],
                'visible': True
            })

        return projections

    def _should_update_cache(self, vehicle_pose: dict) -> bool:
        """Verifica si necesita actualizar cache de sem√°foros cercanos"""
        if self.cache_position is None:
            return True

        displacement = np.linalg.norm(
            np.array(vehicle_pose['position']) -
            np.array(self.cache_position)
        )

        return displacement > self.cache_radius * 0.5

    def _update_nearby_lights(self, vehicle_pose: dict):
        """Actualiza cache de sem√°foros cercanos"""
        vehicle_pos = np.array(vehicle_pose['position'])

        self.nearby_lights_cache = []

        for tl in self.hdmap['traffic_lights']:
            tl_pos = np.array(tl['position_3d'])
            distance = np.linalg.norm(tl_pos - vehicle_pos)

            if distance < self.cache_radius:
                self.nearby_lights_cache.append(tl)

        self.cache_position = vehicle_pose['position']

    def _is_bbox_valid(self, bbox: List[int]) -> bool:
        """Verifica que bbox est√° dentro de l√≠mites de imagen"""
        x1, y1, x2, y2 = bbox

        # Obtener dimensiones de imagen de calibraci√≥n
        img_w = self.projector.calib['image_width']
        img_h = self.projector.calib['image_height']

        # Verificar bounds
        if x2 < 0 or y2 < 0 or x1 > img_w or y1 > img_h:
            return False

        # Verificar tama√±o m√≠nimo
        if (x2 - x1) < 10 or (y2 - y1) < 10:
            return False

        return True

```

---

### 4.5 Integraci√≥n en Pipeline

```python
# src/tlr/pipeline.py (MODIFICADO)

class Pipeline(nn.Module):
    def __init__(self, detector, classifiers, ho, means_det, means_rec,
                 device=None, tracker=None,
                 dynamic_projector=None):  # NUEVO
        super().__init__()
        # ... (inicializaci√≥n existente) ...

        self.dynamic_projector = dynamic_projector

    def forward(self, img, boxes_or_pose, frame_ts=None):
        """
        Args:
            img: Imagen
            boxes_or_pose:
                - Si dynamic_projector=None: boxes est√°ticas
                - Si dynamic_projector!=None: vehicle_pose dict
            frame_ts: Timestamp
        """
        # NUEVO: Generar projection boxes din√°micas
        if self.dynamic_projector:
            vehicle_pose = boxes_or_pose  # Es un dict con pose
            projections_data = self.dynamic_projector.get_projection_boxes(vehicle_pose)

            # Convertir a formato boxes
            boxes = [[p['bbox'][0], p['bbox'][1], p['bbox'][2], p['bbox'][3],
                     p['semantic_id']] for p in projections_data]

            # Guardar semantic IDs para tracking
            self.semantic_ids = {i: p['semantic_id'] for i, p in enumerate(projections_data)}
        else:
            boxes = boxes_or_pose  # Es una lista de boxes est√°ticas
            self.semantic_ids = {i: i for i in range(len(boxes))}

        # Early exit
        if len(boxes) == 0:
            # ...

        # RESTO DEL PIPELINE IGUAL
        detections = self.detect(img, boxes)
        # ...

        # TRACKING con semantic IDs
        if self.tracker:
            # Usar semantic IDs en vez de row indices
            assigns_with_semantic_ids = [
                (self.semantic_ids[proj_idx], det_idx)
                for proj_idx, det_idx in assignments
            ]

            revised = self.tracker.track(
                frame_ts,
                assigns_with_semantic_ids,
                recognitions.cpu().tolist()
            )

        return valid_detections, recognitions, assignments, invalid_detections, revised

# Uso:
# Con projection boxes est√°ticas (actual)
pipeline = load_pipeline(device)
result = pipeline(img, static_boxes, frame_ts)

# Con projection boxes din√°micas (nuevo)
dynamic_proj = DynamicProjector('hdmap.json', 'camera_calib.json')
localizer = VehicleLocalizer()
pipeline = load_pipeline(device, dynamic_projector=dynamic_proj)

vehicle_pose = localizer.get_current_pose()
result = pipeline(img, vehicle_pose, frame_ts)

```

---

### 4.6 Testing Dynamic Projections

```python
# test_dynamic_projections.py

def test_projection_follows_vehicle_movement():
    """
    Test: Projection boxes actualizan con movimiento del veh√≠culo
    """
    # HD-Map con 1 sem√°foro fijo
    hdmap = {
        'traffic_lights': [{
            'semantic_id': 'TL_001',
            'position_3d': [100.0, 50.0, 5.0],  # Coordenadas mundo
            'orientation': 'vertical'
        }]
    }

    projector = DynamicProjector(hdmap, camera_calib)

    # Pose 1: Veh√≠culo en origen
    pose1 = {'position': [0, 0, 0], 'orientation': [0, 0, 0]}
    boxes1 = projector.get_projection_boxes(pose1)

    # Pose 2: Veh√≠culo avanz√≥ 10 metros
    pose2 = {'position': [10, 0, 0], 'orientation': [0, 0, 0]}
    boxes2 = projector.get_projection_boxes(pose2)

    # Verificar que bbox cambi√≥
    assert boxes1[0]['bbox'] != boxes2[0]['bbox'], "Bbox deber√≠a actualizarse"

    # Verificar que semantic ID se mantiene
    assert boxes1[0]['semantic_id'] == boxes2[0]['semantic_id'] == 'TL_001'

def test_semantic_id_persistence():
    """
    Test: Semantic IDs persisten a pesar de cambios espaciales
    """
    # 2 sem√°foros que intercambian posiciones visuales
    hdmap = {
        'traffic_lights': [
            {'semantic_id': 'TL_LEFT', 'position_3d': [100, -5, 5]},
            {'semantic_id': 'TL_RIGHT', 'position_3d': [100, 5, 5]}
        ]
    }

    projector = DynamicProjector(hdmap, camera_calib)
    tracker = TrafficLightTracker()

    # Pose inicial: TL_LEFT aparece a la izquierda en imagen
    pose1 = {'position': [0, 0, 0], 'orientation': [0, 0, 0]}
    boxes1 = projector.get_projection_boxes(pose1)

    # Inicializar tracking
    assignments1 = [(0, 0), (1, 1)]  # TL_LEFT‚Üídet0, TL_RIGHT‚Üídet1
    recognitions1 = [[0,0,0,1], [0,0,1,0]]  # GREEN, YELLOW
    revised1 = tracker.track(0.0, assignments1, recognitions1)

    # Pose nueva: Veh√≠culo gir√≥, ahora TL_LEFT aparece a la derecha
    pose2 = {'position': [0, 0, 0], 'orientation': [0, 0, np.pi]}  # Gir√≥ 180¬∞
    boxes2 = projector.get_projection_boxes(pose2)

    # Verificar: TL_LEFT sigue siendo TL_LEFT (aunque cambi√≥ posici√≥n visual)
    tl_left_box = [b for b in boxes2 if b['semantic_id'] == 'TL_LEFT'][0]

    # History deber√≠a seguir al semantic_id
    assert tracker.semantic.history['TL_LEFT'].color == 'green'
    # ‚úÖ No hay cross-history transfer porque ID es sem√°ntico, no espacial

```

---

### 4.7 Roadmap de Implementaci√≥n

### **Fase 1: Preparaci√≥n (1-2 semanas)**

**Tarea 1.1: Crear HD-Map Simplificado**

```python
# Usar Google Earth + GPS coordinates
# Para escenario de test actual (video doble_chico):

hdmap_test = {
    'traffic_lights': [
        {
            'semantic_id': 'TL_LEFT',
            'position_3d': [-34.603722, -58.381592, 25.5],  # GPS
            'orientation': 'quad',
            'lanes_controlled': ['lane_straight']
        },
        {
            'semantic_id': 'TL_RIGHT',
            'position_3d': [-34.603735, -58.381605, 25.5],
            'orientation': 'quad',
            'lanes_controlled': ['lane_straight']
        }
    ]
}

```

**Tarea 1.2: Calibrar C√°mara**

```bash
# Usar OpenCV calibration tool
python calibrate_camera.py --checkerboard_images ./calib_images/*.jpg
# Output: camera_calibration.json

```

**Tarea 1.3: Implementar Localizer Simulado**

```python
# Para testing sin GPS/IMU real
# Asumir veh√≠culo est√°tico o usar visual odometry
localizer = SimulatedLocalizer(initial_pose={'position': [0,0,0], ...})

```

---

### **Fase 2: Implementaci√≥n Core (2-3 semanas)**

**Tarea 2.1: Implementar Projector3Dto2D**

- Transformaciones world‚Üícamera‚Üípixel
- Manejo de distorsi√≥n de lente
- Estimaci√≥n de tama√±o en imagen

**Tarea 2.2: Implementar DynamicProjector**

- Carga de HD-Map
- Cache de sem√°foros cercanos
- Generaci√≥n de projection boxes

**Tarea 2.3: Modificar Pipeline**

- Aceptar vehicle_pose en vez de boxes est√°ticas
- Usar semantic IDs para tracking
- Backward compatibility con boxes est√°ticas

---

### **Fase 3: Testing y Validaci√≥n (1-2 semanas)**

**Test 1: Projection Accuracy**

```python
# Comparar projection boxes generadas vs ground truth manual
ground_truth_boxes = load_manual_boxes('frame_000001.jpg')
dynamic_boxes = projector.get_projection_boxes(vehicle_pose)

iou = compute_iou(ground_truth_boxes, dynamic_boxes)
assert iou > 0.8, "Projection accuracy insuficiente"

```

**Test 2: Semantic ID Persistence**

```python
# Verificar que semantic IDs no cambian con movimiento
# (test mostrado anteriormente)

```

**Test 3: Cross-History Transfer Fix**

```python
# Verificar que NO ocurre cross-history con semantic IDs
# (test mostrado anteriormente)

```

---

### **Fase 4: Deployment (1 semana)**

**Configuraci√≥n final**:

```python
# config.yaml
dynamic_projection:
  enabled: true
  hdmap_file: "maps/intersection_001.json"
  camera_calib: "calibration/front_6mm.json"
  localizer_type: "simulated"  # or "gps_imu" para producci√≥n

tracking:
  use_semantic_ids: true
  revise_time_s: 1.5
  blink_threshold_s: 0.55

```

**Pipeline final**:

```python
config = load_config('config.yaml')

if config['dynamic_projection']['enabled']:
    projector = DynamicProjector(
        config['dynamic_projection']['hdmap_file'],
        config['dynamic_projection']['camera_calib']
    )
    localizer = create_localizer(config['dynamic_projection']['localizer_type'])
else:
    projector = None
    localizer = None

pipeline = load_pipeline(device, dynamic_projector=projector)

# Loop de procesamiento
for frame in video:
    if projector:
        vehicle_pose = localizer.get_current_pose()
        result = pipeline(frame, vehicle_pose, frame_ts)
    else:
        static_boxes = load_boxes_from_file(frame_name)
        result = pipeline(frame, static_boxes, frame_ts)
```

---

## 5. üè∑Ô∏è Gap #4: ID Management (Row Index ‚Üí Semantic ID)

### 5.1 Problema Fundamental

### **Sistema Actual: Row Index como ID**

```python
# src/tlr/selector.py - Hungarian algorithm
def select_tls(ho, detections, projections, item_shape):
    costs = torch.zeros([len(projections), len(detections)])

    for row, projection in enumerate(projections):  # ‚Üê row = 0, 1, 2, ...
        for col, detection in enumerate(detections):
            costs[row, col] = calculate_score(...)

    assignments = ho.maximize(costs)
    # Resultado: [[row_idx, det_idx], [row_idx, det_idx], ...]
    #              ‚Üë
    #         Este row_idx se usa como proj_id en tracking

```

**En tracking**:

```python
# src/tlr/tracking.py
def update(self, frame_ts, assignments, recognitions):
    for proj_id, det_idx in assignments:  # proj_id = row_idx
        if proj_id not in self.history:
            self.history[proj_id] = SemanticTable(proj_id, ...)

        st = self.history[proj_id]  # ‚Üê Historial indexado por row_idx

```

**Consecuencia**:

```python
# projection_bboxes_master.txt
# Frame 1:
421,165,460,223,0  # row_idx=0, sem√°foro f√≠sico A
466,165,511,256,1  # row_idx=1, sem√°foro f√≠sico B

# Si en Frame 100 intercambiamos orden en archivo:
466,165,511,256,1  # row_idx=0 ‚Üê AHORA sem√°foro f√≠sico B
421,165,460,223,0  # row_idx=1 ‚Üê AHORA sem√°foro f√≠sico A

# Tracking:
history[0] = historial de lo que est√© en row_idx=0
# ‚ùå El historial se "transfiere" entre sem√°foros f√≠sicos

```

---

### **Apollo Original: Semantic ID Persistente**

```cpp
// Apollo's HD-Map based IDs
struct TrafficLight {
    string semantic_id;  // e.g., "TL_INTERSECTION_001_NORTH_LEFT"
    // Este ID:
    // - Viene del HD-Map
    // - Es √∫nico globalmente
    // - Persiste independientemente de orden o posici√≥n
};

// En tracking:
map<string, SemanticTable> history_;
// history_["TL_INTERSECTION_001_NORTH_LEFT"] = estado del sem√°foro f√≠sico espec√≠fico

// ‚úÖ El historial SIEMPRE sigue al mismo sem√°foro f√≠sico

```

**Ventaja cr√≠tica**:

```cpp
// Frame 1: Veh√≠culo ve sem√°foro desde lejos (aparece a la izquierda)
TL_NORTH_LEFT ‚Üí projection bbox (100, 50, 140, 120)
history_["TL_NORTH_LEFT"] = {color: "red", ...}

// Frame 100: Veh√≠culo gir√≥ (mismo sem√°foro ahora a la derecha)
TL_NORTH_LEFT ‚Üí projection bbox (800, 50, 840, 120)  // ‚Üê Cambi√≥ posici√≥n
history_["TL_NORTH_LEFT"] = {color: "red", ...}      // ‚Üê MISMO historial

// ‚úÖ No hay cross-history transfer porque ID es sem√°ntico, no espacial

```

---

### 5.2 An√°lisis del Gap

| Aspecto | Row Index (Actual) | Semantic ID (Apollo) |
| --- | --- | --- |
| **Definici√≥n** | √çndice en array de projections | ID √∫nico del HD-Map |
| **Persistencia** | ‚ùå Depende del orden en archivo | ‚úÖ Persistente entre frames |
| **Scope** | Local (por frame/video) | Global (toda la ciudad) |
| **Tracking** | Sigue posici√≥n espacial (region) | Sigue sem√°foro f√≠sico |
| **Cross-history** | ‚úÖ Puede ocurrir | ‚ùå No ocurre |
| **Requiere** | Nada (solo array) | HD-Map con IDs |
| **Debugging** | Dif√≠cil (n√∫meros sin significado) | F√°cil (nombres descriptivos) |

---

### 5.3 Por Qu√© Semantic IDs Son Cr√≠ticos

### **Raz√≥n 1: Tracking Robusto a Cambios de Vista**

```python
# Escenario: Veh√≠culo girando en intersecci√≥n

# Vista 1 (veh√≠culo mirando norte):
projections = [
    {'bbox': [100, 50, 140, 120], 'id': 'TL_NORTH_LEFT'},
    {'bbox': [800, 50, 840, 120], 'id': 'TL_NORTH_RIGHT'}
]

# Vista 2 (veh√≠culo gir√≥ 90¬∞, ahora mira este):
projections = [
    {'bbox': [100, 50, 140, 120], 'id': 'TL_EAST_LEFT'},   # ‚Üê Nuevo sem√°foro visible
    {'bbox': [800, 50, 840, 120], 'id': 'TL_NORTH_RIGHT'}  # ‚Üê Mismo de antes
]

# Con row_index:
# row=0 en vista 1 = TL_NORTH_LEFT
# row=0 en vista 2 = TL_EAST_LEFT
# ‚ùå history[0] se "reasigna" a sem√°foro diferente

# Con semantic_id:
# history['TL_NORTH_LEFT'] persiste (aunque ya no visible)
# history['TL_EAST_LEFT'] se crea nuevo
# ‚úÖ Cada sem√°foro mantiene su propio historial

```

---

### **Raz√≥n 2: Fusi√≥n Multi-C√°mara**

```python
# Apollo con m√∫ltiples c√°maras

# C√°mara frontal (6mm wide-angle):
projections_front = [
    {'id': 'TL_001', 'bbox': [100, 50, 140, 120], 'camera': 'front'}
]

# C√°mara telephoto (25mm):
projections_tele = [
    {'id': 'TL_001', 'bbox': [500, 300, 580, 420], 'camera': 'tele'}  # ‚Üê MISMO sem√°foro
]

# Fusi√≥n:
detections_front = detector(front_camera_image, projections_front)
detections_tele = detector(tele_camera_image, projections_tele)

# Ambas detecciones del MISMO sem√°foro (TL_001) se fusionan
# porque comparten semantic_id

# Con row_index:
# front: row=0
# tele: row=0
# ‚ùå Son dos "0" diferentes, no se puede fusionar

```

---

### **Raz√≥n 3: Debugging y An√°lisis**

```python
# Log con row_index (actual):
# Frame 100: proj_id=0 changed red‚Üígreen
# Frame 101: proj_id=1 blink detected
# ‚ùì ¬øCu√°l sem√°foro es el 0? ¬øEl 1? ¬øIzquierdo o derecho?

# Log con semantic_id (Apollo):
# Frame 100: TL_INTERSECTION_001_NORTH_LEFT changed red‚Üígreen
# Frame 101: TL_INTERSECTION_001_SOUTH_YELLOW blink detected
# ‚úÖ Inmediatamente se sabe qu√© sem√°foro es

```

---

### 5.4 Soluci√≥n: Migraci√≥n a Semantic IDs

### **Paso 1: Extender Formato de Projection Boxes**

```python
# projection_bboxes_master.txt (formato extendido)
# Antes:
# frame_000001.jpg 421,165,460,223,0 466,165,511,256,1

# Despu√©s (con semantic IDs):
# frame_000001.jpg 421,165,460,223,TL_LEFT 466,165,511,256,TL_RIGHT

# O en JSON para mayor flexibilidad:
# projection_boxes.json
{
    "frames": {
        "frame_000001.jpg": [
            {
                "semantic_id": "TL_LEFT",
                "bbox": [421, 165, 460, 223],
                "orientation": "quad",
                "expected_states": ["red", "yellow", "green"]
            },
            {
                "semantic_id": "TL_RIGHT",
                "bbox": [466, 165, 511, 256],
                "orientation": "quad",
                "expected_states": ["red", "yellow_blink"]
            }
        ]
    }
}

```

---

### **Paso 2: Modificar Selector para Usar Semantic IDs**

```python
# src/tlr/selector.py (MODIFICADO)

def select_tls_with_semantic_ids(ho, detections, projections_with_ids, item_shape):
    """
    Version con semantic IDs

    Args:
        projections_with_ids: Lista de dicts con 'semantic_id' y ProjectionROI
    """
    # Construir mapping
    semantic_id_to_idx = {p['semantic_id']: i for i, p in enumerate(projections_with_ids)}

    # Matriz de costos (igual que antes)
    costs = torch.zeros([len(projections_with_ids), len(detections)])

    for row, proj_data in enumerate(projections_with_ids):
        projection = proj_data['projection']
        for col, detection in enumerate(detections):
            # ... (c√°lculo de score igual) ...
            costs[row, col] = score

    # Hungarian assignment
    row_indices, col_indices = ho.maximize(costs.numpy())

    # Convertir row_indices a semantic_ids
    assignments_with_semantic_ids = []
    for row_idx, det_idx in zip(row_indices, col_indices):
        semantic_id = projections_with_ids[row_idx]['semantic_id']
        assignments_with_semantic_ids.append((semantic_id, det_idx))

    return assignments_with_semantic_ids
    # Retorna: [("TL_LEFT", 0), ("TL_RIGHT", 1), ...]

```

---

### **Paso 3: Modificar Tracking para Usar Semantic IDs**

```python
# src/tlr/tracking.py (MODIFICADO)

class SemanticTable:
    def __init__(self, semantic_id: str, time_stamp: float, color: str):
        self.semantic_id = semantic_id  # ‚Üê String ID en vez de int
        # ... (resto igual) ...

class SemanticDecision:
    def __init__(self, ...):
        # history indexado por semantic_id (string)
        self.history: Dict[str, SemanticTable] = {}  # ‚Üê Cambio de int a str

    def update(self, frame_ts, assignments, recognitions):
        results = {}

        for semantic_id, det_idx in assignments:  # ‚Üê semantic_id es string
            cls = int(max(range(len(recognitions[det_idx])),
                         key=lambda i: recognitions[det_idx][i]))
            color = ["black","red","yellow","green"][cls]

            # Obtener o crear historial
            if semantic_id not in self.history:
                self.history[semantic_id] = SemanticTable(semantic_id, frame_ts, color)

            st = self.history[semantic_id]

            # ... (l√≥gica de tracking igual) ...

            results[semantic_id] = (st.color, st.blink)

        return results
        # Retorna: {"TL_LEFT": ("green", False), "TL_RIGHT": ("red", True)}

```

---

### **Paso 4: Modificar Pipeline**

```python
# src/tlr/pipeline.py (MODIFICADO)

class Pipeline(nn.Module):
    def forward(self, img, boxes_with_ids, frame_ts=None):
        """
        Args:
            boxes_with_ids: Lista de dicts con 'semantic_id' y 'bbox'
                [
                    {'semantic_id': 'TL_LEFT', 'bbox': [x1,y1,x2,y2]},
                    {'semantic_id': 'TL_RIGHT', 'bbox': [x1,y1,x2,y2]}
                ]
        """
        # Convertir a formato interno
        projections_with_ids = []
        for box_data in boxes_with_ids:
            bbox = box_data['bbox']
            projection = ProjectionROI(bbox[0], bbox[1],
                                      bbox[2]-bbox[0], bbox[3]-bbox[1])
            projections_with_ids.append({
                'semantic_id': box_data['semantic_id'],
                'projection': projection
            })

        # Early exit
        if len(projections_with_ids) == 0:
            # ...

        # Detecci√≥n (igual)
        detections = self.detect(img, [p['projection'] for p in projections_with_ids])

        # Filtrado (igual)
        tl_types = torch.argmax(detections[:, 5:], dim=1)
        valid_mask = tl_types != 0
        valid_detections = detections[valid_mask]

        # Assignment CON semantic IDs
        assignments = select_tls_with_semantic_ids(
            self.ho, valid_detections, projections_with_ids, img.shape
        )
        # assignments = [("TL_LEFT", det_idx), ("TL_RIGHT", det_idx)]

        # Reconocimiento (igual)
        recognitions = self.recognize(img, valid_detections, tl_types[valid_mask])

        # Tracking CON semantic IDs
        if self.tracker:
            revised = self.tracker.track(
                frame_ts,
                assignments,  # Ya tienen semantic IDs
                recognitions.cpu().tolist()
            )
            # revised = {"TL_LEFT": ("green", False), "TL_RIGHT": ("red", True)}

        return valid_detections, recognitions, assignments, invalid_detections, revised

```

---

### 5.5 Migraci√≥n Gradual: Backward Compatibility

### **Dual Support: Row Index + Semantic ID**

```python
# src/tlr/utils.py (NUEVO)

def normalize_boxes_input(boxes_input):
    """
    Acepta boxes en m√∫ltiples formatos y normaliza a formato con semantic_ids

    Formatos aceptados:
    1. Lista antigua: [[x1,y1,x2,y2,id_num], ...]
    2. Lista con IDs string: [[x1,y1,x2,y2,"TL_001"], ...]
    3. Lista de dicts: [{'semantic_id': "TL_001", 'bbox': [x1,y1,x2,y2]}, ...]

    Returns:
        Lista de dicts con 'semantic_id' (string) y 'bbox'
    """
    if not boxes_input:
        return []

    # Detectar formato
    first_box = boxes_input[0]

    # Formato 3: Ya est√° normalizado
    if isinstance(first_box, dict) and 'semantic_id' in first_box:
        return boxes_input

    # Formato 1 o 2: Lista
    if isinstance(first_box, (list, tuple)):
        normalized = []
        for i, box in enumerate(boxes_input):
            x1, y1, x2, y2, box_id = box

            # Convertir ID a string
            if isinstance(box_id, (int, float)):
                semantic_id = f"proj_{int(box_id)}"  # Fallback: "proj_0", "proj_1"
            else:
                semantic_id = str(box_id)

            normalized.append({
                'semantic_id': semantic_id,
                'bbox': [x1, y1, x2, y2],
                '_original_index': i  # Para debugging
            })

        return normalized

    raise ValueError(f"Formato de boxes no reconocido: {type(first_box)}")

# Uso en pipeline:
def forward(self, img, boxes_input, frame_ts=None):
    # Normalizar input
    boxes_with_ids = normalize_boxes_input(boxes_input)

    # ... (resto del pipeline con semantic IDs) ...

```

---

### **Helper para Migraci√≥n de Archivos**

```python
# tools/migrate_to_semantic_ids.py (NUEVO SCRIPT)

import json

def migrate_boxes_file(old_file: str, output_file: str, id_mapping: dict = None):
    """
    Migra projection_bboxes_master.txt a formato con semantic IDs

    Args:
        old_file: projection_bboxes_master.txt (formato antiguo)
        output_file: projection_boxes.json (formato nuevo)
        id_mapping: Dict opcional {numeric_id: "semantic_id"}
                   Si None, genera IDs autom√°ticos
    """
    frames_data = {}

    with open(old_file) as f:
        for line in f:
            parts = line.strip().split()
            frame_name = parts[0]
            boxes_str = parts[1:]

            boxes = []
            for box_str in boxes_str:
                coords = list(map(int, box_str.split(',')))
                x1, y1, x2, y2, numeric_id = coords

                # Generar semantic_id
                if id_mapping and numeric_id in id_mapping:
                    semantic_id = id_mapping[numeric_id]
                else:
                    semantic_id = f"TL_{frame_name.split('.')[0]}_{numeric_id}"

                boxes.append({
                    'semantic_id': semantic_id,
                    'bbox': [x1, y1, x2, y2],
                    'orientation': 'unknown',  # Llenar manualmente despu√©s
                    '_migrated_from_id': numeric_id
                })

            frames_data[frame_name] = boxes

    # Guardar en JSON
    output_data = {'frames': frames_data}
    with open(output_file, 'w') as f:
        json.dump(output_data, f, indent=2)

    print(f"Migraci√≥n completada: {len(frames_data)} frames procesados")
    print(f"Output guardado en: {output_file}")

# Uso:
if __name__ == '__main__':
    # Opci√≥n 1: IDs autom√°ticos
    migrate_boxes_file(
        'projection_bboxes_master.txt',
        'projection_boxes.json'
    )

    # Opci√≥n 2: Con mapeo manual
    id_mapping = {
        0: "TL_INTERSECTION_001_LEFT",
        1: "TL_INTERSECTION_001_RIGHT"
    }
    migrate_boxes_file(
        'projection_bboxes_master.txt',
        'projection_boxes.json',
        id_mapping
    )

```

---

### 5.6 Testing Semantic IDs

```python
# test_semantic_ids.py

def test_semantic_id_persistence():
    """
    Test: Semantic IDs persisten entre frames
    """
    # Frame 1
    boxes1 = [
        {'semantic_id': 'TL_LEFT', 'bbox': [100, 50, 140, 120]},
        {'semantic_id': 'TL_RIGHT', 'bbox': [800, 50, 840, 120]}
    ]

    pipeline = load_pipeline(device)
    tracker = pipeline.tracker

    # Procesar frame 1
    img1 = load_image('frame_001.jpg')
    result1 = pipeline(img1, boxes1, frame_ts=0.0)

    # Frame 2: Boxes intercambiadas espacialmente
    boxes2 = [
        {'semantic_id': 'TL_RIGHT', 'bbox': [100, 50, 140, 120]},  # ‚Üê Cambi√≥ posici√≥n
        {'semantic_id': 'TL_LEFT', 'bbox': [800, 50, 840, 120]}    # ‚Üê Cambi√≥ posici√≥n
    ]

    img2 = load_image('frame_002.jpg')
    result2 = pipeline(img2, boxes2, frame_ts=0.033)

    # Verificar: Historiales siguen a semantic_id, NO a posici√≥n
    assert 'TL_LEFT' in tracker.semantic.history
    assert 'TL_RIGHT' in tracker.semantic.history

    # TL_LEFT deber√≠a tener su propio historial (independiente de posici√≥n)
    assert tracker.semantic.history['TL_LEFT'].semantic_id == 'TL_LEFT'
    assert tracker.semantic.history['TL_RIGHT'].semantic_id == 'TL_RIGHT'

def test_backward_compatibility():
    """
    Test: Sistema acepta formato antiguo y nuevo
    """
    # Formato antiguo (lista con numeric IDs)
    old_format = [
        [100, 50, 140, 120, 0],
        [800, 50, 840, 120, 1]
    ]

    # Formato nuevo (dicts con semantic IDs)
    new_format = [
        {'semantic_id': 'TL_LEFT', 'bbox': [100, 50, 140, 120]},
        {'semantic_id': 'TL_RIGHT', 'bbox': [800, 50, 840, 120]}
    ]

    pipeline = load_pipeline(device)
    img = load_image('frame_001.jpg')

    # Ambos formatos deber√≠an funcionar
    result_old = pipeline(img, old_format, frame_ts=0.0)
    result_new = pipeline(img, new_format, frame_ts=0.0)

    # Verificar que producen resultados equivalentes
    # (excepto por IDs: "proj_0" vs "TL_LEFT")
    assert len(result_old[0]) == len(result_new[0])  # Same detections

def test_cross_history_fix():
    """
    Test: Semantic IDs eliminan cross-history transfer
    """
    boxes = [
        {'semantic_id': 'TL_LEFT', 'bbox': [100, 50, 140, 120]},
        {'semantic_id': 'TL_RIGHT', 'bbox': [800, 50, 840, 120]}
    ]

    pipeline = load_pipeline(device)

    # Frame 1-100: TL_LEFT=green, TL_RIGHT=yellow_blink
    for i in range(100):
        img = create_frame_with_lights(left_color='green', right_color='yellow')
        _ = pipeline(img, boxes, frame_ts=i*0.033)

    # Verificar historiales
    assert pipeline.tracker.semantic.history['TL_LEFT'].color == 'green'
    assert pipeline.tracker.semantic.history['TL_RIGHT'].blink == True

    # Frame 101: Intercambio f√≠sico de sem√°foros
    # (simular con modificaci√≥n de detecciones)
    img_swapped = create_frame_with_lights(left_color='yellow', right_color='green')
    result = pipeline(img_swapped, boxes, frame_ts=101*0.033)

    # Con semantic IDs, cada sem√°foro mantiene su historial
    # TL_LEFT ahora ve yellow ‚Üí transici√≥n green‚Üíyellow (v√°lida)
    # TL_RIGHT ahora ve green ‚Üí blink se detiene (correcto)

    assert pipeline.tracker.semantic.history['TL_LEFT'].color == 'yellow'
    assert pipeline.tracker.semantic.history['TL_RIGHT'].blink == False
    # ‚úÖ No hay cross-history transfer

```

---

### 5.7 Roadmap de Implementaci√≥n

### **Fase 1: Preparaci√≥n (3-5 d√≠as)**

**D√≠a 1-2: Extender Formato de Datos**

```bash
# Crear nuevos archivos con semantic IDs
python tools/migrate_to_semantic_ids.py \
    --input projection_bboxes_master.txt \
    --output projection_boxes.json \
    --id-mapping id_mapping.yaml

```

**D√≠a 3: Implementar Normalizaci√≥n**

```python
# Implementar normalize_boxes_input() en utils.py
# Testing con ambos formatos

```

**D√≠a 4-5: Documentaci√≥n**

```markdown
# Actualizar README.md con nuevo formato
# Crear gu√≠a de migraci√≥n para usuarios

```

---

### **Fase 2: Modificaci√≥n de C√≥digo (1 semana)**

**D√≠a 1-2: Selector**

- Implementar¬†`select_tls_with_semantic_ids()`
- Testing unitario

**D√≠a 3-4: Tracking**

- Modificar¬†`SemanticTable`¬†y¬†`SemanticDecision`
- Cambiar¬†`Dict[int, ...]`¬†a¬†`Dict[str, ...]`
- Testing de persistencia

**D√≠a 5: Pipeline**

- Integrar semantic IDs en¬†`forward()`
- Backward compatibility con¬†`normalize_boxes_input()`

---

### **Fase 3: Testing y Validaci√≥n (3-5 d√≠as)**

**Test Suite**:

```bash
# Test 1: Persistence
pytest test_semantic_ids.py::test_semantic_id_persistence

# Test 2: Backward compatibility
pytest test_semantic_ids.py::test_backward_compatibility

# Test 3: Cross-history fix
pytest test_semantic_ids.py::test_cross_history_fix

# Test 4: Integration
pytest test_semantic_ids.py::test_full_pipeline_with_semantic_ids

```

---

### **Fase 4: Deployment (2-3 d√≠as)**

**Configuraci√≥n**:

```yaml
# config.yaml
projection_boxes:
  format: "semantic_ids"  # or "numeric_ids" for backward compat
  file: "projection_boxes.json"

tracking:
  use_semantic_ids: true
  history_backend: "dict"  # Future: "redis" for distributed

```

**Rollout**:

1. Deploy con backward compatibility activada
2. Migrar datos existentes
3. Monitorear logs para warnings de formato antiguo
4. Deprecar formato antiguo despu√©s de per√≠odo de transici√≥n

---

## 6. üß† Gap #6: Dependencia Espacial del Recognizer

### 6.1 Problema Descubierto

### **Comportamiento Observado**

```python
# Test de Swapping (experimento cr√≠tico)

# Configuraci√≥n normal (posiciones esperadas):
Det0 en posici√≥n (432,176,452,212):  # Izquierda
  ‚Üí Input: ROI de sem√°foro verde
  ‚Üí Output: [0.0, 0.0, 0.0, 1.0]  # GREEN ‚úÖ

Det1 en posici√≥n (476,175,501,247):  # Derecha
  ‚Üí Input: ROI de sem√°foro amarillo
  ‚Üí Output: [0.0, 0.0, 1.0, 0.0]  # YELLOW ‚úÖ

# Swap f√≠sico (intercambio de detecciones):
Det0 en posici√≥n (476,175,501,247):  # Derecha (intercambiado)
  ‚Üí Input: MISMOS P√çXELES de sem√°foro verde
  ‚Üí Output: [1.0, 0.0, 0.0, 0.0]  # BLACK ‚ùå (¬°cambi√≥!)

Det1 en posici√≥n (432,176,452,212):  # Izquierda (intercambiado)
  ‚Üí Input: MISMOS P√çXELES de sem√°foro amarillo
  ‚Üí Output: [1.0, 0.0, 0.0, 0.0]  # BLACK ‚ùå (¬°cambi√≥!)

```

**Hallazgo cr√≠tico**: El modelo NO clasific√≥ seg√∫n p√≠xeles, sino seg√∫n¬†**posici√≥n espacial**

---

### **Root Cause: Sobreajuste Espacial en Entrenamiento**

```python
# Dataset de entrenamiento (hip√≥tesis basada en comportamiento):
# Todos los ejemplos tienen estructura espacial fija:

training_data = [
    # Sem√°foro verde SIEMPRE en posici√≥n ~(432, 176)
    {'image': img1, 'bbox': [430, 175, 450, 210], 'label': 'GREEN'},
    {'image': img2, 'bbox': [432, 176, 452, 212], 'label': 'GREEN'},
    # ...

    # Sem√°foro amarillo SIEMPRE en posici√≥n ~(476, 175)
    {'image': img10, 'bbox': [475, 174, 500, 246], 'label': 'YELLOW'},
    {'image': img11, 'bbox': [476, 175, 501, 247], 'label': 'YELLOW'},
    # ...
]

# El modelo aprendi√≥ correlaci√≥n espuria:
# "Si bbox est√° en ~(432, 176) Y p√≠xeles muestran luz ‚Üí GREEN"
# "Si bbox est√° en ~(476, 175) Y p√≠xeles muestran luz ‚Üí YELLOW"
# "Si bbox NO est√° en posici√≥n esperada ‚Üí BLACK (desconocido)"

```

---

### 6.2 ¬øC√≥mo es Posible Esta Dependencia?

### **An√°lisis de la Arquitectura del Recognizer**

```python
# src/tlr/recognizer.py
class Recognizer(nn.Module):
    def forward(self, x):
        # x shape: [1, 3, H, W] donde H√óW depende del tipo
        # Para quad: [1, 3, 64, 64]

        conv1 = F.max_pool2d(F.relu(self.conv1(x)), kernel_size=3, stride=2, padding=1)
        # Shape: [1, 32, 32, 32]

        conv2 = F.max_pool2d(F.relu(self.conv2(conv1)), kernel_size=3, stride=2, padding=1)
        # Shape: [1, 64, 16, 16]

        conv3 = F.max_pool2d(F.relu(self.conv3(conv2)), kernel_size=3, stride=2, padding=1)
        # Shape: [1, 128, 8, 8]

        conv4 = F.max_pool2d(F.relu(self.conv4(conv3)), kernel_size=3, stride=2, padding=1)
        # Shape: [1, 128, 4, 4]

        conv5 = self.pool5(F.relu(self.conv5(conv4)))
        # pool5 = AvgPool2d(kernel_size=(4,4), stride=(4,4))
        # Shape: [1, 128, 1, 1]

        ft = F.relu(self.ft(conv5))
        # Shape: [1, 128, 1, 1]

        logits = self.logits(ft.reshape(-1, 128))
        # Shape: [1, 4]

        prob = F.softmax(logits, dim=1)
        return prob

```

**¬øD√≥nde est√° la informaci√≥n espacial?**

Teor√≠a 1:¬†**Pooling con informaci√≥n de posici√≥n**

```python
# pool5 par√°metros espec√≠ficos por tipo:
# quad:  kernel=(4,4), stride=(4,4) ‚Üí de 4√ó4 a 1√ó1
# hori:  kernel=(2,6), stride=(2,6) ‚Üí de 2√ó6 a 1√ó1
# vert:  kernel=(6,2), stride=(6,2) ‚Üí de 6√ó2 a 1√ó1

# Si el crop NO est√° perfectamente centrado en el sem√°foro,
# la posici√≥n del sem√°foro dentro del crop puede variar
# ‚Üí Pooling "recoge" diferentes activaciones seg√∫n d√≥nde est√© la luz

```

Teor√≠a 2:¬†**Preprocesamiento variable por posici√≥n**

```python
# src/tlr/tools/utils.py:241-252
def preprocess4rec(img, det_box, shape, means_rec):
    xl, xr, yt, yb = det_box[0], det_box[2], det_box[1], det_box[3]
    src = img[yt:yb, xl:xr]  # ‚Üê Crop usa coordenadas absolutas

    # Resize a tama√±o fijo
    dst = torch.zeros(shape, device=src.device)
    resized = ResizeGPU(src, dst, means_rec)
    return resized

# Problema potencial:
# Si resize introduce artifacts que correlacionan con posici√≥n original...
# O si means_rec fueron calculados con bias espacial...

```

Teor√≠a 3:¬†**Feature leakage en entrenamiento**

```python
# Durante entrenamiento, si el modelo tuvo acceso a metadata:
# - Coordenadas absolutas del bbox
# - ID de la imagen
# - Cualquier info correlacionada con posici√≥n

# Ejemplo de leakage accidental:
training_input = {
    'image_crop': cropped_image,
    'bbox_coords': [x1, y1, x2, y2],  # ‚Üê Si esto se pas√≥ al modelo
    'frame_id': 'frame_000123'
}

# El modelo podr√≠a usar bbox_coords para mejorar predicci√≥n
# ‚Üí Aprende "si x~430 ‚Üí verde, si x~476 ‚Üí amarillo"

```

---

### 6.3 Impacto del Problema

### **Escenarios Afectados**

**Escenario 1: Cambio de √Ångulo de C√°mara**

```python
# Video 1: C√°mara frontal
# Sem√°foro izq en (432, 176) ‚Üí GREEN ‚úÖ
# Sem√°foro der en (476, 175) ‚Üí YELLOW ‚úÖ

# Video 2: C√°mara con √°ngulo diferente (5¬∞ rotada)
# MISMO sem√°foro izq ahora en (450, 185) ‚Üí BLACK ‚ùå
# MISMO sem√°foro der ahora en (495, 190) ‚Üí BLACK ‚ùå

# Accuracy: 100% ‚Üí 0% solo por cambio de √°ngulo

```

**Escenario 2: Diferentes Tipos de Veh√≠culos**

```python
# Veh√≠culo bajo (sedan): C√°mara a 1.2m altura
# Sem√°foros aparecen en posiciones (432, 176) y (476, 175) ‚Üí OK ‚úÖ

# Veh√≠culo alto (SUV): C√°mara a 1.6m altura
# MISMOS sem√°foros aparecen en (432, 150) y (476, 145) ‚Üí FAIL ‚ùå

```

**Escenario 3: Nuevas Intersecciones**

```python
# Intersecci√≥n entrenamiento: Sem√°foros separados por 44 p√≠xeles
# Modelo aprendi√≥: "verde a la izquierda, amarillo a la derecha"

# Nueva intersecci√≥n: Sem√°foros separados por 200 p√≠xeles
# Posiciones: (200, 150) y (600, 150)
# Modelo: BLACK para ambos ‚ùå

```

---

### 6.4 ¬øPor Qu√© Apollo No Tiene Este Problema?

### **Dise√±o de Apollo: Position-Agnostic Training**

```cpp
// Apollo's training data preparation
void PrepareTrainingData() {
    for (auto &sample : dataset) {
        // 1. Detectar sem√°foro en imagen completa
        BBox traffic_light = DetectTrafficLight(sample.image);

        // 2. Crop con MARGIN VARIABLE (data augmentation espacial)
        int margin_x = random_uniform(-20, 20);  // ‚Üê Variaci√≥n espacial
        int margin_y = random_uniform(-20, 20);

        BBox crop_box = {
            traffic_light.x1 + margin_x,
            traffic_light.y1 + margin_y,
            traffic_light.x2 + margin_x,
            traffic_light.y2 + margin_y
        };

        // 3. Crop y resize
        cv::Mat cropped = CropAndResize(sample.image, crop_box, target_size);

        // 4. Agregar a dataset de entrenamiento
        training_samples.push_back({
            'image': cropped,
            'label': sample.ground_truth_color
        });
    }
}

```

**Caracter√≠sticas clave**:

- ‚úÖ Crops con offsets aleatorios (sem√°foro no siempre centrado)
- ‚úÖ M√∫ltiples escalas (zoom in/out)
- ‚úÖ Rotaciones ligeras
- ‚úÖ Solo p√≠xeles como input (sin coordenadas absolutas)

---

### 6.5 Soluci√≥n: Re-entrenamiento con Data Augmentation

### **Estrategia 1: Spatial Augmentation**

```python
# tools/retrain_recognizer.py

import torch
import torchvision.transforms as T
from torch.utils.data import Dataset, DataLoader

class TrafficLightDataset(Dataset):
    """
    Dataset con augmentation espacial para eliminar dependencia de posici√≥n
    """

    def __init__(self, images, labels, augment=True):
        self.images = images
        self.labels = labels
        self.augment = augment

        # Augmentation pipeline
        self.spatial_aug = T.Compose([
            # 1. Random crop (simula diferentes posiciones)
            T.RandomCrop(size=(64, 64), padding=8),

            # 2. Random affine (rotaci√≥n + traslaci√≥n + escala)
            T.RandomAffine(
                degrees=10,           # ¬±10¬∞ rotaci√≥n
                translate=(0.2, 0.2), # ¬±20% traslaci√≥n
                scale=(0.8, 1.2),     # 80%-120% escala
                shear=5               # ¬±5¬∞ shear
            ),

            # 3. Random horizontal flip (solo si sem√°foro sim√©trico)
            T.RandomHorizontalFlip(p=0.3),
        ])

        self.color_aug = T.Compose([
            # 4. Color jitter (robustez a iluminaci√≥n)
            T.ColorJitter(
                brightness=0.3,
                contrast=0.3,
                saturation=0.2,
                hue=0.1
            ),

            # 5. Random noise
            T.Lambda(lambda x: x + torch.randn_like(x) * 0.05),
        ])

    def __getitem__(self, idx):
        img = self.images[idx]
        label = self.labels[idx]

        if self.augment:
            # Aplicar augmentation espacial
            img = self.spatial_aug(img)
            img = self.color_aug(img)

        return img, label

    def __len__(self):
        return len(self.images)

# Training loop
def train_position_agnostic_recognizer(model, train_dataset, val_dataset, epochs=50):
    """
    Entrena recognizer SIN dependencia espacial
    """
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = torch.nn.CrossEntropyLoss()

    for epoch in range(epochs):
        model.train()
        train_loss = 0

        for images, labels in train_loader:
            optimizer.zero_grad()

            # Forward
            outputs = model(images)
            loss = criterion(outputs, labels)

            # Backward
            loss.backward()
            optimizer.step()

            train_loss += loss.item()

        # Validation
        model.eval()
        val_acc = evaluate_position_robustness(model, val_loader)

        print(f"Epoch {epoch}: train_loss={train_loss:.4f}, val_acc={val_acc:.4f}")

    return model

def evaluate_position_robustness(model, val_loader):
    """
    Eval√∫a robustez a cambios de posici√≥n
    """
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in val_loader:
            # Test 1: Posici√≥n original
            outputs = model(images)
            pred = torch.argmax(outputs, dim=1)
            correct += (pred == labels).sum().item()
            total += labels.size(0)

            # Test 2: Shifted positions (augmentation)
            for shift_x in [-10, 0, 10]:
                for shift_y in [-10, 0, 10]:
                    shifted = T.functional.affine(
                        images,
                        angle=0,
                        translate=[shift_x, shift_y],
                        scale=1.0,
                        shear=0
                    )
                    outputs_shifted = model(shifted)
                    pred_shifted = torch.argmax(outputs_shifted, dim=1)

                    # Verificar consistencia
                    if not torch.equal(pred, pred_shifted):
                        print(f"WARNING: Inconsistency at shift ({shift_x}, {shift_y})")

    return correct / total

```

---

### **Estrategia 2: Position Encoding Removal**

```python
# Verificar que el modelo NO recibe coordenadas como input

def audit_model_inputs(model, sample_batch):
    """
    Audita qu√© informaci√≥n recibe realmente el modelo
    """
    # Hook para capturar inputs
    inputs_captured = []

    def hook_fn(module, input, output):
        inputs_captured.append(input[0].shape)

    # Registrar hook en primera capa
    hook = model.conv1.register_forward_hook(hook_fn)

    # Forward pass
    _ = model(sample_batch)

    # Verificar shape
    input_shape = inputs_captured[0]
    print(f"Model input shape: {input_shape}")

    # Deber√≠a ser: [batch, 3, H, W] (solo p√≠xeles)
    # NO deber√≠a ser: [batch, 5, H, W] (con coords) o similar
    assert input_shape[1] == 3, f"Model receives {input_shape[1]} channels, expected 3 (RGB only)"

    hook.remove()
    print("‚úÖ Model only receives pixel data (no position info)")

# Ejecutar audit
audit_model_inputs(quad_recognizer, torch.randn(1, 3, 64, 64))

```

---

### **Estrategia 3: Curriculum Learning con Posiciones Variadas**

```python
# Progressive training: f√°cil ‚Üí dif√≠cil

def curriculum_training(model, dataset, curriculum_stages=3):
    """
    Entrena con dificultad creciente en variaci√≥n espacial
    """

    # Stage 1: Sin variaci√≥n espacial (baseline)
    print("Stage 1: Original positions only")
    stage1_dataset = TrafficLightDataset(
        dataset.images,
        dataset.labels,
        augment=False  # Sin augmentation
    )
    train_position_agnostic_recognizer(model, stage1_dataset, epochs=20)

    # Stage 2: Variaci√≥n leve (¬±10 p√≠xeles)
    print("Stage 2: Light spatial variation")
    stage2_aug = T.RandomAffine(degrees=5, translate=(0.1, 0.1))
    stage2_dataset = TrafficLightDataset(
        dataset.images,
        dataset.labels,
        augment=True,
        custom_aug=stage2_aug
    )
    train_position_agnostic_recognizer(model, stage2_dataset, epochs=15)

    # Stage 3: Variaci√≥n fuerte (¬±30 p√≠xeles, rotaci√≥n)
    print("Stage 3: Strong spatial variation")
    stage3_aug = T.RandomAffine(degrees=15, translate=(0.3, 0.3), scale=(0.7, 1.3))
    stage3_dataset = TrafficLightDataset(
        dataset.images,
        dataset.labels,
        augment=True,
        custom_aug=stage3_aug
    )
    train_position_agnostic_recognizer(model, stage3_dataset, epochs=15)

    return model

```

---

### 6.6 Soluci√≥n Alternativa: Feature Normalization

Si re-entrenar no es posible, aplicar normalizaci√≥n espacial:

```python
# src/tlr/recognizer_wrapper.py (NUEVO)

class PositionNormalizedRecognizer(nn.Module):
    """
    Wrapper que normaliza features espaciales antes de clasificaci√≥n
    """

    def __init__(self, base_recognizer):
        super().__init__()
        self.base_recognizer = base_recognizer

        # Spatial Transformer Network para normalizaci√≥n
        self.stn = SpatialTransformerNetwork()

    def forward(self, x):
        # 1. Normalizar posici√≥n con STN
        x_normalized = self.stn(x)

        # 2. Clasificar con recognizer original
        output = self.base_recognizer(x_normalized)

        return output

class SpatialTransformerNetwork(nn.Module):
    """
    Red que aprende a normalizar posici√≥n del sem√°foro en el crop
    """

    def __init__(self):
        super().__init__()

        # Localization network (aprende transformaci√≥n)
        self.localization = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=7),
            nn.MaxPool2d(2, stride=2),
            nn.ReLU(True),
            nn.Conv2d(16, 32, kernel_size=5),
            nn.MaxPool2d(2, stride=2),
            nn.ReLU(True)
        )

        # Regressor para par√°metros de transformaci√≥n
        self.fc_loc = nn.Sequential(
            nn.Linear(32 * 12 * 12, 128),
            nn.ReLU(True),
            nn.Linear(128, 6)  # Affine transform: 2√ó3 matrix
        )

        # Inicializar con identidad
        self.fc_loc[-1].weight.data.zero_()
        self.fc_loc[-1].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))

    def forward(self, x):
        # 1. Localizar sem√°foro
        xs = self.localization(x)
        xs = xs.view(-1, 32 * 12 * 12)

        # 2. Predecir transformaci√≥n
        theta = self.fc_loc(xs)
        theta = theta.view(-1, 2, 3)

        # 3. Aplicar transformaci√≥n (centra sem√°foro)
        grid = F.affine_grid(theta, x.size(), align_corners=False)
        x_normalized = F.grid_sample(x, grid, align_corners=False)

        return x_normalized

# Uso:
quad_recognizer_original = Recognizer(quad_pool_params)
quad_recognizer_original.load_state_dict(torch.load('quad.torch'))

# Wrap con normalizaci√≥n
quad_recognizer_normalized = PositionNormalizedRecognizer(quad_recognizer_original)

# Entrenar SOLO el STN (freeze recognizer)
for param in quad_recognizer_normalized.base_recognizer.parameters():
    param.requires_grad = False

# Train STN para aprender a centrar sem√°foros
optimizer = torch.optim.Adam(quad_recognizer_normalized.stn.parameters(), lr=1e-3)
# ...

```

---

### 6.7 Testing y Validaci√≥n

```python
# test_position_robustness.py

def test_spatial_invariance():
    """
    Test: Recognizer clasifica igual independiente de posici√≥n
    """
    model = load_recognizer('quad_retrained.torch')

    # Imagen base con sem√°foro verde
    base_image = create_traffic_light_image(color='green', position='center')

    # Test en m√∫ltiples posiciones
    positions = [
        ('top-left', -20, -20),
        ('top-right', 20, -20),
        ('center', 0, 0),
        ('bottom-left', -20, 20),
        ('bottom-right', 20, 20)
    ]

    results = []
    for pos_name, shift_x, shift_y in positions:
        # Shift image
        shifted = shift_image(base_image, shift_x, shift_y)

        # Classify
        output = model(shifted)
        pred_class = torch.argmax(output)

        results.append((pos_name, pred_class))
        print(f"{pos_name}: {['BLACK','RED','YELLOW','GREEN'][pred_class]}")

    # Verificar consistencia
    predictions = [r[1] for r in results]
    assert all(p == predictions[0] for p in predictions), \
        f"Inconsistent predictions across positions: {results}"

    print("‚úÖ Model is spatially invariant")

def test_swapping_robustness():
    """
    Test: Swapping ya NO causa clasificaci√≥n err√≥nea
    """
    model = load_recognizer('quad_retrained.torch')

    # Sem√°foro verde en posici√≥n izquierda
    green_left = create_light_at_position(color='green', x=432, y=176)
    output1 = model(green_left)

    # MISMO sem√°foro verde en posici√≥n derecha
    green_right = create_light_at_position(color='green', x=476, y=175)
    output2 = model(green_right)

    # Deber√≠an clasificar igual
    pred1 = torch.argmax(output1)
    pred2 = torch.argmax(output2)

    assert pred1 == pred2 == 3, f"GREEN classification failed: {pred1}, {pred2}"
    print("‚úÖ Swapping test passed")

def benchmark_position_robustness(model, test_dataset):
    """
    Benchmark: Accuracy en m√∫ltiples posiciones
    """
    shifts = [
        (0, 0),      # Original
        (-20, 0),    # Left
        (20, 0),     # Right
        (0, -20),    # Up
        (0, 20),     # Down
        (-10, -10),  # Diagonal
        (10, 10)
    ]

    results = {}

    for shift_x, shift_y in shifts:
        correct = 0
        total = 0

        for img, label in test_dataset:
            shifted = shift_image(img, shift_x, shift_y)
            output = model(shifted)
            pred = torch.argmax(output)

            if pred == label:
                correct += 1
            total += 1

        accuracy = correct / total
        results[(shift_x, shift_y)] = accuracy
        print(f"Shift ({shift_x:3}, {shift_y:3}): {accuracy:.2%}")

    # Calcular varianza (deber√≠a ser baja)
    accuracies = list(results.values())
    variance = np.var(accuracies)

    print(f"\nAccuracy variance: {variance:.4f}")
    print(f"Min accuracy: {min(accuracies):.2%}")
    print(f"Max accuracy: {max(accuracies):.2%}")

    # Criterio: Varianza < 0.01, Min accuracy > 90%
    assert variance < 0.01, f"High variance: {variance}"
    assert min(accuracies) > 0.90, f"Low min accuracy: {min(accuracies)}"

    print("‚úÖ Position robustness benchmark passed")

```

---

### 6.8 Roadmap de Implementaci√≥n

### **Opci√≥n A: Re-entrenamiento Completo (Recomendado)**

**Fase 1: Data Collection (2-3 semanas)**

```python
# 1. Expandir dataset con variaci√≥n espacial
# - Capturar mismo sem√°foro desde m√∫ltiples √°ngulos
# - Diferentes alturas de c√°mara
# - Diferentes distancias

# 2. Synthetic data generation
# - Generar crops con offsets aleatorios
# - Rotaciones, escalas, traslaciones

```

**Fase 2: Training (1-2 semanas)**

```bash
# Entrenar con spatial augmentation
python tools/retrain_recognizer.py \
    --dataset augmented_traffic_lights/ \
    --augment spatial \
    --epochs 100 \
    --model quad

# Validar robustez
python tools/validate_position_robustness.py \
    --model quad_retrained.torch

```

**Fase 3: Deployment (1 semana)**

```python
# Reemplazar modelos
quad_recognizer.load_state_dict(torch.load('quad_retrained.torch'))
hori_recognizer.load_state_dict(torch.load('hori_retrained.torch'))
vert_recognizer.load_state_dict(torch.load('vert_retrained.torch'))

```

---

### **Opci√≥n B: STN Wrapper (Quick Fix - 1 semana)**

**D√≠a 1-3: Implementar STN**

```python
# Implementar PositionNormalizedRecognizer
# Test unitario con synthetic data

```

**D√≠a 4-5: Train STN**

```python
# Entrenar SOLO el STN (freeze recognizer)
# Dataset: pares (desalineado, alineado)

```

**D√≠a 6-7: Integration y Testing**

```python
# Integrar en pipeline
# Testing de robustez
```

---

## 7. üó∫Ô∏è Roadmap de Implementaci√≥n Completo

### 7.1 Estrategia de Implementaci√≥n

### **Enfoque: Incremental & Validated**

```
Principio: Cada gap se cierra de forma independiente y se valida antes de continuar

Gap 1 (Selection) ‚Üí Test ‚Üí ‚úÖ
    ‚Üì
Gap 2 (Multi-detection) ‚Üí Test ‚Üí ‚úÖ
    ‚Üì
Gap 3 (Dynamic Projections) ‚Üí Test ‚Üí ‚úÖ
    ‚Üì
Gap 4 (Semantic IDs) ‚Üí Test ‚Üí ‚úÖ
    ‚Üì
Gap 5 (Spatial Dependency) ‚Üí Test ‚Üí ‚úÖ
    ‚Üì
Integration Testing ‚Üí ‚úÖ
    ‚Üì
Production Deployment

```

---

### 7.2 Priorizaci√≥n por Impacto

| Gap | Impacto Funcional | Complejidad | Prioridad | Duraci√≥n Estimada |
| --- | --- | --- | --- | --- |
| **Gap #1: Selection Algorithm** | üî¥ Cr√≠tico | üü° Media | **P0** | 1-2 semanas |
| **Gap #4: Semantic IDs** | üü† Alto | üü¢ Baja | **P0** | 3-5 d√≠as |
| **Gap #2: Multi-Detection** | üî¥ Cr√≠tico | üü° Media | **P1** | 1-2 semanas |
| **Gap #6: Spatial Dependency** | üî¥ Cr√≠tico | üî¥ Alta | **P1** | 2-4 semanas |
| **Gap #3: Dynamic Projections** | üî¥ Cr√≠tico | üî¥ Muy Alta | **P2** | 1-3 meses |
| **Gap #5: Multi-Camera** | üü° Medio | üî¥ Muy Alta | **P3** | 2-3 meses |

---

### 7.3 Roadmap Detallado por Fase

---

## üìÖ FASE 1: Quick Wins (Semanas 1-3)

**Objetivo**: Implementar mejoras de alto impacto y baja complejidad

---

### Semana 1: Gap #4 - Semantic IDs

**D√≠a 1-2: Preparaci√≥n**

```bash
# Crear herramienta de migraci√≥n
git checkout -b feature/semantic-ids

# Implementar migraci√≥n de datos
python tools/migrate_to_semantic_ids.py \
    --input projection_bboxes_master.txt \
    --output projection_boxes.json \
    --id-mapping configs/id_mapping.yaml

```

**D√≠a 3-4: C√≥digo Core**

```python
# src/tlr/utils.py
- Implementar normalize_boxes_input()
- Testing con formatos antiguos y nuevos

# src/tlr/tracking.py
- Cambiar Dict[int, ...] ‚Üí Dict[str, ...]
- Actualizar SemanticTable

# src/tlr/selector.py
- Implementar select_tls_with_semantic_ids()

```

**D√≠a 5: Testing**

```bash
pytest tests/test_semantic_ids.py -v
pytest tests/test_backward_compatibility.py -v
pytest tests/test_cross_history_fix.py -v

```

**Entregables**:

- ‚úÖ Sistema acepta ambos formatos (backward compatible)
- ‚úÖ Semantic IDs funcionando en tracking
- ‚úÖ Cross-history transfer eliminado
- ‚úÖ Tests pasando

---

### Semana 2-3: Gap #1 - Selection Algorithm

**Semana 2, D√≠a 1-3: Implementaci√≥n Apollo Selector**

```python
# src/tlr/apollo_selector.py (NUEVO)
- Implementar SelectionCriteria class
- Implementar ApolloSelector class
- 4 m√©tricas: detection, spatial, shape, temporal

# Tests unitarios
- test_selection_criteria()
- test_score_calculation()

```

**Semana 2, D√≠a 4-5: Integraci√≥n Pipeline**

```python
# src/tlr/pipeline.py
- Agregar flag use_apollo_selector
- Integrar ApolloSelector en forward()
- Mantener Hungarian para backward compat

```

**Semana 3, D√≠a 1-2: Testing Comparativo**

```python
# tests/test_apollo_vs_hungarian.py
def test_multiple_detections_same_light():
    # Hungarian: Solo asigna 1
    # Apollo: Selecciona mejor
    assert len(apollo_assignments) == 1
    assert apollo_assignments[0] == best_detection

def test_temporal_consistency():
    # Apollo usa temporal_score
    # Hungarian no
    assert apollo_selected_consistent_detection

```

**Semana 3, D√≠a 3-5: Validation & Deployment**

```bash
# Correr pipeline completo con ambos
python run_pipeline_comparison.py \
    --selector hungarian \
    --selector apollo \
    --compare-outputs

# An√°lisis de resultados
python analyze_selector_performance.py

```

**Entregables**:

- ‚úÖ Apollo Selection Algorithm implementado
- ‚úÖ Tests mostrando mejora vs Hungarian
- ‚úÖ Backward compatible (flag configurable)
- ‚úÖ Documentaci√≥n de diferencias

---

## üìÖ FASE 2: Core Improvements (Semanas 4-7)

**Objetivo**: Cerrar gaps funcionales cr√≠ticos

---

### Semana 4-5: Gap #2 - M√∫ltiples Detections por ROI

**Semana 4, D√≠a 1-2: Dise√±o**

```python
# Decidir estrategia:
# Opci√≥n A: Split projection boxes (Quick)
# Opci√≥n B: Detecci√≥n iterativa (Medium)
# Opci√≥n C: HD-Map integration (completo, pero requiere Fase 3)

# Decisi√≥n: Implementar B (detecci√≥n iterativa)

```

**Semana 4, D√≠a 3-5: Implementaci√≥n**

```python
# src/tlr/pipeline.py
def detect_multi(self, image, boxes):
    """
    Permite m√∫ltiples detections por projection
    Mantiene mapeo det_idx ‚Üí proj_id
    """
    all_detections = []
    detection_to_projection = []

    for proj_id, box in enumerate(boxes):
        # Detectar en esta ROI
        detections = self.detect_in_roi(image, box)

        for det in detections:
            all_detections.append(det)
            detection_to_projection.append(proj_id)

    # NMS global
    # ...

    return all_detections, detection_to_projection

```

**Semana 5, D√≠a 1-3: Integraci√≥n con Selection**

```python
# Modificar ApolloSelector para considerar mapeo
def select(self, detections, projections, det_to_proj_map, ...):
    # Solo considerar detections que vienen de ROI correcta
    # ...

```

**Semana 5, D√≠a 4-5: Testing**

```bash
pytest tests/test_multi_detection.py::test_two_lights_same_roi
pytest tests/test_multi_detection.py::test_detection_mapping
pytest tests/test_integration_selection_multi.py

```

**Entregables**:

- ‚úÖ Sistema maneja N detections por ROI
- ‚úÖ Selection algorithm usa mapeo correcto
- ‚úÖ Tests con casos de m√∫ltiples sem√°foros
- ‚úÖ No regresiones en casos simples

---

### Semana 6-7: Gap #6 - Dependencia Espacial (Opci√≥n STN)

**Nota**: Re-entrenamiento completo requiere Fase 3. STN es quick fix.

**Semana 6, D√≠a 1-3: Implementar STN**

```python
# src/tlr/recognizer_wrapper.py (NUEVO)
class PositionNormalizedRecognizer(nn.Module):
    def __init__(self, base_recognizer):
        self.base_recognizer = base_recognizer
        self.stn = SpatialTransformerNetwork()

    def forward(self, x):
        x_normalized = self.stn(x)
        return self.base_recognizer(x_normalized)

# Implementar SpatialTransformerNetwork

```

**Semana 6, D√≠a 4-5: Training STN**

```python
# Crear dataset de pares (desalineado, ground_truth)
# Entrenar SOLO STN (freeze recognizer)

python tools/train_stn.py \
    --base-model quad.torch \
    --dataset stn_training_data/ \
    --epochs 50

```

**Semana 7, D√≠a 1-2: Integration**

```python
# src/tlr/pipeline.py
# Reemplazar recognizers con wrapped versions
self.quad_recognizer = PositionNormalizedRecognizer(
    quad_recognizer_base
)

```

**Semana 7, D√≠a 3-5: Validation**

```bash
# Test de robustez espacial
pytest tests/test_position_robustness.py::test_spatial_invariance
pytest tests/test_position_robustness.py::test_swapping_robustness

# Benchmark
python tools/benchmark_position_robustness.py \
    --model quad_stn.torch \
    --test-shifts -30 -20 -10 0 10 20 30

```

**Entregables**:

- ‚úÖ STN funcionando y normalizando posici√≥n
- ‚úÖ Swapping test pasa (sin BLACK falso)
- ‚úÖ Accuracy estable en m√∫ltiples posiciones
- ‚úÖ Performance no degradada (<10% overhead)

---

## üìÖ FASE 3: Apollo-Level Features (Semanas 8-20)

**Objetivo**: Implementar features completas de Apollo

---

### Semana 8-11: Gap #3 - Projection Boxes Din√°micas

**Semana 8: Preparaci√≥n Infraestructura**

**D√≠a 1-2: HD-Map Creation**

```bash
# Opci√≥n 1: Manual (para escenario test)
# - Usar Google Earth para obtener coords GPS
# - Crear hdmap_test.json manualmente

# Opci√≥n 2: Semi-autom√°tico
# - Capturar video con GPS logger
# - Marcar sem√°foros manualmente
# - Script genera HD-Map

python tools/create_hdmap.py \
    --video test_video.mp4 \
    --gps-log gps_data.csv \
    --output hdmap_test.json

```

**D√≠a 3-4: Camera Calibration**

```bash
# Calibrar c√°mara con checkerboard
python tools/calibrate_camera.py \
    --images calibration_images/*.jpg \
    --pattern-size 9x6 \
    --square-size 0.025 \
    --output camera_calib.json

# Validar calibraci√≥n
python tools/validate_calibration.py \
    --calib camera_calib.json \
    --test-images test_calib/*.jpg

```

**D√≠a 5: Localization Setup**

```python
# Implementar SimulatedLocalizer para testing
# (GPS real requiere hardware adicional)

# src/tlr/localization.py
class SimulatedLocalizer:
    def __init__(self, trajectory_file):
        # Cargar trayectoria pre-grabada
        self.trajectory = load_trajectory(trajectory_file)

    def get_current_pose(self, timestamp):
        # Interpolar pose en timestamp
        return interpolate_pose(self.trajectory, timestamp)

```

**Semana 9-10: Implementaci√≥n Core**

**D√≠a 1-3: Projector 3D‚Üí2D**

```python
# src/tlr/projection_3d_to_2d.py
- Implementar Projector3Dto2D
- Transformaciones world‚Üícamera‚Üípixel
- Manejo de distorsi√≥n de lente
- Tests unitarios con casos conocidos

```

**D√≠a 4-7: Dynamic Projector**

```python
# src/tlr/dynamic_projector.py
- Implementar DynamicProjector
- Carga de HD-Map
- Cache de sem√°foros cercanos
- Generaci√≥n de projection boxes

# Tests
- test_projection_accuracy()
- test_cache_updates()
- test_out_of_view_filtering()

```

**D√≠a 8-10: Pipeline Integration**

```python
# src/tlr/pipeline.py
- Modificar forward() para aceptar vehicle_pose
- Usar semantic_ids del HD-Map
- Backward compatibility con boxes est√°ticas

```

**Semana 11: Testing & Validation**

```bash
# Test 1: Projection accuracy
python tests/test_dynamic_projections.py::test_projection_vs_ground_truth

# Test 2: Semantic ID persistence
python tests/test_dynamic_projections.py::test_semantic_id_persistence

# Test 3: Cross-history fix
python tests/test_dynamic_projections.py::test_no_cross_history_with_dynamic

# Integration test
python run_pipeline_with_dynamic_projections.py \
    --video test_video.mp4 \
    --hdmap hdmap_test.json \
    --calib camera_calib.json \
    --localization simulated

```

**Entregables**:

- ‚úÖ HD-Map del escenario de prueba
- ‚úÖ Calibraci√≥n de c√°mara validada
- ‚úÖ Dynamic projector funcionando
- ‚úÖ Semantic IDs persistentes (no cross-history)
- ‚úÖ Pipeline integrado con pose tracking

---

### Semana 12-16: Gap #6 - Dependencia Espacial (Re-entrenamiento)

**Semana 12-13: Data Collection & Preparation**

**Tarea 1: Expandir Dataset**

```python
# Capturar nuevo data con variaci√≥n espacial
# - Mismos sem√°foros desde m√∫ltiples √°ngulos
# - Diferentes alturas de c√°mara (sedan, SUV, truck)
# - Diferentes distancias (5m, 20m, 50m, 100m)

# Meta: 10,000+ samples con diversidad espacial

```

**Tarea 2: Synthetic Augmentation**

```python
# tools/generate_augmented_dataset.py

def augment_dataset(original_dataset, output_dir):
    for img, label in original_dataset:
        # Generar 20 variaciones por imagen
        for i in range(20):
            # Random spatial transform
            augmented = apply_random_transform(
                img,
                shift_range=(-30, 30),
                rotation_range=(-15, 15),
                scale_range=(0.7, 1.3)
            )

            save(augmented, label, f"{output_dir}/{img_id}_{i}.jpg")

# Resultado: 200,000+ samples augmented

```

**Semana 14-15: Training**

```bash
# Train recognizers con spatial augmentation
for model in quad hori vert; do
    python tools/train_recognizer.py \
        --model $model \
        --dataset augmented_traffic_lights/ \
        --augment spatial \
        --epochs 100 \
        --batch-size 64 \
        --lr 1e-4 \
        --output ${model}_spatially_robust.torch
done

# Validaci√≥n continua
python tools/validate_during_training.py \
    --watch-dir checkpoints/ \
    --test-suite position_robustness

```

**Semana 16: Validation & Deployment**

```bash
# Benchmark completo
python tools/benchmark_recognizers.py \
    --old-models quad.torch hori.torch vert.torch \
    --new-models quad_robust.torch hori_robust.torch vert_robust.torch \
    --test-suite comprehensive

# Comparison report
# - Accuracy en posiciones originales
# - Accuracy en posiciones shifted
# - Varianza entre posiciones
# - Casos edge (rotaciones extremas)

# Si mejora > 20% en robustez Y mantiene accuracy:
# ‚Üí Deploy nuevos modelos

```

**Entregables**:

- ‚úÖ Dataset augmented (200K+ samples)
- ‚úÖ Recognizers re-entrenados
- ‚úÖ Benchmark mostrando mejora en robustez
- ‚úÖ Sin degradaci√≥n en accuracy base
- ‚úÖ Swapping test pasa consistentemente

---

### Semana 17-20: Integration Testing & Refinement

**Semana 17: System Integration**

```bash
# Integrar TODOS los gaps implementados
# Gap #1: Apollo Selection ‚úÖ
# Gap #2: Multi-detection ‚úÖ
# Gap #3: Dynamic Projections ‚úÖ
# Gap #4: Semantic IDs ‚úÖ
# Gap #6: Spatial robustness ‚úÖ

# Pipeline completo Apollo-equivalent
python run_full_apollo_pipeline.py \
    --video test_suite/*.mp4 \
    --hdmap maps/*.json \
    --config config_apollo_mode.yaml

```

**Semana 18: Performance Optimization**

```python
# Profiling
python -m cProfile run_full_apollo_pipeline.py > profile.txt

# Identificar bottlenecks
# - Selection algorithm: O(N¬≥) ‚Üí implementar optimizaciones
# - Dynamic projections: cache agresivo
# - STN overhead: considerar TensorRT

# Optimizaciones
- Batch processing donde sea posible
- GPU acceleration para Hungarian (si disponible)
- Compiled models (TorchScript)

```

**Semana 19: Stress Testing**

```bash
# Test 1: Intersecciones complejas (10+ sem√°foros)
python tests/test_complex_intersection.py

# Test 2: Long videos (1000+ frames)
python tests/test_long_video_memory.py

# Test 3: Edge cases
python tests/test_edge_cases.py
# - Sem√°foros muy cercanos
# - Oclusiones temporales
# - Cambios de iluminaci√≥n extremos
# - Movimiento r√°pido de veh√≠culo

```

**Semana 20: Documentation & Handoff**

```markdown
# Crear documentaci√≥n completa
docs/
  ‚îú‚îÄ‚îÄ architecture_apollo_mode.md
  ‚îú‚îÄ‚îÄ api_reference.md
  ‚îú‚îÄ‚îÄ configuration_guide.md
  ‚îú‚îÄ‚îÄ troubleshooting.md
  ‚îî‚îÄ‚îÄ migration_guide.md

# Training materials
training/
  ‚îú‚îÄ‚îÄ setup_guide.md
  ‚îú‚îÄ‚îÄ hdmap_creation_tutorial.md
  ‚îú‚îÄ‚îÄ camera_calibration_guide.md
  ‚îî‚îÄ‚îÄ video_walkthrough.mp4

```

**Entregables Fase 3**:

- ‚úÖ Sistema Apollo-equivalent completo
- ‚úÖ Todos los gaps cerrados
- ‚úÖ Performance optimizado
- ‚úÖ Testing exhaustivo
- ‚úÖ Documentaci√≥n completa

---

## üìÖ FASE 4: Production Readiness (Opcional - Semanas 21-24)

### Semana 21-22: Deployment Infrastructure

**Containerization**

```docker
# Dockerfile
FROM pytorch/pytorch:2.0-cuda11.8

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY src/ ./src/
COPY configs/ ./configs/

CMD ["python", "run_apollo_pipeline.py"]

```

**Orchestration**

```yaml
# docker-compose.yml
services:
  apollo-tlr:
    build: .
    volumes:
      - ./data:/data
      - ./models:/models
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - CONFIG_FILE=/configs/production.yaml

```

---

### Semana 23: Monitoring & Logging

```python
# src/tlr/monitoring.py (NUEVO)

import logging
from prometheus_client import Counter, Histogram

# Metrics
frames_processed = Counter('frames_processed_total', 'Total frames')
detection_latency = Histogram('detection_latency_seconds', 'Detection time')
recognition_accuracy = Gauge('recognition_accuracy', 'Accuracy')

class PipelineMonitor:
    def log_frame(self, frame_id, results):
        frames_processed.inc()

        # Log detections
        logging.info(f"Frame {frame_id}: {len(results['detections'])} lights detected")

        # Log anomalies
        if len(results['detections']) == 0:
            logging.warning(f"Frame {frame_id}: No detections")

        if any(r['color'] == 'BLACK' for r in results['recognitions']):
            logging.warning(f"Frame {frame_id}: Unknown color detected")

```

---

### Semana 24: CI/CD Pipeline

```yaml
# .github/workflows/ci.yml
name: Apollo TLR CI/CD

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run tests
        run: |
          pytest tests/ -v --cov=src/

      - name: Benchmark performance
        run: |
          python tools/benchmark.py --report ci_report.json

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: test-results
          path: ci_report.json

  deploy:
    needs: test
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Build Docker image
        run: docker build -t apollo-tlr:latest .

      - name: Push to registry
        run: docker push apollo-tlr:latest

```

---

## 8. üìä Plan de Testing y Validaci√≥n

### 8.1 Test Pyramid

```
                    ‚ñ≤
                   / \
                  /   \
                 /  E2E \          10% - End-to-End (1-2 tests)
                /       \
               /_________\
              /           \
             / Integration \      30% - Integration (10-15 tests)
            /               \
           /_________________\
          /                   \
         /    Unit Tests       \   60% - Unit (50+ tests)
        /_______________________\

```

---

### 8.2 Test Suite por Gap

**Gap #1: Selection Algorithm**

```python
tests/test_apollo_selector.py
‚îú‚îÄ‚îÄ test_selection_criteria_scoring()
‚îú‚îÄ‚îÄ test_multiple_detections_fusion()
‚îú‚îÄ‚îÄ test_temporal_consistency_scoring()
‚îú‚îÄ‚îÄ test_shape_validation()
‚îú‚îÄ‚îÄ test_vs_hungarian_comparison()
‚îî‚îÄ‚îÄ test_performance_benchmark()

```

**Gap #2: Multi-Detection**

```python
tests/test_multi_detection.py
‚îú‚îÄ‚îÄ test_two_lights_same_roi()
‚îú‚îÄ‚îÄ test_detection_to_projection_mapping()
‚îú‚îÄ‚îÄ test_nms_across_rois()
‚îî‚îÄ‚îÄ test_complex_intersection()

```

**Gap #3: Dynamic Projections**

```python
tests/test_dynamic_projections.py
‚îú‚îÄ‚îÄ test_3d_to_2d_projection()
‚îú‚îÄ‚îÄ test_projection_accuracy_vs_ground_truth()
‚îú‚îÄ‚îÄ test_semantic_id_persistence()
‚îú‚îÄ‚îÄ test_cache_updates()
‚îî‚îÄ‚îÄ test_vehicle_movement_tracking()

```

**Gap #4: Semantic IDs**

```python
tests/test_semantic_ids.py
‚îú‚îÄ‚îÄ test_semantic_id_persistence()
‚îú‚îÄ‚îÄ test_backward_compatibility()
‚îú‚îÄ‚îÄ test_cross_history_fix()
‚îî‚îÄ‚îÄ test_tracking_with_semantic_ids()

```

**Gap #6: Spatial Robustness**

```python
tests/test_position_robustness.py
‚îú‚îÄ‚îÄ test_spatial_invariance()
‚îú‚îÄ‚îÄ test_swapping_robustness()
‚îú‚îÄ‚îÄ test_rotation_robustness()
‚îî‚îÄ‚îÄ benchmark_position_robustness()

```

---

### 8.3 Acceptance Criteria

### **Funcionalidad**

- ‚úÖ Selection algorithm selecciona mejor detection (vs Hungarian)
- ‚úÖ M√∫ltiples detections por ROI manejadas correctamente
- ‚úÖ Projection boxes actualizan din√°micamente con pose
- ‚úÖ Semantic IDs persisten entre frames
- ‚úÖ No cross-history transfer
- ‚úÖ Recognizer robusto a cambios de posici√≥n (variance < 1%)

### **Performance**

- ‚úÖ Pipeline completo: <100ms por frame (GPU)
- ‚úÖ Selection algorithm: <10ms
- ‚úÖ Dynamic projection: <5ms
- ‚úÖ Memory usage: <1GB

### **Robustez**

- ‚úÖ Accuracy > 95% en test set original
- ‚úÖ Accuracy > 90% con spatial shifts (¬±30px)
- ‚úÖ No degradaci√≥n con cambios de √°ngulo (¬±10¬∞)
- ‚úÖ Maneja intersecciones con 10+ sem√°foros

---

## 9. üéØ Resumen Ejecutivo para Implementaci√≥n

### 9.1 Quick Start (Semanas 1-3)

**Si solo tienes 3 semanas, implementa**:

1. **Semantic IDs**¬†(Gap #4) - 5 d√≠as
    - Elimina cross-history transfer
    - Bajo riesgo, alto impacto
2. **Apollo Selection**¬†(Gap #1) - 10 d√≠as
    - Mejora inmediata en assignment
    - Fusiona m√∫ltiples detections

**Resultado**: Sistema 40% m√°s robusto con 3 semanas de trabajo

---

### 9.2 Medium Term (Semanas 1-7)

**Si tienes 2 meses, agrega**: 3.¬†**Multi-Detection**¬†(Gap #2) - 10 d√≠as

- Maneja casos complejos
1. **STN Wrapper**¬†(Gap #6 quick fix) - 10 d√≠as
    - Mejora robustez espacial

**Resultado**: Sistema 70% Apollo-equivalent

---

### 9.3 Long Term (Semanas 1-20)

**Para sistema completo Apollo-level**: 5.¬†**Dynamic Projections**¬†(Gap #3) - 4 semanas

- Requiere infraestructura (HD-Map, localization)
1. **Re-entrenamiento Recognizer**¬†(Gap #6 completo) - 5 semanas
    - Elimina dependencia espacial completamente

**Resultado**: Sistema 95%+ Apollo-equivalent

---

## 10. üìù Checklist de Implementaci√≥n

### Para Ti (Cuando Vuelvas a Trabajar en Esto)

**Antes de empezar:**

```bash
# 1. Revisar este documento completo
# 2. Entender estado actual del c√≥digo
git log --oneline --graph --all

# 3. Verificar tests baseline
pytest tests/ -v

# 4. Crear branch de trabajo
git checkout -b feature/apollo-gaps-implementation

```

**Durante implementaci√≥n:**

- [ ]  Implementar un gap a la vez (no mezclar)
- [ ]  Escribir tests ANTES de c√≥digo (TDD)
- [ ]  Validar cada gap antes de continuar
- [ ]  Documentar decisiones en commits
- [ ]  Mantener backward compatibility

**Despu√©s de cada gap:**

- [ ]  Tests unitarios pasan
- [ ]  Tests de integraci√≥n pasan
- [ ]  Performance no degradado
- [ ]  Documentaci√≥n actualizada
- [ ]  Code review (si aplica)

---

## 11. üìö Referencias y Recursos

### Papers y Documentaci√≥n

- **Apollo Platform**:¬†https://github.com/ApolloAuto/apollo
- **Traffic Light Detection Paper**: "Apollo: An Open Autonomous Driving Platform" (Baidu Research)
- **Hungarian Algorithm**: Kuhn-Munkres algorithm explanation
- **Spatial Transformer Networks**: Jaderberg et al., 2015

### Herramientas Recomendadas

- **HD-Map Creation**: Apollo Studio, JOSM (OpenStreetMap editor)
- **Camera Calibration**: Kalibr, OpenCV calibration tool
- **Profiling**: PyTorch Profiler, cProfile
- **Visualization**: TensorBoard, Weights & Biases

### Datasets √ötiles

- **LISA Traffic Light Dataset**: Labeled images
- **Bosch Small Traffic Lights Dataset**: Multiple scenarios
- **Synthetic Data**: CARLA simulator