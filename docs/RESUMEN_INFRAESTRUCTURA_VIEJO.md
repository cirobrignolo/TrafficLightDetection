# ğŸ“‹ Informe TÃ©cnico - Sistema de DetecciÃ³n de SemÃ¡foros

## 1. ğŸ—ï¸ IntroducciÃ³n a la Arquitectura General

### 1.1 DescripciÃ³n del Sistema

El sistema de detecciÃ³n de semÃ¡foros es una reimplementaciÃ³n enÂ **PyTorch**Â de la arquitecturaÂ **ApolloTLR**Â (Traffic Light Recognition). El sistema implementa un pipeline completo de 3 etapas para la detecciÃ³n, reconocimiento y seguimiento temporal de semÃ¡foros en secuencias de video.

---

### 1.2 Componentes Principales del Sistema

El sistema se estructura enÂ **mÃ³dulos especializados**Â que trabajan en conjunto:

### **ğŸ” MÃ³dulo de DetecciÃ³n**

- **Archivo**:Â `src/tlr/detector.py`
- **Clase principal**:Â `TLDetector`
- **Arquitectura**: SSD (Single Shot Multibox Detector) adaptada para semÃ¡foros
- **FunciÃ³n**: Detectar bounding boxes de semÃ¡foros dentro de regiones de proyecciÃ³n predefinidas

### **ğŸ¨ MÃ³dulo de Reconocimiento**

- **Archivo**:Â `src/tlr/recognizer.py`
- **Clase principal**:Â `Recognizer`
- **Arquitectura**: CNN especializada por orientaciÃ³n
- **FunciÃ³n**: Clasificar el estado de los semÃ¡foros detectados (Rojo, Amarillo, Verde, Negro/Desconocido)

### **ğŸ”— MÃ³dulo de Tracking**

- **Archivo**:Â `src/tlr/tracking.py`
- **Clases principales**:Â `TrafficLightTracker`,Â `SemanticDecision`
- **FunciÃ³n**: Mantener consistencia temporal, filtrar parpadeos y aplicar histÃ©resis

### **ğŸ§© MÃ³dulo de AsignaciÃ³n**

- **Archivo**:Â `src/tlr/hungarian_optimizer.py`
- **Algoritmo**: Hungarian Algorithm (Algoritmo HÃºngaro)
- **FunciÃ³n**: AsignaciÃ³n Ã³ptima detecciÃ³n-proyecciÃ³n

### **ğŸ”„ Pipeline Principal**

- **Archivo**:Â `src/tlr/pipeline.py`
- **Clase principal**:Â `Pipeline`
- **FunciÃ³n**: Orquestar todo el flujo de procesamiento

---

### 1.3 Modelos de Deep Learning

El sistema utilizaÂ **4 modelos pre-entrenados**Â independientes:

| Modelo | Archivo de Pesos | PropÃ³sito | Dimensiones de Entrada |
| --- | --- | --- | --- |
| **Detector** | `tl.torch` | Detectar semÃ¡foros (SSD) | 270Ã—270Ã—3 |
| **Recognizer Vertical** | `vert.torch` | Clasificar semÃ¡foros verticales | 96Ã—32Ã—3 |
| **Recognizer Horizontal** | `hori.torch` | Clasificar semÃ¡foros horizontales | 32Ã—96Ã—3 |
| **Recognizer Quad** | `quad.torch` | Clasificar semÃ¡foros cuÃ¡druples | 64Ã—64Ã—3 |

**UbicaciÃ³n de pesos**:Â `src/tlr/weights/`

**Configuraciones**:Â `src/tlr/confs/`Â (parÃ¡metros JSON)

---

### 1.4 Flujo de Datos del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ENTRADA DEL SISTEMA                        â”‚
â”‚  â€¢ Imagen/Frame (HÃ—WÃ—3)                                         â”‚
â”‚  â€¢ Projection Boxes [x1,y1,x2,y2,id] (ROIs predefinidas)       â”‚
â”‚  â€¢ Timestamp (para tracking)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ETAPA 1: DETECCIÃ“N (SSD)                     â”‚
â”‚  â€¢ Preprocesamiento: crop ROI + resize 270Ã—270                  â”‚
â”‚  â€¢ Feature extraction (FeatureNet)                              â”‚
â”‚  â€¢ RPN + RCNN proposals                                         â”‚
â”‚  â€¢ Output: Bboxes [score, x1,y1,x2,y2, type_scores...]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FILTRADO Y CLASIFICACIÃ“N DE TIPO                   â”‚
â”‚  â€¢ Filtrar detecciones invÃ¡lidas (type != unknown)              â”‚
â”‚  â€¢ NMS (Non-Maximum Suppression, threshold=0.7)                 â”‚
â”‚  â€¢ Clasificar orientaciÃ³n: vertical/horizontal/quad             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ETAPA 2: RECONOCIMIENTO DE COLOR (CNN)                â”‚
â”‚  â€¢ Crop detecciÃ³n + resize segÃºn tipo                           â”‚
â”‚  â€¢ Preprocesamiento especÃ­fico (means, scale=0.01)              â”‚
â”‚  â€¢ ClasificaciÃ³n: [Black, Red, Yellow, Green]                   â”‚
â”‚  â€¢ Threshold de confianza: 0.5 (Apollo style)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          ASIGNACIÃ“N HÃšNGARA (Detection-Projection)              â”‚
â”‚  â€¢ Matching Ã³ptimo detecciones â†’ proyecciones                   â”‚
â”‚  â€¢ Maximiza IoU y minimiza costos                               â”‚
â”‚  â€¢ Output: pares (detection_idx, projection_id)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ETAPA 3: TRACKING TEMPORAL                         â”‚
â”‚  â€¢ HistÃ©resis: threshold de cambios consecutivos               â”‚
â”‚  â€¢ DetecciÃ³n de parpadeo (blink < 0.55s â†’ force RED)           â”‚
â”‚  â€¢ Reglas de seguridad: Yellow despuÃ©s de Red â†’ keep Red       â”‚
â”‚  â€¢ Ventana de revisiÃ³n: 1.5 segundos                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SALIDA DEL SISTEMA                         â”‚
â”‚  â€¢ valid_detections: Tensor (nÃ—9) - detecciones vÃ¡lidas        â”‚
â”‚  â€¢ recognitions: Tensor (nÃ—4) - clasificaciones one-hot        â”‚
â”‚  â€¢ assignments: Tensor (mÃ—2) - asignaciones det-proj           â”‚
â”‚  â€¢ invalid_detections: Tensor (kÃ—9) - detecciones filtradas    â”‚
â”‚  â€¢ revised_states: Dict {proj_id: (color, blink)}              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

```

---

## 2. âš™ï¸ CÃ³digo Nativo C++ vs Pipeline Python

### 2.1 Arquitectura Original: Apollo (C++ / Caffe)

El sistema original de Baidu Apollo estÃ¡ implementado como unÂ **sistema distribuido en C++**Â utilizando el frameworkÂ **Cyber RT**Â para comunicaciÃ³n entre componentes.

### **Componentes C++ Originales**

| Componente C++ | UbicaciÃ³n | Responsabilidad |
| --- | --- | --- |
| **TrafficLightDetection** | [`perception recortado/traffic_light_detection/`](perception recortado/traffic_light_detection/) | DetecciÃ³n SSD basada en Caffe |
| **ClassifyBySimple** | [`perception recortado/traffic_light_recognition/`](perception recortado/traffic_light_recognition/) | Reconocimiento con CNNs Caffe |
| **SemanticReviser** | [`perception recortado/traffic_light_tracking/`](perception recortado/traffic_light_tracking/) | Tracking y revisiÃ³n semÃ¡ntica |

### **CaracterÃ­sticas del Sistema C++**

- **Framework de inferencia**: Caffe + TensorRT/Paddle
- **ComunicaciÃ³n**: Cyber RT (pub/sub messages)
- **Procesamiento**: Pipeline asÃ­ncrono con buffers
- **GestiÃ³n de memoria**: Shared pointers + object pools
- **ConfiguraciÃ³n**: Protocol Buffers (.pb.txt)

---

### 2.2 Arquitectura ROI de Apollo (AnÃ¡lisis Detallado)

### **Estrategia de ROI Expansion**

Apollo diseÃ±Ã³ intencionalmente un sistema queÂ **compensa la imprecisiÃ³n de proyecciones HD-Map**:

```cpp
// Apollo's ROI creation logic (cropbox.h)
float crop_scale = 2.5;  // Expansion factor
int min_crop_size = 270; // Minimum size

float resize = crop_scale * max(projection.w, projection.h);
resize = max(resize, min_crop_size);
resize = min(resize, width);
resize = min(resize, height);

// ROI centrada en proyecciÃ³n pero MUCHO mÃ¡s grande
xl = projection.center_x - resize/2;
yt = projection.center_y - resize/2;

```

**RazÃ³n del diseÃ±o**:

- Proyecciones HD-Map son imprecisas (errores de calibraciÃ³n/GPS/IMU)
- ROIs grandes (2.5Ã— el tamaÃ±o estimado) compensan esta imprecisiÃ³n
- Mejor detectar de mÃ¡s que perderse semÃ¡foros reales

---

### **MÃºltiples Detecciones por ROI**

Apollo espera yÂ **maneja mÃºltiples semÃ¡foros en una sola ROI**:

```cpp
// Apollo's detection flow (detection.cc)
for (auto &light : lights_ref) {
    // 1. Crea ROI expandida alrededor de proyecciÃ³n HD-Map
    base::RectI cbox;
    crop_->getCropBox(img_width, img_height, light, &cbox);

    // 2. Procesa ROI con CNN detector
    if (!camera::OutOfValidRegion(cbox, img_width, img_height)) {
        // âš ï¸ Puede encontrar MÃšLTIPLES traffic lights en 1 ROI
        Inference(&lights_ref, data_provider);
    }
}

// Resultado: 1 ROI â†’ N detecciones (N â‰¥ 0)

```

**Casos cubiertos**:

- 1 ROI grande contiene varios semÃ¡foros fÃ­sicos
- Intersecciones complejas con mÃºltiples luces
- False positives que requieren filtrado posterior

---

### 2.3 Selection Algorithm: Apollo vs Sistema Actual

### **Apollo Original: Score-based Selection**

Apollo usaÂ **criterios mÃºltiples**Â para seleccionar la mejor detecciÃ³n:

```cpp
// Apollo's selection criteria (documentaciÃ³n oficial)
struct SelectionCriteria {
    float detection_score;        // CNN confidence (0-1)
    float spatial_proximity;      // Distance to HD-Map projection
    float shape_consistency;      // Geometric validation
    float temporal_consistency;   // History matching
};

// Weighted scoring
float final_score = 0.4 * detection_score +
                    0.3 * spatial_proximity +
                    0.2 * shape_consistency +
                    0.1 * temporal_consistency;

// Selecciona detecciÃ³n con mayor final_score por cada HD-Map light

```

**CaracterÃ­sticas**:

- **N detecciones â†’ 1 selecciÃ³n**Â por semÃ¡foro HD-Map
- Fusiona mÃºltiples detecciones del mismo objeto
- Considera historia temporal en la decisiÃ³n

---

### **Sistema Actual: Hungarian Algorithm**

Nuestro sistema usaÂ **asignaciÃ³n Ã³ptima 1:1**:

```python
# src/tlr/selector.py
def select_tls(ho, detections, projections, item_shape):
    costs = torch.zeros([len(projections), len(detections)])

    for row, projection in enumerate(projections):
        for col, detection in enumerate(detections):
            # Score combinado
            distance_score = calc_2d_gaussian_score(center_proj, center_det, 100, 100)
            detection_score = torch.max(detection[5:])

            costs[row, col] = 0.3 * detection_score + 0.7 * distance_score

    # Hungarian: assignment Ã³ptimo 1:1
    assignments = ho.maximize(costs)
    return assignments

```

**CaracterÃ­sticas**:

- **M projections Ã— N detections â†’ asignaciÃ³n 1:1**
- No fusiona detecciones mÃºltiples del mismo objeto
- No considera historia temporal en assignment

---

### **ComparaciÃ³n de Estrategias**

| Aspecto | Apollo (Score-based) | Sistema Actual (Hungarian) |
| --- | --- | --- |
| **MÃºltiples detecciones/semÃ¡foro** | âœ… Fusiona con scoring | âš ï¸ Solo asigna 1, resto â†’ ID -1 |
| **Criterios de selecciÃ³n** | 4 mÃ©tricas ponderadas | 2 mÃ©tricas (distance + confidence) |
| **Temporal consistency** | âœ… Incluida en scoring | âŒ Aplicada despuÃ©s (tracker) |
| **Shape validation** | âœ… Valida geometrÃ­a | âŒ No implementado |
| **Complexity** | O(N) por semÃ¡foro | O(NÂ³) Hungarian |
| **Robustez** | Alta (mÃºltiples criterios) | Media (solo spatial + confidence) |

---

### 2.4 Multi-Camera Selection (Apollo)

### **Sistema Multi-CÃ¡mara de Apollo**

Apollo tiene lÃ³gica adaptativa para seleccionar la mejor cÃ¡mara:

```cpp
// Apollo's multi-camera strategy
enum CameraType {
    TELEPHOTO_25MM,   // Long range, high resolution
    WIDE_ANGLE_6MM    // Short range, wide FOV
};

// Selection criteria
if (traffic_light_distance > 100m) {
    camera = TELEPHOTO_25MM;  // Mejor resoluciÃ³n para lejos
} else if (traffic_light_distance < 30m) {
    camera = WIDE_ANGLE_6MM;  // Mejor FOV para cerca
} else {
    // Fusion: usa ambas cÃ¡maras y fusiona resultados
    camera = BOTH;
}

```

**Ventajas**:

- Telephoto (25mm): SemÃ¡foros lejanos, mejor resoluciÃ³n
- Wide-angle (6mm): SemÃ¡foros cercanos, campo de visiÃ³n amplio
- Fusion: Mayor robustez combinando ambas vistas

**Sistema Actual**: Single camera (sin selecciÃ³n multi-cÃ¡mara)

---

### 2.5 ReimplementaciÃ³n Python (PyTorch)

Nuestra implementaciÃ³nÂ **traduce la arquitectura C++ a Python/PyTorch**Â manteniendoÂ **fidelidad funcional parcial**.

### **Diferencias Arquitecturales Clave**

| Aspecto | C++ (Apollo Original) | Python (Sistema Actual) |
| --- | --- | --- |
| **Framework ML** | Caffe / TensorRT | PyTorch |
| **Arquitectura** | Componentes distribuidos (Cyber RT) | Pipeline monolÃ­tico (claseÂ `Pipeline`) |
| **ComunicaciÃ³n** | Mensajes asÃ­ncronos (pub/sub) | Llamadas sÃ­ncronas |
| **ConfiguraciÃ³n** | Protocol Buffers (.pb.txt) | JSON + argumentos Python |
| **Tipos de datos** | `base::TrafficLightPtr`Â (C++ shared_ptr) | `torch.Tensor` |
| **ImÃ¡genes** | `Image8U`Â (custom struct) | NumPy arrays / Torch tensors |
| **Inferencia** | Multi-backend (Caffe/TRT/Paddle) | PyTorch JIT (`.torch`Â files) |
| **NMS** | Custom C++ optimizado | PyTorch opsÂ `utils.py:nms()` |
| **Assignment** | Score-based selection | Hungarian algorithm |

---

### **Diferencias de ROI Processing**

| Aspecto | Apollo Original | Sistema Actual |
| --- | --- | --- |
| **ROI Processing** | 1 ROI â†’ MÃºltiples detecciones â†’ SelecciÃ³n | 1 Projection â†’ MÃºltiples detecciones â†’ NMS global |
| **Assignment Logic** | Score-based (4 mÃ©tricas) | Hungarian (2 mÃ©tricas) |
| **Region Fusion** | âœ… Fusiona regiones superpuestas | âŒ Trata cada projection independientemente |
| **ID Management** | HD-Map semantic IDs (persistentes) | Projection box row indices (espaciales) |
| **Temporal Consistency** | âœ… En selection algorithm | âœ… En tracking module (despuÃ©s) |
| **Multi-camera** | âœ… Telephoto + Wide-angle fusion | âŒ Single camera |

---

### **Diferencias de Projection Boxes**

| Aspecto | Apollo Original | Sistema Actual |
| --- | --- | --- |
| **Origen** | HD-Map 3D â†’ ProyecciÃ³n 2D dinÃ¡mica | Archivo estÃ¡tico manual |
| **ActualizaciÃ³n** | Cada frame (pose del vehÃ­culo) | Fijas (o propagaciÃ³n manual) |
| **PrecisiÃ³n** | Baja (compensada con ROI expansion) | Alta (definidas manualmente) |
| **Escalabilidad** | AutomÃ¡tica (del HD-Map) | Manual (requiere annotaciÃ³n) |
| **Robustez** | âœ… Sigue semÃ¡foros fÃ­sicos | âŒ Cross-history transfer posible |

---

### 2.6 Equivalencias de CÃ³digo

### **Ejemplo 1: DetecciÃ³n**

**C++ (Apollo)**:

```cpp
// perception/traffic_light_detection/detector/caffe_detection/detection.cc
bool TrafficLightDetection::Detect(camera::TrafficLightFrame *frame) {
    for (auto &light : frame->traffic_lights) {
        // 1. Crop ROI with expansion
        base::RectI cbox;
        crop_->getCropBox(img_width, img_height, light, &cbox);

        // 2. Caffe inference
        rt_net_->Infer();

        // 3. Select best detections
        SelectOutputBoxes(...);

        // 4. Apply NMS
        ApplyNMS(...);
    }
}

```

**Python (Sistema Actual)**:

```python
# src/tlr/pipeline.py:26-38
def detect(self, image, boxes):
    detected_boxes = []
    projections = boxes2projections(boxes)

    for projection in projections:
        # 1. Crop ROI with Apollo expansion (crop_scale=2.5)
        input = preprocess4det(image, projection, self.means_det)

        # 2. PyTorch inference
        bboxes = self.detector(input.unsqueeze(0).permute(0, 3, 1, 2))
        detected_boxes.append(bboxes)

    # 3. Restore coordinates
    detections = restore_boxes_to_full_image(image, detected_boxes, projections)
    detections = torch.vstack(detections).reshape(-1, 9)

    # 4. Global NMS
    idxs = nms(detections[:, 1:5], 0.7)
    return detections[idxs]

```

**Diferencias**:

- Apollo: Selection algorithm despuÃ©s de NMS
- Sistema actual: Solo NMS, sin selection adicional

---

### **Ejemplo 2: Reconocimiento**

**C++ (Apollo)**:

```cpp
// perception/traffic_light_recognition/recognition/caffe_recognizer/classify.h:63
void Prob2Color(const float* out_put_data, float threshold,
                base::TrafficLightPtr light) {
    int max_idx = argmax(out_put_data, 4);
    float max_prob = out_put_data[max_idx];

    if (max_prob > threshold) {
        light->status.color = static_cast<TLColor>(max_idx);
    } else {
        light->status.color = TLColor::TL_UNKNOWN_COLOR;
    }
}

```

**Python (Sistema Actual)**:

```python
# src/tlr/pipeline.py:40-82
def recognize(self, img, detections, tl_types):
    # Apollo's EXACT Prob2Color logic
    max_prob, max_idx = torch.max(output_probs, dim=0)
    threshold = 0.5

    if max_prob > threshold:
        color_id = max_idx.item()
    else:
        color_id = 0  # Force to BLACK like Apollo

    # One-hot result
    result = torch.zeros(4)
    result[color_id] = 1.0

```

**Equivalencia**: âœ… LÃ³gica idÃ©ntica (Prob2Color replicada exactamente)

---

### **Ejemplo 3: Tracking Temporal**

**C++ (Apollo)**:

```cpp
// perception/traffic_light_tracking/tracker/semantic_decision.h:31-44
struct SemanticTable {
    double time_stamp = 0.0;
    double last_bright_time_stamp = 0.0;
    double last_dark_time_stamp = 0.0;
    bool blink = false;
    std::string semantic;
    std::vector<int> light_ids;
    base::TLColor color;
    HystereticWindow hystertic_window;
};

base::TLColor ReviseBySemantic(SemanticTable semantic_table,
                               std::vector<base::TrafficLightPtr> *lights);

```

**Python (Sistema Actual)**:

```python
# src/tlr/tracking.py:18-34
class SemanticTable:
    def __init__(self, semantic_id: int, time_stamp: float, color: str):
        self.semantic_id = semantic_id
        self.time_stamp = time_stamp
        self.color = color
        self.last_bright_time = time_stamp
        self.last_dark_time = time_stamp
        self.blink = False
        self.hysteretic_color = color
        self.hysteretic_count = 0

def update(self, frame_ts, assignments, recognitions):
    # Apollo-style revision logic
    ...

```

**Equivalencia**: âœ… Estructura y lÃ³gica muy similares

---

### 2.7 Ventajas y Desventajas

### **âœ… Ventajas de la ImplementaciÃ³n Python**

- **Simplicidad**: Pipeline monolÃ­tico mÃ¡s fÃ¡cil de entender y debuggear
- **Portabilidad**: Solo requiere PyTorch (sin dependencias de Cyber RT, Caffe, TensorRT)
- **Flexibilidad**: FÃ¡cil experimentaciÃ³n y modificaciones
- **Debugging**: Herramientas Python (pdb, print, visualizaciones)
- **Reproducibilidad**: ConfiguraciÃ³n en JSON/cÃ³digo simple
- **Prototipado rÃ¡pido**: Ideal para investigaciÃ³n y desarrollo

### **âŒ Desventajas vs C++**

- **Performance**: 2-3Ã— mÃ¡s lento que C++ optimizado (especialmente Hungarian vs Selection)
- **Escalabilidad**: Pipeline sÃ­ncrono vs asÃ­ncrono distribuido de Apollo
- **Memoria**: Python tiene mayor overhead que C++
- **ProducciÃ³n**: C++ es mÃ¡s adecuado para sistemas embebidos
- **Multi-camera**: No implementado (Apollo tiene fusion)
- **Selection Logic**: Hungarian 1:1 vs Score-based N:1 de Apollo

---

### 2.8 Comparativa de Flujo de Datos

### **ğŸ›ï¸ Apollo Original (C++/Caffe)**

```
HD-Map Projections (3D coords)
    â†“
Pose Update (GPS + IMU + Odometry)
    â†“
3Dâ†’2D Projection (Camera calibration)
    â†“
ROI Expansion (crop_scale=2.5, min_size=270)
    â†“
CNN Detection (Caffe/TensorRT) â†’ Multiple detections per ROI
    â†“
Selection Algorithm (4 criteria scoring)
    â†“
1 Best Detection per HD-Map Light
    â†“
Recognition (Multi-camera fusion if needed)
    â†“
Temporal Revision (SemanticReviser)
    â†“
Final Output (with semantic IDs)

```

---

### **ğŸ”§ Sistema Actual (Python/PyTorch)**

```
Projection Boxes (Manual file, static)
    â†“
Individual Detection per Projection (PyTorch SSD)
    â†“
Multiple Detections â†’ Global NMS
    â†“
Hungarian Assignment (2D Gaussian + Confidence)
    â†“
1:1 Projection:Detection Assignment
    â†“
Recognition (Single camera, orientation-specific CNNs)
    â†“
Tracking (Temporal consistency module)
    â†“
Final Output (with row indices as IDs)

```

---

### 2.9 Tabla Comparativa Completa

| CaracterÃ­stica | Apollo C++ | Sistema Python | Ventaja |
| --- | --- | --- | --- |
| **Projection Source** | HD-Map 3D dinÃ¡mico | Archivo estÃ¡tico | Apollo |
| **ROI Expansion** | crop_scale=2.5 | crop_scale=2.5 | Igual |
| **Detections/ROI** | MÃºltiples manejadas | MÃºltiples â†’ NMS | Apollo |
| **Assignment** | Score-based (4 criterios) | Hungarian (2 criterios) | Apollo |
| **Selection Complexity** | O(N) | O(NÂ³) | Apollo |
| **Multi-camera** | âœ… Telephoto + Wide | âŒ Single | Apollo |
| **ID Persistence** | âœ… Semantic IDs | âŒ Row indices | Apollo |
| **Cross-history Bug** | âŒ No ocurre | âœ… Puede ocurrir | Apollo |
| **Temporal in Selection** | âœ… Incluido | âŒ Separado | Apollo |
| **Framework** | Caffe/TensorRT | PyTorch | Python (flexibilidad) |
| **Deployment** | Embebido/ProducciÃ³n | InvestigaciÃ³n/Prototipo | Apollo |
| **Debugging** | DifÃ­cil (C++/distribuido) | FÃ¡cil (Python/monolÃ­tico) | Python |
| **Performance** | 15-25ms/frame | 50-71ms/frame | Apollo |
| **PrecisiÃ³n (mAP)** | Baseline 100% | ~97% | Apollo |

---

### 2.10 Resumen de Diferencias CrÃ­ticas

### **Diferencias que Afectan Funcionalidad**

1. **Assignment Strategy**
    - Apollo: Score-based N:1 (fusiona mÃºltiples detecciones)
    - Actual: Hungarian 1:1 (algunas detecciones â†’ ID -1)
2. **ID Management**
    - Apollo: Semantic IDs del HD-Map (persistentes entre frames)
    - Actual: Row indices (pueden causar cross-history transfer)
3. **Projection Updates**
    - Apollo: DinÃ¡micas cada frame (siguen semÃ¡foros fÃ­sicos)
    - Actual: EstÃ¡ticas (o propagaciÃ³n manual)

### **Diferencias que Afectan Performance**

1. **Hungarian O(NÂ³)**Â vsÂ **Selection O(N)**
2. **Python overhead**Â vsÂ **C++ optimizado**
3. **Single-threaded**Â vsÂ **Multi-threaded asÃ­ncrono**

### **Equivalencias Mantenidas**

1. âœ…Â **Prob2Color logic**Â (reconocimiento)
2. âœ…Â **SemanticReviser logic**Â (tracking temporal)
3. âœ…Â **ROI expansion**Â (crop_scale=2.5)
4. âœ…Â **NMS threshold**Â (0.7 para detecciÃ³n)
5. âœ…Â **Safety rules**Â (blink detection, hysteresis)

---

## 3. ğŸ¯ Zonas de Reconocimiento (Projection Boxes)

### 3.1 Concepto de Projection Boxes

LasÂ **projection boxes**Â (cajas de proyecciÃ³n) sonÂ **regiones de interÃ©s (ROI)**Â predefinidas donde el sistema busca semÃ¡foros. Este concepto es fundamental en la arquitectura Apollo y reduce significativamente el espacio de bÃºsqueda.

### **Â¿Por quÃ© Projection Boxes?**

- **Eficiencia computacional**: Solo procesar regiones relevantes (no toda la imagen)
- **ReducciÃ³n de falsos positivos**: Limitar bÃºsqueda a zonas esperadas
- **Aprovechamiento de HD Maps**: Usar informaciÃ³n geomÃ©trica del mapa
- **Tracking robusto**: Asociar detecciones a semÃ¡foros conocidos

---

### 3.2 Estructura de Projection Boxes

Cada projection box se define como:

```python
[x_min, y_min, x_max, y_max, projection_id]

```

| Campo | Tipo | DescripciÃ³n |
| --- | --- | --- |
| `x_min, y_min` | int | Coordenada superior izquierda (pÃ­xeles) |
| `x_max, y_max` | int | Coordenada inferior derecha (pÃ­xeles) |
| `projection_id` | int | Identificador Ãºnico del semÃ¡foro |

**Ejemplo**:

```python
boxes = [
    [100, 50, 150, 120, 0],  # SemÃ¡foro izquierdo
    [200, 45, 250, 115, 1],  # SemÃ¡foro derecho
]

```

---

### 3.3 ğŸ”‘ HALLAZGO CRÃTICO: AsociaciÃ³n de Historiales a Regiones

### **Concepto Fundamental**

El sistemaÂ **NO asocia historiales a semÃ¡foros fÃ­sicos**, sino aÂ **posiciones espaciales (regiones)**:

```python
# âŒ Lo que NO hace el sistema:
history[semaforo_ID] = estado_del_semaforo

# âœ… Lo que SÃ hace el sistema:
history[region_index] = estado_del_semaforo_que_este_en_esa_region

```

### **ImplicaciÃ³n PrÃ¡ctica**

```python
# ConfiguraciÃ³n inicial
projections = [
    [421,165,460,223,0],  # row_index=0 (regiÃ³n izquierda)
    [466,165,511,256,1]   # row_index=1 (regiÃ³n derecha)
]

# El historial se asocia asÃ­:
history[0] = historial_de_lo_que_este_en_posicion_izquierda
history[1] = historial_de_lo_que_este_en_posicion_derecha

# Si los semÃ¡foros fÃ­sicos intercambian posiciones:
# â†’ Los historiales se "transfieren" entre semÃ¡foros fÃ­sicos

```

---

### 3.4 GeneraciÃ³n de Projection Boxes

El sistema proveeÂ **dos mÃ©todos**Â para generar projection boxes:

### **ğŸ–±ï¸ MÃ©todo 1: SelecciÃ³n Manual Interactiva**

**Script**:Â `select_projection_and_append.py`

```bash
python select_projection_and_append.py

```

**Flujo de trabajo**:

1. Carga imagen de referencia
2. Usuario dibuja rectÃ¡ngulos con el mouse
3. Sistema asigna IDs automÃ¡ticamente
4. Guarda enÂ `projection_bbboxes_master.txt`

**Formato de salida**Â (`projection_bbboxes_master.txt`):

```
frame_000001.jpg 100,50,150,120,0 200,45,250,115,1
frame_000002.jpg 101,51,151,121,0 201,46,251,116,1

```

---

### **ğŸ“ MÃ©todo 2: GeneraciÃ³n ProgramÃ¡tica**

**Script**:Â `projection_boxes_generator.py`

Genera projection boxes basÃ¡ndose en:

- Detecciones previas conocidas
- Reglas geomÃ©tricas (altura, ancho esperados)
- PropagaciÃ³n temporal (frames consecutivos)

**Ventaja**: Escalable para secuencias largas de video

---

### 3.5 Preprocesamiento con Projection Boxes

Una vez definidas las projection boxes, el pipeline las utiliza asÃ­:

### **Paso 1: ConversiÃ³n a Proyecciones**

```python
# src/tlr/tools/utils.py
def boxes2projections(boxes):
    """
    Convierte bboxes [x1,y1,x2,y2,id] a proyecciones internas
    Returns: List[ProjectionROI]
    """
    projections = []
    for box in boxes:
        x1, y1, x2, y2, proj_id = box  # proj_id del archivo (ignorado despuÃ©s)
        projections.append(ProjectionROI(x1, y1, x2-x1, y2-y1))
    return projections

```

---

### **Paso 2: Crop y Resize de ROI**

```python
# src/tlr/tools/utils.py:234-239
def preprocess4det(image, projection, means):
    """
    1. Crop regiÃ³n de proyecciÃ³n con expansiÃ³n Apollo (crop_scale=2.5)
    2. Resize a 270Ã—270 (entrada del detector)
    3. Restar means [102.98, 115.95, 122.77]
    """
    xl, xr, yt, yb = crop(image.shape, projection)
    src = image[yt:yb, xl:xr]
    dst = torch.zeros(270, 270, 3, device=src.device)
    resized = ResizeGPU(src, dst, means)
    return resized

```

**LÃ³gica de ExpansiÃ³n Apollo**:

```python
# src/tlr/tools/utils.py:211-232
crop_scale = 2.5  # Apollo default
min_crop_size = 270
resize = crop_scale * max(projection.w, projection.h)
resize = max(resize, min_crop_size)
resize = min(resize, width, height)

```

---

### **Paso 3: RestauraciÃ³n a Coordenadas Originales**

âš ï¸Â **BUG CORREGIDO**: Apollo Coordinate Scaling

```python
# src/tlr/tools/utils.py:257-298
def restore_boxes_to_full_image(image, detected_boxes, projections):
    """
    FIXED: Apollo coordinate scaling bug

    âŒ Bug original:
    detection[:, x] += xl  # Agregar offset directamente a coords 270Ã—270

    âœ… Fix correcto (Apollo style):
    1. Escalar de 270Ã—270 a tamaÃ±o real del crop
    2. LUEGO agregar offset del crop
    """
    for detection, projection in zip(detected_boxes, projections):
        xl, xr, yt, yb = crop(image.shape, projection)

        # Calcular scaling factors
        crop_width = xr - xl + 1
        crop_height = yb - yt + 1
        scale_x = crop_width / 270.0
        scale_y = crop_height / 270.0

        # Paso 1: ESCALAR (270Ã—270 â†’ crop size)
        detection[:, 1] *= scale_x  # x1
        detection[:, 2] *= scale_y  # y1
        detection[:, 3] *= scale_x  # x2
        detection[:, 4] *= scale_y  # y2

        # Paso 2: TRASLADAR (crop â†’ imagen completa)
        detection[:, 1] += xl
        detection[:, 2] += yt
        detection[:, 3] += xl
        detection[:, 4] += yt

    return detected_boxes

```

---

### 3.6 ğŸ§® Algoritmo HÃºngaro: AnÃ¡lisis Detallado

### **FunciÃ³n del Algoritmo**

El algoritmo hÃºngaro resuelve el problema deÂ **asignaciÃ³n Ã³ptima**Â entre:

- **Detecciones**Â (semÃ¡foros encontrados por el detector)
- **Projection boxes**Â (regiones esperadas)

```python
# Objetivo: Maximizar suma total de scores de proximidad
# Constraint: Assignment 1:1 (1 detection â†’ max 1 projection)

```

---

### **ConstrucciÃ³n de Matriz de Costos**

```python
# src/tlr/selector.py
def select_tls(ho, detections, projections, item_shape):
    costs = torch.zeros([len(projections), len(detections)])

    for row, projection in enumerate(projections):  # row = proj_index
        center_hd = [projection.center_x, projection.center_y]

        for col, detection in enumerate(detections):  # col = det_index
            # Centro de detecciÃ³n
            center_refine = [(det[3] + det[1])/2, (det[4] + det[2])/2]

            # Score de distancia (Gaussiana 2D)
            distance_score = calc_2d_gaussian_score(center_hd, center_refine, 100, 100)
            # Formula: exp(-0.5 * ((dx/Ïƒx)Â² + (dy/Ïƒy)Â²))

            # Score de detecciÃ³n (confianza del modelo)
            detection_score = torch.max(detection[5:])  # Max de type scores

            # Score final combinado
            costs[row, col] = 0.3 * detection_score + 0.7 * distance_score

```

---

### **Ejemplo Concreto**

**Input**:

```python
projections = [
    ProjectionROI(421,165,460,223),  # Centro: (440.5, 194)   - row=0
    ProjectionROI(466,165,511,256)   # Centro: (488.5, 210.5) - row=1
]

detections = [
    [0.95, 432, 176, 452, 212, 0.006, 0.984, 0.008, 0.002],  # Centro: (442, 194)   - col=0
    [0.98, 476, 175, 501, 247, 0.0005, 0.999, 0.0003, 0.0003] # Centro: (488.5, 211) - col=1
]

```

**Matriz de Costos Calculada**:

```python
# Projection 0 (row=0) vs Detection 0 (col=0):
distance_score = exp(-0.5 * ((1.5Â²/100Â²) + (0Â²/100Â²))) â‰ˆ 0.999  # MUY CERCA
detection_score = 0.984
costs[0,0] = 0.3 * 0.984 + 0.7 * 0.999 â‰ˆ 0.994 âœ…

# Projection 0 (row=0) vs Detection 1 (col=1):
distance_score = exp(-0.5 * ((48Â²/100Â²) + (17Â²/100Â²))) â‰ˆ 0.156  # LEJOS
costs[0,1] â‰ˆ 0.156 âŒ

# Matriz completa:
costs = [
    [0.994, 0.156],  # Proj 0 prefiere Det 0
    [0.156, 0.994]   # Proj 1 prefiere Det 1
]

```

**Assignment Ã“ptimo**:

```python
assignments = ho.maximize(costs)
# Resultado: [[0, 0], [1, 1]]
#             â†‘   â†‘    â†‘   â†‘
#           row col  row col
#         (proj_idx, det_idx)

```

---

### 3.7 ğŸ”‘ HALLAZGO CRÃTICO: IDs son Ãndices, NO del Archivo

### **Concepto Fundamental**

```python
# âŒ MALENTENDIDO COMÃšN:
# "El proj_id del assignment viene del archivo projection_bboxes_master.txt"

# âœ… REALIDAD:
# El proj_id es el ROW INDEX en el array de projections

```

### **Prueba Experimental**

**Test 1: Cambiar IDs en archivo**

```python
# Archivo original:
421,165,460,223,0  # row_index=0, file_id=0
466,165,511,256,1  # row_index=1, file_id=1

# Archivo modificado:
421,165,460,223,1  # row_index=0, file_id=1 (cambiado)
466,165,511,256,0  # row_index=1, file_id=0 (cambiado)

# Resultado: Â¡NO CAMBIA NADA!
# assignments sigue siendo [[0, 0], [1, 1]] (usa row_index, ignora file_id)

```

**Test 2: Intercambiar coordenadas**

```python
# Archivo modificado (intercambio fÃ­sico):
466,165,511,256,0  # row_index=0 ahora en posiciÃ³n DERECHA
421,165,460,223,1  # row_index=1 ahora en posiciÃ³n IZQUIERDA

# Resultado: Â¡Assignments cambian!
# Porque row_index=0 ahora estÃ¡ en posiciÃ³n derecha

```

---

### 3.8 ğŸš¨ FenÃ³meno de Cross-History Transfer

### **DescripciÃ³n del Problema**

Cuando semÃ¡foros fÃ­sicosÂ **intercambian posiciones**, losÂ **historiales se transfieren**Â entre ellos.

**Escenario**:

```python
# Frame 1-214:
# SemÃ¡foro_Izq (verde) en posiciÃ³n (432,176) â†’ row_index=0 â†’ history[0]
# SemÃ¡foro_Der (amarillo blink) en posiciÃ³n (476,175) â†’ row_index=1 â†’ history[1]

# Frame 215+ (despuÃ©s de swap fÃ­sico):
# SemÃ¡foro_Der ahora en (432,176) â†’ row_index=0 â†’ Â¡hereda history[0]!
# SemÃ¡foro_Izq ahora en (476,175) â†’ row_index=1 â†’ Â¡hereda history[1]!

```

**Resultado Observado**:

```python
# SemÃ¡foro derecho (amarillo parpadeante):
# â†’ Se mueve a posiciÃ³n izquierda
# â†’ Recibe history[0] que tiene "green estable, no blink"
# â†’ Output: YELLOW sin blink âŒ

# SemÃ¡foro izquierdo (verde estable):
# â†’ Se mueve a posiciÃ³n derecha
# â†’ Recibe history[1] que tiene "blink=True"
# â†’ Output: Mantiene blink flag incorrectamente âŒ

```

---

### 3.9 ğŸ—ï¸ ComparaciÃ³n con Apollo Original

### **Sistema Actual (Projection Boxes EstÃ¡ticas)**

```python
# Projection boxes definidas manualmente, NO se actualizan
projections = [
    [421,165,460,223,0],  # Fijas para todo el video
    [466,165,511,256,1]
]

# Problema: Si semÃ¡foros se mueven â†’ cross-history transfer

```

---

### **Apollo Original (Projection Boxes DinÃ¡micas)**

```cpp
// Apollo actualiza projection boxes cada frame
if (!preprocessor_->UpdateLightsProjection(pose, option, camera_name,
                                          &frame->traffic_lights)) {
  // ProyecciÃ³n basada en:
  // 1. HD-Map: Coordenadas 3D de semÃ¡foros reales
  // 2. Pose del vehÃ­culo: GPS + IMU + odometrÃ­a
  // 3. CalibraciÃ³n de cÃ¡mara: ProyecciÃ³n 3Dâ†’2D

  if (!ProjectLights(pose, camera_name, lights, &lights_on_image_)) {
    // Projection boxes siguen a semÃ¡foros fÃ­sicos
  }
}

```

**Flujo Apollo**:

```
Frame N:
1. VehÃ­culo en pose (x, y, Î¸)
2. HD-Map dice "semÃ¡foro A en coord 3D (X, Y, Z)"
3. ProyecciÃ³n 3Dâ†’2D con calibraciÃ³n: semÃ¡foro A â†’ bbox 2D (432,176,452,212)
4. Projection box para semÃ¡foro A: [432,176,452,212]

Frame N+1:
1. VehÃ­culo moviÃ³ a pose (x', y', Î¸')
2. Mismo semÃ¡foro A en (X, Y, Z)
3. Nueva proyecciÃ³n: semÃ¡foro A â†’ bbox 2D (435,178,455,214) (se moviÃ³)
4. Projection box actualizada: [435,178,455,214]

Historial sigue al semÃ¡foro fÃ­sico âœ…

```

---

### 3.10 ValidaciÃ³n Post-AsignaciÃ³n

```python
# src/tlr/selector.py
for assignment in assignments:  # [[proj_idx, det_idx], ...]
    proj_idx, det_idx = assignment

    # Verificar que detection estÃ¡ DENTRO de projection
    coors = crop(item_shape, projections[proj_idx])
    detection = detections[det_idx]

    # Bounds check
    if coors[0] <= detection[1] and coors[1] >= detection[3] and \
       coors[2] <= detection[2] and coors[3] >= detection[4]:
        # âœ… Assignment vÃ¡lido
        final_assignments.append([proj_idx, det_idx])
    else:
        # âŒ Detection fuera de projection â†’ rechazado
        pass

```

---

### 3.11 ğŸ” ID -1 Phenomenon

### **Causas de Detecciones sin ID**

**Caso 1: Detection fuera de todas las Projection Boxes**

```python
# Detector encuentra semÃ¡foro en (600, 400)
# Projections solo cubren (0-500, 0-300)
# â†’ Detection no puede asignarse â†’ ID -1

```

**Caso 2: MÃºltiples Detecciones del Mismo SemÃ¡foro**

```python
# Detector genera 2 bboxes para 1 semÃ¡foro:
detections = [
    [0.95, 430, 174, 454, 214, ...],  # Detection A
    [0.90, 432, 176, 452, 212, ...]   # Detection B (muy cercana)
]

# Hungarian solo asigna 1:1
# â†’ Detection con mejor score se asigna
# â†’ Otra queda ID -1

```

**Caso 3: False Positives Lejanos**

```python
# Detector confunde objeto con semÃ¡foro
# Pero estÃ¡ muy lejos de projections
# â†’ Score bajo â†’ no se asigna â†’ ID -1

```

**Frecuencia observada**: 5-10% de detecciones vÃ¡lidas

---

### 3.12 Resumen de Conceptos Clave

| Concepto | ImplementaciÃ³n Actual | Apollo Original |
| --- | --- | --- |
| **Projection Boxes** | EstÃ¡ticas (archivo manual) | DinÃ¡micas (HD-Map + pose) |
| **Assignment IDs** | Row index de array | Semantic IDs del HD-Map |
| **Historial** | Asociado a row_index (regiÃ³n espacial) | Asociado a semantic_id (semÃ¡foro fÃ­sico) |
| **Cross-history** | âœ… Ocurre (bug) | âŒ No ocurre (boxes siguen semÃ¡foros) |
| **ID -1** | âœ… ComÃºn (5-10%) | âš ï¸ Raro (ROIs grandes cubren todo) |
| **ROI Expansion** | crop_scale = 2.5 | crop_scale = 2.5 (mismo) |

---

## 4. ğŸ”¬ Detalles de Cada Pipeline

### 4.1 Pipeline de DetecciÃ³n (SSD-based)

### **Arquitectura del Detector**

```python
# src/tlr/detector.py:8-40
class TLDetector(nn.Module):
    def __init__(self, ...):
        self.feature_net = FeatureNet()           # ExtracciÃ³n de features
        self.proposal = RPNProposalSSD(...)       # Region Proposal Network
        self.psroi_rois = DFMBPSROIAlign(...)     # Position-Sensitive ROI Align
        self.inner_rois = nn.Linear(490, 2048)    # FC layer
        self.cls_score = nn.Linear(2048, 4)       # ClasificaciÃ³n de tipo
        self.bbox_pred = nn.Linear(2048, 16)      # PredicciÃ³n de bbox
        self.rcnn_proposal = RCNNProposal(...)    # RCNN refinement

```

---

### **Feature Net (Backbone)**

```python
# src/tlr/feature_net.py
class FeatureNet(nn.Module):
    """
    Red convolucional para extracciÃ³n de features
    Input: 270Ã—270Ã—3
    Output: 34Ã—34Ã—490 (multi-scale features)
    """

```

**Flujo**:

1. **Conv layers**: ExtracciÃ³n de caracterÃ­sticas multi-escala
2. **RPN**: Genera proposals iniciales (bboxes candidatas)
3. **PSROIAlign**: Pooling position-sensitive de ROIs
4. **RCNN head**: Refinamiento final de bboxes + clasificaciÃ³n de tipo

---

### **MÃºltiples Detecciones por Projection Box**

âš ï¸Â **HALLAZGO IMPORTANTE**: El detector puede encontrarÂ **mÃºltiples semÃ¡foros**Â en una sola projection box.

```python
# src/tlr/pipeline.py:26-38
def detect(self, image, boxes):
    detected_boxes = []
    projections = boxes2projections(boxes)

    for projection in projections:  # Para cada ROI
        input = preprocess4det(image, projection, self.means_det)
        bboxes = self.detector(input.unsqueeze(0).permute(0, 3, 1, 2))
        # âš ï¸ bboxes puede contener N detecciones (N â‰¥ 0)
        detected_boxes.append(bboxes)

    # FusiÃ³n global de todas las detecciones
    detections = restore_boxes_to_full_image(image, detected_boxes, projections)
    detections = torch.vstack(detections).reshape(-1, 9)

    # NMS elimina duplicados
    idxs = nms(detections[:, 1:5], 0.7)
    detections = detections[idxs]

    return detections

```

**Implicaciones**:

- 1 Projection Box â†’ puede generar 0, 1, 2+ detecciones
- NMS fusiona duplicados globalmente
- Assignment hÃºngaro luego selecciona 1:1

---

### **Salida del Detector**

```python
# Formato de salida: Tensor (N Ã— 9)
# [score, x1, y1, x2, y2, type_vert, type_quad, type_hori, type_unknown]

```

| Ãndice | Campo | DescripciÃ³n |
| --- | --- | --- |
| 0 | `score` | Confianza de detecciÃ³n (0-1) |
| 1-4 | `x1,y1,x2,y2` | Coordenadas del bounding box |
| 5 | `type_vert` | Score para tipo vertical |
| 6 | `type_quad` | Score para tipo quad |
| 7 | `type_hori` | Score para tipo horizontal |
| 8 | `type_unknown` | Score para tipo desconocido |

---

### 4.2 Pipeline de Reconocimiento (CNN Especializada)

### **Arquitectura del Recognizer**

```python
# src/tlr/recognizer.py:41-63
class Recognizer(nn.Module):
    def __init__(self, pool5_params):
        self.conv1 = ConvBNScale4Rec(3, 32, ...)    # 32 channels
        self.conv2 = ConvBNScale4Rec(32, 64, ...)   # 64 channels
        self.conv3 = ConvBNScale4Rec(64, 128, ...)  # 128 channels
        self.conv4 = ConvBNScale4Rec(128, 128, ...) # 128 channels
        self.conv5 = ConvBNScale4Rec(128, 128, ...) # 128 channels
        self.pool5 = nn.AvgPool2d(**pool5_params)   # Pooling especÃ­fico por tipo
        self.ft = FNBNScale(128, 128)               # FC + BN
        self.logits = nn.Linear(128, 4)             # ClasificaciÃ³n final

```

---

### **Preprocesamiento EspecÃ­fico por Tipo**

Cada tipo de semÃ¡foro requiereÂ **dimensiones diferentes**:

| Tipo | Dimensiones | Pool Params | Modelo |
| --- | --- | --- | --- |
| **Vertical** | 96Ã—32Ã—3 | kernel=(6,2), stride=(6,2) | `vert.torch` |
| **Horizontal** | 32Ã—96Ã—3 | kernel=(2,6), stride=(2,6) | `hori.torch` |
| **Quad** | 64Ã—64Ã—3 | kernel=(4,4), stride=(4,4) | `quad.torch` |

```python
# src/tlr/tools/utils.py:preprocess4rec()
def preprocess4rec(img, det_box, shape, means_rec):
    h, w, c = shape
    cropped = img[det_box[1]:det_box[3], det_box[0]:det_box[2]]
    resized = cv2.resize(cropped, (w, h))
    preprocessed = resized - means_rec  # means=[69.06, 66.58, 66.56]
    return torch.from_numpy(preprocessed).float()

```

---

### **LÃ³gica de ClasificaciÃ³n (Apollo-style Prob2Color)**

```python
# src/tlr/pipeline.py:40-82 (mÃ©todo recognize)
def recognize(self, img, detections, tl_types):
    for detection, tl_type in zip(detections, tl_types):
        # 1. Seleccionar recognizer segÃºn tipo
        recognizer, shape = self.classifiers[tl_type-1]

        # 2. Preprocesar
        input = preprocess4rec(img, det_box, shape, self.means_rec)
        input_scaled = input.permute(2, 0, 1).unsqueeze(0) * 0.01  # Apollo scale

        # 3. Inferencia
        output_probs = recognizer(input_scaled)[0]  # [black, red, yellow, green]

        # 4. Apollo's Prob2Color logic
        max_prob, max_idx = torch.max(output_probs, dim=0)
        threshold = 0.5

        if max_prob > threshold:
            color_id = max_idx.item()
        else:
            color_id = 0  # Force BLACK (desconocido)

        # 5. One-hot result
        result = torch.zeros(4)
        result[color_id] = 1.0

        recognitions.append(result)

```

**Mapeo de colores**:

```python
status_map = {0: 'BLACK', 1: 'RED', 2: 'YELLOW', 3: 'GREEN'}

```

---

### 4.3 ğŸš¨ HALLAZGO CRÃTICO: Dependencia Espacial ImplÃ­cita

### **DescripciÃ³n del Problema**

El modelo de reconocimiento no solo aprendiÃ³ a reconocerÂ **colores**, sino que tambiÃ©n memorizÃ³Â **posiciones espaciales**Â donde espera ver cada semÃ¡foro.

### **Evidencia Experimental**

**Test: Intercambio de Detecciones (Swap)**

```python
# ConfiguraciÃ³n normal (posiciones esperadas)
Det0 en (432,176,452,212):  # PosiciÃ³n izquierda
  â†’ Input al recognizer: ROI de semÃ¡foro verde
  â†’ Output: [0.0, 0.0, 0.0, 1.0]  # GREEN âœ…

Det1 en (476,175,501,247):  # PosiciÃ³n derecha
  â†’ Input al recognizer: ROI de semÃ¡foro amarillo
  â†’ Output: [0.0, 0.0, 1.0, 0.0]  # YELLOW âœ…

# Swap fÃ­sico (posiciones intercambiadas)
Det0 en (476,175,501,247):  # PosiciÃ³n derecha (movido)
  â†’ Input: MISMO ROI de semÃ¡foro verde
  â†’ Output: [1.0, 0.0, 0.0, 0.0]  # BLACK âŒ (Â¡cambiÃ³!)

Det1 en (432,176,452,212):  # PosiciÃ³n izquierda (movido)
  â†’ Input: MISMO ROI de semÃ¡foro amarillo
  â†’ Output: [1.0, 0.0, 0.0, 0.0]  # BLACK âŒ (Â¡cambiÃ³!)

```

---

### **AnÃ¡lisis TÃ©cnico**

**Â¿QuÃ© aprendiÃ³ el modelo?**

```python
# Modelo esperado (position-agnostic):
if pixels_show_green_light:
    output = GREEN  # Independiente de posiciÃ³n

# Modelo real (spatially-dependent):
if pixels_show_green_light AND position == LEFT:
    output = GREEN âœ…
elif pixels_show_green_light AND position == RIGHT:
    output = BLACK âŒ  # "Esto no deberÃ­a estar aquÃ­"

```

**Causa raÃ­z**: Sobreajuste espacial durante entrenamiento

- Datos de entrenamiento: Siempre semÃ¡foro verde en izquierda, amarillo en derecha
- Modelo aprendiÃ³ correlaciÃ³n espuria:Â `color + posiciÃ³n â†’ clasificaciÃ³n`
- No aprendiÃ³ caracterÃ­sticas visuales puras del color

---

### **Implicaciones PrÃ¡cticas**

**Para el sistema actual**:

- âœ… Funciona bien en escenario de entrenamiento
- âŒ Falla con nuevos Ã¡ngulos de cÃ¡mara
- âŒ Falla si semÃ¡foros cambian de posiciÃ³n fÃ­sica
- âŒ No robusto a variaciones espaciales

**Para deployment real**:

- Requiere re-entrenamiento con data augmentation espacial
- Necesita arquitecturas mÃ¡s robustas
- Debe enfocarse en caracterÃ­sticas intrÃ­nsecas del color

---

### 4.4 â“ El Historial NO se Usa como Input en Modelos

### **Pregunta ComÃºn**

*"Â¿El historial de tracking se usa como input para el detector o recognizer?"*

### **Respuesta: NO**

```python
# Orden de operaciones en pipeline.forward()

# 1. DETECCIÃ“N (sin historial)
detections = self.detect(img, boxes)
# Input: solo imagen + projection boxes

# 2. RECONOCIMIENTO (sin historial)
recognitions = self.recognize(img, valid_detections, tl_types)
# Input: solo imagen + coordenadas de detecciÃ³n

# 3. TRACKING (AQUÃ entra el historial)
revised = self.tracker.track(frame_ts, assigns_list, recs_list)
# Input: recognitions + historial previo

```

**DiseÃ±o Apollo Original**:

1. **Modelos puros**: CNNs solo ven pÃ­xeles, sin contexto temporal
2. **SeparaciÃ³n de responsabilidades**:
    - Detector/Recognizer â†’ "Â¿QuÃ© veo ahora?"
    - Tracker â†’ "Â¿QuÃ© significa esto en contexto temporal?"

**Ventajas**:

- Modelos mÃ¡s simples y generalizables
- Tracking como post-processing independiente
- MÃ¡s fÃ¡cil debugging y desarrollo

---

### 4.5 Pipeline de Tracking Temporal

### **Componentes del Tracker**

```python
# src/tlr/tracking.py
class TrafficLightTracker:
    def __init__(self):
        self.semantic = SemanticDecision(
            revise_time_s=1.5,           # Ventana temporal de revisiÃ³n
            blink_threshold_s=0.55,      # Umbral de parpadeo
            hysteretic_threshold=1       # Cambios consecutivos necesarios
        )
        self.frame_counter = 0

```

---

### **LÃ³gica de RevisiÃ³n Temporal (SemanticDecision)**

```python
# src/tlr/tracking.py:52-123
def update(self, frame_ts, assignments, recognitions):
    results = {}

    for proj_id, det_idx in assignments:
        # 1. Determinar color actual
        cls = int(max(range(len(recognitions[det_idx])),
                      key=lambda i: recognitions[det_idx][i]))
        color = ["black","red","yellow","green"][cls]

        # 2. Obtener o crear historial
        if proj_id not in self.history:
            self.history[proj_id] = SemanticTable(proj_id, frame_ts, color)
        st = self.history[proj_id]

        # 3. DETECCIÃ“N DE PARPADEO
        dt = frame_ts - st.time_stamp
        if color == "yellow" and dt < self.blink_threshold_s:
            st.blink = True
            color = "red"  # SAFETY: Yellow blink â†’ force RED
        else:
            st.blink = False

        # 4. REGLA DE SEGURIDAD: Yellow despuÃ©s de Red â†’ keep Red
        if color == "yellow" and st.color == "red":
            color = "red"  # Esperar hasta ver green

        # 5. HISTÃ‰RESIS (solo al salir de BLACK)
        if st.color == "black":
            # Conservative: need evidence to leave unknown state
            if st.hysteretic_color == color:
                st.hysteretic_count += 1
            else:
                st.hysteretic_color = color
                st.hysteretic_count = 1

            # Solo cambiar con suficiente evidencia
            if st.hysteretic_count > self.hysteretic_threshold:
                st.color = color
                st.hysteretic_count = 0
        else:
            # Entre estados conocidos: cambio inmediato
            st.color = color
            st.hysteretic_count = 0

        # 6. Actualizar timestamps
        st.time_stamp = frame_ts
        if color in ("red","green"):
            st.last_bright_time = frame_ts
        else:
            st.last_dark_time = frame_ts

        # 7. Reset histÃ©resis si pasa ventana de tiempo
        if frame_ts - st.time_stamp > self.revise_time_s:
            st.hysteretic_count = 0

        results[proj_id] = (st.color, st.blink)

    return results

```

---

### **ParÃ¡metros Configurables**

| ParÃ¡metro | Valor Default | PropÃ³sito |
| --- | --- | --- |
| `REVISE_TIME_S` | 1.5s | Ventana de historia considerada |
| `BLINK_THRESHOLD_S` | 0.55s | DuraciÃ³n mÃ­nima para cambio vÃ¡lido (no blink) |
| `HYSTERETIC_THRESHOLD_COUNT` | 1 | Frames consecutivos para confirmar cambio desde BLACK |

**UbicaciÃ³n**:Â `src/tlr/tracking.py:10-15`

---

### **Reglas de Seguridad Implementadas**

**1. Parpadeo de Amarillo â†’ Forzar Rojo**

```python
if color == "yellow" and dt < BLINK_THRESHOLD_S:
    st.blink = True
    color = "red"  # Safety override

```

- Si amarillo dura < 0.55s â†’ es parpadeo, no cambio real
- Por seguridad: tratar como ROJO

**2. Amarillo despuÃ©s de Rojo â†’ Mantener Rojo**

```python
if color == "yellow" and st.color == "red":
    color = "red"  # Invalid transition

```

- TransiciÃ³n Redâ†’Yellow es invÃ¡lida en semÃ¡foros reales
- DeberÃ­a ser Redâ†’Green, luego Greenâ†’Yellow
- Mantener RED hasta que se vea GREEN

**3. HistÃ©resis al Salir de BLACK**

```python
if st.color == "black":
    # Requiere confirmaciÃ³n (threshold) para cambiar
    if st.hysteretic_count > HYSTERETIC_THRESHOLD_COUNT:
        st.color = color

```

- BLACK = estado desconocido
- Requiere evidencia repetida para salir
- Entre colores conocidos: cambio inmediato

---

### 4.6 Pipeline Completo (MÃ©todoÂ `forward`)

```python
# src/tlr/pipeline.py:84-135
def forward(self, img, boxes, frame_ts=None):
    """
    Pipeline completo de detecciÃ³n, reconocimiento y tracking

    Returns:
        valid_detections: Tensor (nÃ—9) - Detecciones vÃ¡lidas
        recognitions: Tensor (nÃ—4) - Clasificaciones one-hot
        assignments: Tensor (mÃ—2) - Asignaciones [proj_id, det_idx]
        invalid_detections: Tensor (kÃ—9) - Detecciones filtradas
        revised_states: Dict {proj_id: (color, blink)} - Estados post-tracking
    """

    # 1. Early exit si no hay cajas
    if len(boxes) == 0:
        empty9 = torch.empty((0, 9), device=self.device)
        empty4 = torch.empty((0, 4), device=self.device)
        empty2 = torch.empty((0, 2), device=self.device)
        revised = {} if self.tracker else None
        return empty9, empty4, empty2, empty9, revised

    # 2. DETECCIÃ“N
    detections = self.detect(img, boxes)  # SSD detector

    # 3. FILTRADO POR TIPO
    tl_types = torch.argmax(detections[:, 5:], dim=1)
    valid_mask = tl_types != 0  # type 0 = background/unknown
    valid_detections = detections[valid_mask]
    invalid_detections = detections[~valid_mask]

    # 4. ASIGNACIÃ“N HÃšNGARA
    assignments = select_tls(self.ho, valid_detections,
                            boxes2projections(boxes), img.shape).to(self.device)

    # 5. RECONOCIMIENTO
    # Apollo: Solo reconoce las detecciones seleccionadas
    # Sistema actual: Reconoce TODAS las detecciones vÃ¡lidas
    if len(valid_detections) != 0:
        recognitions = self.recognize(img, valid_detections, tl_types[valid_mask])
    else:
        recognitions = torch.empty((0, 4), device=self.device)

    # 6. TRACKING / REVISIÃ“N TEMPORAL
    revised = None
    if self.tracker:
        if frame_ts is None:
            raise ValueError("Para usar tracking debes pasar frame_ts")

        # Convertir tensors a listas Python para el tracker
        assigns_list = assignments.cpu().tolist()
        recs_list = recognitions.cpu().tolist()

        # Aplicar lÃ³gica temporal
        revised = self.tracker.track(frame_ts, assigns_list, recs_list)

    return valid_detections, recognitions, assignments, invalid_detections, revised

```

---

### 4.7 ğŸ“Š Resumen de Hallazgos CrÃ­ticos

| Hallazgo | DescripciÃ³n | Impacto | MitigaciÃ³n |
| --- | --- | --- | --- |
| **Dependencia Espacial** | Recognizer memoriza posiciones | âŒ Falla con nuevos Ã¡ngulos | Re-entrenamiento con augmentation |
| **Historial NO en Input** | Tracking es post-processing puro | âœ… Modelos mÃ¡s simples | Ninguna (diseÃ±o correcto) |
| **MÃºltiples Detecciones/ROI** | 1 projection â†’ N detections | âš ï¸ Algunas quedan ID -1 | Selection algorithm mejorado |
| **Cross-History Transfer** | Historiales siguen regiones, no semÃ¡foros | âŒ Bug al intercambiar posiciones | Projection boxes dinÃ¡micas |
| **Coordinate Bug (Fixed)** | Scaling antes de offset | âœ… Ya corregido | N/A |

---

## 5. ğŸ–¥ï¸ Infraestructura de EjecuciÃ³n

### 5.1 Dependencias del Sistema

### **Dependencias Core**

```
torch        # PyTorch - Framework de deep learning
numpy        # Operaciones numÃ©ricas
pyyaml       # ConfiguraciÃ³n (opcional)

```

**UbicaciÃ³n**:Â `requirements.txt`

### **Dependencias Adicionales (ImplÃ­citas)**

```python
# DetecciÃ³n y visualizaciÃ³n
import cv2                    # OpenCV - Procesamiento de imÃ¡genes
from scipy.optimize import linear_sum_assignment  # Hungarian algorithm
import matplotlib.pyplot as plt  # VisualizaciÃ³n de resultados
import pandas as pd          # ExportaciÃ³n CSV

```

---

### 5.2 Soporte de Hardware

### **GPU (CUDA)**

**ConfiguraciÃ³n**:

```python
device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
pipeline = load_pipeline(device)

```

**Ventajas GPU**:

- **DetecciÃ³n SSD**: 15-20ms por ROI (vs 150-200ms CPU)
- **Reconocimiento CNN**: 5-10ms por semÃ¡foro (vs 50-80ms CPU)
- **Batch processing**: ParalelizaciÃ³n de mÃºltiples ROIs

**Requisitos**:

- CUDA 11.0+
- GPU con 2GB+ VRAM (modelos pequeÃ±os)
- cuDNN para optimizaciÃ³n

---

### **CPU Fallback**

**ConfiguraciÃ³n**:

```python
device = 'cpu'
pipeline = load_pipeline(device)

```

**Performance esperada**:

- **Pipeline completo**: ~300-500ms por frame (2 semÃ¡foros)
- **Bottleneck**: Detector SSD (70% del tiempo)
- **Viable para**: AnÃ¡lisis offline, debugging, desarrollo

---

### 5.3 Estructura de Archivos del Sistema

```
TrafficLightDetection/
â”‚
â”œâ”€â”€ src/tlr/                          # CÃ³digo fuente principal
â”‚   â”œâ”€â”€ pipeline.py                   # Pipeline orquestador
â”‚   â”œâ”€â”€ detector.py                   # Detector SSD
â”‚   â”œâ”€â”€ recognizer.py                 # CNNs de reconocimiento
â”‚   â”œâ”€â”€ tracking.py                   # Sistema de tracking temporal
â”‚   â”œâ”€â”€ hungarian_optimizer.py        # Assignment algorithm
â”‚   â”œâ”€â”€ selector.py                   # LÃ³gica de selecciÃ³n
â”‚   â”‚
â”‚   â”œâ”€â”€ weights/                      # Modelos pre-entrenados
â”‚   â”‚   â”œâ”€â”€ tl.torch                  # Detector (SSD)
â”‚   â”‚   â”œâ”€â”€ quad.torch                # Recognizer cuÃ¡druple
â”‚   â”‚   â”œâ”€â”€ hori.torch                # Recognizer horizontal
â”‚   â”‚   â””â”€â”€ vert.torch                # Recognizer vertical
â”‚   â”‚
â”‚   â”œâ”€â”€ confs/                        # Configuraciones JSON
â”‚   â”‚   â”œâ”€â”€ bbox_reg_param.json
â”‚   â”‚   â”œâ”€â”€ detection_output_ssd_param.json
â”‚   â”‚   â”œâ”€â”€ dfmb_psroi_pooling_param.json
â”‚   â”‚   â”œâ”€â”€ rcnn_bbox_reg_param.json
â”‚   â”‚   â””â”€â”€ rcnn_detection_output_ssd_param.json
â”‚   â”‚
â”‚   â””â”€â”€ tools/
â”‚       â””â”€â”€ utils.py                  # Utilidades (NMS, IoU, preprocesamiento)
â”‚
â”œâ”€â”€ perception recortado/             # CÃ³digo C++ original de Apollo
â”‚   â”œâ”€â”€ traffic_light_detection/
â”‚   â”œâ”€â”€ traffic_light_recognition/
â”‚   â””â”€â”€ traffic_light_tracking/
â”‚
â”œâ”€â”€ frames_auto_labeled/              # Frames procesados con annotations
â”‚   â”œâ”€â”€ frame_000001.jpg
â”‚   â”œâ”€â”€ projection_bboxes_master.txt  # Projection boxes por frame
â”‚   â””â”€â”€ outputs_debug_stages/         # Outputs organizados por etapa
â”‚
â”œâ”€â”€ robustness_tests/                 # Tests de robustez
â”‚   â”œâ”€â”€ original/
â”‚   â”œâ”€â”€ dark/
â”‚   â”œâ”€â”€ bright/
â”‚   â”œâ”€â”€ fog_light/
â”‚   â”œâ”€â”€ rain_light/
â”‚   â””â”€â”€ noise_light/
â”‚
â”œâ”€â”€ run_pipeline_debug_stages_fixed.py  # Script principal de ejecuciÃ³n
â”œâ”€â”€ select_projection_and_append.py     # Herramienta de annotaciÃ³n manual
â””â”€â”€ requirements.txt                    # Dependencias Python

```

---

### 5.4 Comandos de EjecuciÃ³n

### **EjecuciÃ³n Principal (Debug Completo)**

```bash
python run_pipeline_debug_stages_fixed.py

```

**Output generado**:

- CSV por etapa:Â `0_all_detections.csv`,Â `1_detection_results.csv`,Â `2_recognition_results.csv`,Â `3_final_results.csv`
- ImÃ¡genes visualizadas: CarpetasÂ `1_detection/`,Â `2_recognition/`,Â `3_final/`
- Logs detallados por consola

---

### **Procesamiento Batch**

```bash
python run_pipeline_batch.py

```

**CaracterÃ­sticas**:

- Procesa mÃºltiples frames secuencialmente
- Sin visualizaciÃ³n (mÃ¡s rÃ¡pido)
- Exporta solo CSVs finales

---

### **Tracking Temporal**

```bash
python run_pipeline_with_tracking.py

```

**Features**:

- Activa mÃ³dulo de tracking
- Detecta blinking automÃ¡ticamente
- Aplica reglas de seguridad Apollo

---

### **Tests de Robustez**

```bash
# Individual
python robustness_tests/dark/run_test_dark.py

# Todos los tests
python robustness_tests/run_all_tests.py

```

**Condiciones evaluadas**:

- IluminaciÃ³n:Â `dark`,Â `bright`,Â `low_contrast`
- Clima:Â `fog_light`,Â `rain_light`
- DegradaciÃ³n:Â `noise_light`,Â `jpeg_compression`,Â `sepia`,Â `blue_night`

---

### 5.5 ConfiguraciÃ³n de Projection Boxes

### **Formato del Archivo Master**

```
# projection_bboxes_master.txt
frame_000001.jpg 421,165,460,223,0 466,165,511,256,1
frame_000002.jpg 422,166,461,224,0 467,166,512,257,1

```

**Estructura**:Â `filename x1,y1,x2,y2,id x1,y1,x2,y2,id ...`

---

### **GeneraciÃ³n Manual de Projection Boxes**

```bash
python select_projection_and_append.py

```

**Workflow interactivo**:

1. Muestra frame de referencia
2. Usuario dibuja rectÃ¡ngulos con mouse
3. Sistema asigna IDs incrementales
4. Guarda/actualiza archivo master

---

### **PropagaciÃ³n AutomÃ¡tica (Videos)**

```bash
# En carpetas de test especÃ­ficas
python test_doble_chico/propagate_projections.py

```

**Estrategias implementadas**:

- **Constante**: Mismas coordenadas todo el video
- **DinÃ¡mica**: ActualizaciÃ³n basada en detecciones previas
- **Perspectiva**: CompensaciÃ³n de movimiento de cÃ¡mara

---

### 5.6 ParÃ¡metros Configurables del Sistema

### **Constantes de Tracking**

```python
# src/tlr/tracking.py:10-15
REVISE_TIME_S = 1.5              # Ventana temporal de revisiÃ³n
BLINK_THRESHOLD_S = 0.55         # Umbral de detecciÃ³n de parpadeo
HYSTERETIC_THRESHOLD_COUNT = 1   # Frames para confirmar cambio de estado

```

**ModificaciÃ³n**:

```python
tracker = TrafficLightTracker(
    revise_time_s=2.0,           # Aumentar ventana temporal
    blink_threshold_s=0.4,       # MÃ¡s sensible a parpadeo
    hysteretic_threshold=2       # MÃ¡s conservador en cambios
)

```

---

### **ParÃ¡metros de DetecciÃ³n**

```python
# src/tlr/tools/utils.py:214-218
crop_scale = 2.5        # ExpansiÃ³n de ROI (Apollo default)
min_crop_size = 270     # TamaÃ±o mÃ­nimo de crop
detector_size = 270     # Input size del detector SSD

```

**Impacto**:

- `crop_scale > 2.5`: Mayor contexto, mÃ¡s false positives
- `crop_scale < 2.5`: Menos contexto, riesgo de perder semÃ¡foros

---

### **Umbrales de Reconocimiento**

```python
# src/tlr/pipeline.py:65-69
threshold = 0.5  # Apollo's classify_threshold

if max_prob > threshold:
    color_id = max_idx.item()
else:
    color_id = 0  # Force BLACK

```

**Trade-off**:

- `threshold = 0.3`: MÃ¡s decisiones, menos "BLACK/Unknown"
- `threshold = 0.7`: MÃ¡s conservador, mÃ¡s rechazos

---

### **NMS (Non-Maximum Suppression)**

```python
# src/tlr/pipeline.py:36
idxs = nms(detections[:, 1:5], 0.7)  # IoU threshold

```

**Ajuste**:

- `threshold = 0.5`: MÃ¡s agresivo, elimina mÃ¡s duplicados
- `threshold = 0.9`: MÃ¡s permisivo, mantiene detecciones cercanas

---

### 5.7 Outputs del Sistema

### **CSVs Generados**

**1. All Detections**Â (`0_all_detections.csv`):

```
frame,det_id,status,conf,x1,y1,x2,y2,type_vert,type_quad,type_hori,type_bg
frame_000001.jpg,0,valid,0.95,432,176,452,212,0.006,0.984,0.008,0.002
frame_000001.jpg,1,valid,0.98,476,175,501,247,0.0005,0.999,0.0003,0.0003

```

**2. Recognition Results**Â (`2_recognition_results.csv`):

```
frame,det_id,proj_id,p_black,p_red,p_yellow,p_green,predicted_color
frame_000001.jpg,0,0,0.0,0.0,0.0,1.0,GREEN
frame_000001.jpg,1,1,0.0,0.0,1.0,0.0,YELLOW

```

**3. Final Tracking**Â (`3_final_results.csv`):

```
frame,proj_id,det_id,original_color,revised_color,blink_detected
frame_000001.jpg,0,0,GREEN,GREEN,False
frame_000001.jpg,1,1,YELLOW,RED,True

```

---

### **Visualizaciones por Etapa**

**Etapa 1 - Detection**Â (`1_detection/`):

- **Cajas azules**: Projection boxes originales
- **Cajas verdes**: Detecciones vÃ¡lidas
- **Cajas rojas**: Detecciones invÃ¡lidas (filtradas)
- **Labels**: Scores de tipo de semÃ¡foro

**Etapa 2 - Recognition**Â (`2_recognition/`):

- **Color de caja**: PredicciÃ³n de color (rojo/amarillo/verde/gris)
- **Labels**: Color + confianza (ej: "GREEN 0.98")

**Etapa 3 - Final Tracking**Â (`3_final/`):

- **Labels completos**:Â `Det0>P0: green>green`Â (detection â†’ projection: original â†’ revised)
- **Grosor de lÃ­nea**: LÃ­neas gruesas = cambio aplicado por tracking
- **Asterisco (*)**: Indica blinking detectado

---

## 6. ğŸ“Š AnÃ¡lisis de Performance

### 6.1 Benchmarks de Tiempo de EjecuciÃ³n

### **Tiempos por Componente (GPU - CUDA)**

| Componente | Tiempo (ms) | % del Total | OptimizaciÃ³n |
| --- | --- | --- | --- |
| **Detector SSD**Â (2 ROIs) | 30-40 | 60% | Batch processing posible |
| **NMS Global** | 2-3 | 4% | ImplementaciÃ³n PyTorch optimizada |
| **Assignment HÃºngaro** | 5-8 | 10% | Python puro (bottleneck CPU) |
| **Recognizer CNN**Â (2 lights) | 10-15 | 20% | GPU acelerado |
| **Tracking/Revision** | 1-2 | 3% | Lookups en dict |
| **Preprocesamiento** | 2-3 | 3% | Resize GPU-acelerado |
| **TOTAL por Frame** | **50-71 ms** | **100%** | **~14-20 FPS** |

---

### **Tiempos por Componente (CPU)**

| Componente | Tiempo (ms) | % del Total | LimitaciÃ³n |
| --- | --- | --- | --- |
| **Detector SSD**Â (2 ROIs) | 250-350 | 75% | Convolutions sin aceleraciÃ³n |
| **NMS Global** | 5-10 | 2% | Aceptable en CPU |
| **Assignment HÃºngaro** | 10-20 | 4% | Mismo que GPU |
| **Recognizer CNN**Â (2 lights) | 50-80 | 16% | Sin GPU |
| **Tracking/Revision** | 2-5 | 1% | Mismo que GPU |
| **Preprocesamiento** | 5-10 | 2% | Resize sin aceleraciÃ³n |
| **TOTAL por Frame** | **322-475 ms** | **100%** | **~2-3 FPS** |

---

### 6.2 AnÃ¡lisis de Bottlenecks

### **1. Detector SSD - Principal Cuello de Botella**

**Problema**:

```python
for projection in projections:  # Loop secuencial
    input = preprocess4det(image, projection, means)
    bboxes = self.detector(input.unsqueeze(0).permute(0, 3, 1, 2))
    detected_boxes.append(bboxes)

```

**OptimizaciÃ³n posible**:

```python
# Batch processing (no implementado actualmente)
all_inputs = torch.stack([preprocess4det(img, proj, means) for proj in projections])
all_bboxes = self.detector(all_inputs.permute(0, 3, 1, 2))  # Batch inference

```

**Ganancia esperada**: 30-40% reducciÃ³n en tiempo de detecciÃ³n

---

### **2. Hungarian Algorithm - CPU Bound**

**Problema**:

```python
# src/tlr/hungarian_optimizer.py
from scipy.optimize import linear_sum_assignment
row_ind, col_ind = linear_sum_assignment(cost_matrix)  # Python puro

```

**Impacto**:

- Con 2 projections Ã— 2 detections: ~5ms
- Con 10 projections Ã— 20 detections: ~50ms (cuadrÃ¡tico)

**Alternativa**:

```python
# ImplementaciÃ³n GPU-based (no disponible en scipy)
# LibrerÃ­as: lap, torch-hungarian

```

---

### **3. Coordinate Restoration - OperaciÃ³n Costosa**

**Problema identificado**Â (del resumen):

```python
# Bug original: coordenadas incorrectas por mal scaling
# Fix Apollo-style: scale LUEGO offset
detection[:, start_col] *= scale_x      # Primero escalar
detection[:, start_col] += xl           # Luego trasladar

```

**ObservaciÃ³n**: Fix corregido enÂ `utils.py:257-298`

---

### 6.3 Uso de Memoria

### **Footprint de Modelos**

| Modelo | TamaÃ±o (MB) | ParÃ¡metros | VRAM (GPU) | RAM (CPU) |
| --- | --- | --- | --- | --- |
| `tl.torch`Â (Detector) | 45 | ~12M | 180MB | 200MB |
| `quad.torch` | 8 | ~2M | 35MB | 40MB |
| `hori.torch` | 8 | ~2M | 35MB | 40MB |
| `vert.torch` | 8 | ~2M | 35MB | 40MB |
| **Total cargado** | **69 MB** | **~18M** | **285MB** | **320MB** |

---

### **Memoria Runtime**

```python
# Por frame procesado (imagen 1920Ã—1080)
Input image: 1920Ã—1080Ã—3 Ã— 4 bytes = 24.8 MB
Intermediate tensors (crops, detections): ~15 MB
Peak memory: ~40 MB por frame

# Con tracking (historial acumulado)
History per traffic light: ~500 bytes
Con 100 semÃ¡foros tracked: ~50 KB (negligible)

```

**Total VRAM requerido (GPU)**: ~350-400 MB

**Total RAM requerido (CPU)**: ~500-600 MB

---

### 6.4 Escalabilidad

### **Scaling con NÃºmero de Projection Boxes**

| # Projections | Tiempo DetecciÃ³n (ms) | Tiempo Assignment (ms) | Total (ms) | FPS |
| --- | --- | --- | --- | --- |
| 2 | 35 | 5 | 55 | 18 |
| 5 | 90 | 12 | 125 | 8 |
| 10 | 180 | 30 | 250 | 4 |
| 20 | 360 | 120 | 550 | 1.8 |

**ObservaciÃ³n**:Â **No escalable linealmente**Â debido a:

1. Loop secuencial en detector
2. Complejidad O(nÂ³) del Hungarian algorithm

---

### **OptimizaciÃ³n para MÃºltiples SemÃ¡foros**

**Estrategia Apollo Original**Â (del resumen):

- **ROI Expansion**: crop_scale = 2.5 Ã— max(w,h)
- **Multi-detection per ROI**: 1 ROI grande puede contener varios semÃ¡foros
- **Selection Algorithm**: Fusiona detecciones mÃºltiples

**Sistema Actual**:

- **1 Projection = 1 SemÃ¡foro esperado**
- LimitaciÃ³n: Assignment 1:1 estricto

---

### 6.5 Comparativa con Apollo Original (C++)

| MÃ©trica | Apollo C++ (TensorRT) | Sistema Actual (PyTorch GPU) | Sistema Actual (PyTorch CPU) |
| --- | --- | --- | --- |
| **Tiempo por frame (2 lights)** | 15-25 ms | 50-71 ms | 322-475 ms |
| **FPS mÃ¡ximo** | 40-66 | 14-20 | 2-3 |
| **Latencia total** | <30 ms | <80 ms | <500 ms |
| **Uso VRAM** | ~250 MB | ~350 MB | N/A |
| **Uso RAM** | ~150 MB | ~320 MB | ~600 MB |
| **PrecisiÃ³n (mAP)** | Baseline | **~97% del baseline** | ~97% del baseline |

**ConclusiÃ³n**: Sistema Python esÂ **2-3Ã— mÃ¡s lento**Â que C++/TensorRT pero mantiene precisiÃ³n similar.

---

### 6.6 Limitaciones de Performance Identificadas

### **1. Dependencia Espacial ImplÃ­cita**Â (Hallazgo CrÃ­tico del Resumen)

**Problema**:

```python
# Modelo de reconocimiento sobreajustado a posiciones espaciales
# SemÃ¡foro en posiciÃ³n "incorrecta" â†’ clasificado como BLACK
Det0 en (476,175): YELLOW âœ…
Det0 en (432,176): BLACK âŒ (mismos pÃ­xeles, diferente posiciÃ³n)

```

**Impacto en Performance**:

- **Accuracy degrada**Â cuando semÃ¡foros cambian de posiciÃ³n
- **No robusto**Â a cambios de Ã¡ngulo de cÃ¡mara
- **Requiere re-entrenamiento**Â con data augmentation espacial

---

### **2. ID -1 Phenomenon**Â (Detecciones No Asignadas)

**Causas identificadas**Â (del resumen):

- DetecciÃ³n fuera de projection boxes
- MÃºltiples detecciones del mismo semÃ¡foro (solo 1 se asigna)
- False positives lejanos de projections

**Frecuencia observada**: 5-10% de detecciones vÃ¡lidas quedan sin ID

---

### **3. Cross-History Transfer**Â (Bug de Tracking)

**Escenario**Â (del resumen):

```python
# Si semÃ¡foros intercambian posiciones fÃ­sicamente:
# Frame 214: Proj0 â† SemÃ¡foro_A, Proj1 â† SemÃ¡foro_B
# Frame 215: Proj0 â† SemÃ¡foro_B, Proj1 â† SemÃ¡foro_A

# Resultado: Historiales se transfieren entre semÃ¡foros
# SemÃ¡foro_A hereda historial de SemÃ¡foro_B (y viceversa)

```

**MitigaciÃ³n Apollo Original**: Projection boxes dinÃ¡micas que siguen semÃ¡foros fÃ­sicos.

---

### 6.7 Tests de Robustez - Resultados

### **Condiciones Adversas Evaluadas**

| Test | Accuracy | DetecciÃ³n | Reconocimiento | Observaciones |
| --- | --- | --- | --- | --- |
| **Original** | 100% | âœ… | âœ… | Baseline |
| **Dark** | 85% | âš ï¸ | âœ… | DetecciÃ³n falla en sombras |
| **Bright** | 90% | âœ… | âš ï¸ | SobreexposiciÃ³n confunde colores |
| **Low Contrast** | 80% | âš ï¸ | âš ï¸ | Ambos mÃ³dulos degradan |
| **Fog Light** | 88% | âœ… | âš ï¸ | Borrosidad afecta reconocimiento |
| **Rain Light** | 92% | âœ… | âœ… | Robusto a lluvia moderada |
| **Noise Light** | 87% | âœ… | âš ï¸ | Ruido confunde clasificador |
| **JPEG Compression** | 95% | âœ… | âœ… | Robusto a compresiÃ³n |
| **Sepia/Blue Night** | 70% | âŒ | âŒ | Cambios de color crÃ­ticos |

**ConclusiÃ³n**: Sistema esÂ **robusto a condiciones leves**Â peroÂ **vulnerable a cambios de iluminaciÃ³n extremos y shifts de color**.

---

### 6.8 Recomendaciones de OptimizaciÃ³n

### **Corto Plazo (ImplementaciÃ³n RÃ¡pida)**

1. **Batch Processing en Detector**
    
    ```python
    # Procesar mÃºltiples ROIs en paralelo
    all_inputs = torch.stack(inputs)
    all_outputs = self.detector(all_inputs)
    
    ```
    
    **Ganancia**: 30-40% reducciÃ³n en tiempo de detecciÃ³n
    
2. **Hungarian Algorithm Lazy Evaluation**
    
    ```python
    # Solo calcular assignment si hay cambios significativos
    if detection_positions_changed_significantly:
        assignments = hungarian_optimizer.optimize(...)
    else:
        assignments = previous_assignments  # Reuse
    
    ```
    
    **Ganancia**: 50% reducciÃ³n en frames con tracking estable
    
3. **NMS Early Stopping**
    
    ```python
    # Parar NMS cuando confidence < threshold
    if detection.confidence < min_threshold:
        break  # Resto son irrelevantes
    
    ```
    
    **Ganancia**: 10-15% en casos con muchos false positives
    

---

### **Mediano Plazo (Re-arquitectura)**

1. **TensorRT Conversion**
    - Convertir modelos PyTorch â†’ TensorRT
    - **Ganancia esperada**: 3-5Ã— speedup
2. **Multi-threading de Componentes**
    - Detection en thread 1
    - Recognition en thread 2
    - Tracking en thread 3
    - **Ganancia**: 40-60% con pipeline overlapping
3. **Dynamic Projection Box Updates**
    - Implementar lÃ³gica Apollo de proyecciÃ³n 3Dâ†’2D
    - Evitar cross-history transfer
    - Mejorar robustez a movimiento de cÃ¡mara

---

### **Largo Plazo (Re-entrenamiento)**

1. **Data Augmentation Espacial**
    
    ```python
    # Entrenar con semÃ¡foros en mÃºltiples posiciones
    augmented_data = {
        'spatial_shifts': [-50, -25, 0, +25, +50],  # pÃ­xeles
        'rotations': [-10, -5, 0, +5, +10],         # grados
        'scales': [0.8, 0.9, 1.0, 1.1, 1.2]
    }
    
    ```
    
    **Objetivo**: Eliminar dependencia espacial implÃ­cita
    
2. **Adversarial Training**
    - Entrenar con condiciones adversas (dark, bright, fog)
    - **Objetivo**: Mejorar robustez de 70-90% â†’ 95%+
3. **End-to-End Training**
    - Entrenar detector + recognizer juntos
    - **Objetivo**: Mejor co-adaptaciÃ³n de mÃ³dulos